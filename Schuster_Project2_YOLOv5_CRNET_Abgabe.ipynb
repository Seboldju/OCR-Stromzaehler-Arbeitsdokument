{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seboldju/OCR-Stromzaehler-Arbeitsdokument/blob/main/Schuster_Project2_YOLOv5_CRNET_Abgabe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2: Automatic Utility Meter Reading from Photos\n",
        "\n",
        "Thomas Bowduj (318186);\n",
        "Niklas Kluge (329596);\n",
        "Jan Riedel (322148);\n",
        "Julian Sebold (321768)\n"
      ],
      "metadata": {
        "id": "BfrXEHh6WuPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Einführung\n",
        "Im Rahmen des Projektes wurden verschiedene Ansätze untersucht, um das Projektziel zu erreichen.\n",
        "\n",
        "  ### 1. Counter Detection (Yolov5) - Jan & Julian\n",
        "  ### 2. Digit Detection (CR-NET) - Jan & Julian\n",
        "3. Digit Segmentation / Digit Recognition (CNN) - Niklas\n",
        "4. Digit Detection / Recognition (OCR) - Thomas\n",
        "\n",
        "Das Projekt wurde in enger Absprache mit allen Projektmitglieder über die einzelnen Projektphasen durchgeführt und zu gleichen Teilen erledigt.\n",
        "Für die oben genannten Ansätze war jeweils eine Person im Lead. Aufgrund der Kombination der Ansätze 1. und 2. zum besten Gesamtmodell haben Jan und Julian so eng zusammengearbeitet, sodass hier beide in gleichen Teilen verantwortlich waren.\n",
        "Aufgrund der Tatsachen, dass unsere zur Verfügung stehender Datensatz sehr groß ist, wir aber nicht auf Daten verzichten wollten, und auch die Modelle selber sehr groß wurden, war es uns nicht möglich das Projekt vollumfänglich in Colab abzubilden. Daher wurde das Projekt hauptsächlich über die Microsoft Python-Erweiterung in VS Code ausgeführt.\n",
        "\n",
        "### Jedoch wurde dieses Colab von Jan und Julian für eine ausführliche Beschreibung des Gesamtmodells (Kombination YOLOv5 - CRNET) erstellt\n"
      ],
      "metadata": {
        "id": "EakgdkY1GX04"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRrEghKI-kEW"
      },
      "outputs": [],
      "source": [
        "### Installation Bibliotheken\n",
        "\n",
        "!pip install absl-py==2.1.0 \\\n",
        "albucore==0.0.12 \\\n",
        "albumentations==1.4.11 \\\n",
        "annotated-types==0.7.0 \\\n",
        "astunparse==1.6.3 \\\n",
        "certifi==2024.7.4 \\\n",
        "charset-normalizer==3.3.2 \\\n",
        "cloudpickle==3.0.0 \\\n",
        "contourpy==1.2.1 \\\n",
        "cycler==0.12.1 \\\n",
        "easyocr==1.7.1 \\\n",
        "eval_type_backport==0.2.0 \\\n",
        "filelock==3.15.4 \\\n",
        "flatbuffers==24.3.25 \\\n",
        "fonttools==4.53.1 \\\n",
        "fsspec==2024.6.1 \\\n",
        "future==1.0.0 \\\n",
        "gast==0.6.0 \\\n",
        "gitdb==4.0.11 \\\n",
        "GitPython==3.1.43 \\\n",
        "google-pasta==0.2.0 \\\n",
        "grpcio==1.64.1 \\\n",
        "h5py==3.11.0 \\\n",
        "hyperopt==0.2.7 \\\n",
        "idna==3.7 \\\n",
        "imageio==2.34.2 \\\n",
        "Jinja2==3.1.4 \\\n",
        "joblib==1.4.2 \\\n",
        "keras==3.4.1 \\\n",
        "kiwisolver==1.4.5 \\\n",
        "lazy_loader==0.4 \\\n",
        "libclang==18.1.1 \\\n",
        "Markdown==3.6 \\\n",
        "markdown-it-py==3.0.0 \\\n",
        "MarkupSafe==2.1.5 \\\n",
        "matplotlib==3.9.1 \\\n",
        "mdurl==0.1.2 \\\n",
        "ml-dtypes==0.3.2 \\\n",
        "mpmath==1.3.0 \\\n",
        "namex==0.0.8 \\\n",
        "networkx==3.3 \\\n",
        "ninja==1.11.1.1 \\\n",
        "numpy==1.26.4 \\\n",
        "opencv-python==4.10.0.84 \\\n",
        "opencv-python-headless==4.10.0.84 \\\n",
        "opt-einsum==3.3.0 \\\n",
        "optree==0.12.1 \\\n",
        "packaging==24.1 \\\n",
        "pandas==2.2.2 \\\n",
        "pillow==10.4.0 \\\n",
        "pip==24.1.2 \\\n",
        "protobuf==4.25.3 \\\n",
        "psutil==6.0.0 \\\n",
        "py-cpuinfo==9.0.0 \\\n",
        "py4j==0.10.9.7 \\\n",
        "pyclipper==1.3.0.post5 \\\n",
        "pycocotools==2.0.8 \\\n",
        "pydantic==2.8.2 \\\n",
        "pydantic_core==2.20.1 \\\n",
        "Pygments==2.18.0 \\\n",
        "pyparsing==3.1.2 \\\n",
        "python-bidi==0.4.2 \\\n",
        "python-dateutil==2.9.0.post0 \\\n",
        "pytz==2024.1 \\\n",
        "PyYAML==6.0.1 \\\n",
        "requests==2.32.3 \\\n",
        "rich==13.7.1 \\\n",
        "scikit-image==0.24.0 \\\n",
        "scikit-learn==1.5.1 \\\n",
        "scipy==1.14.0 \\\n",
        "seaborn==0.13.2 \\\n",
        "setuptools==70.3.0 \\\n",
        "shapely==2.0.5 \\\n",
        "six==1.16.0 \\\n",
        "smmap==5.0.1 \\\n",
        "sympy==1.13.0 \\\n",
        "tensorboard==2.16.2 \\\n",
        "tensorboard-data-server==0.7.2 \\\n",
        "tensorflow==2.16.2 \\\n",
        "termcolor==2.4.0 \\\n",
        "thop==0.1.1-2209072238 \\\n",
        "threadpoolctl==3.5.0 \\\n",
        "tifffile==2024.7.2 \\\n",
        "tomli==2.0.1 \\\n",
        "torch==2.3.1 \\\n",
        "torchvision==0.18.1 \\\n",
        "tqdm==4.66.4 \\\n",
        "typing_extensions==4.12.2 \\\n",
        "tzdata==2024.1 \\\n",
        "ultralytics==8.2.55 \\\n",
        "ultralytics-thop==2.0.0 \\\n",
        "urllib3==2.2.2 \\\n",
        "Werkzeug==3.0.3 \\\n",
        "wheel==0.43.0 \\\n",
        "wrapt==1.16.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Datensatz\n",
        "\n",
        "Grundlage für das Projekt und Voraussetzung für unser Training war ein geeigneter Datensatz mit Testbildern von Stromzählern. Ursprünglich war eine Kooperation mit den Stadtwerken Pforzheim geplant, die uns aber keine Bilddaten zur Verfügung stellen konnten. Nach Rücksprache mit Prof. Schuster entschieden wir uns für einen Datensatz aus dem Internet von R. Laroca et al. (2019). Dieser entstand im Rahmen der Forschungsarbeit 'Convolutional Neural Networks for Automatic Meter Reading' und wurde uns auf Anfrage per E-Mail über einen Download-Link zur Verfügung gestellt.\n",
        "Der verwendete „UFPR-AMR“-Datensatz enthält 2000 Bilder von Stromzählern aus einem Lager der Energiegesellschaft von Paraná (Copel), die über 4 Millionen Verbraucher im brasilianischen Bundesstaat Paraná direkt beliefert. Jedem Bild ('image0001.jpg') sind Textdateien ('image0001.txt') zugeordnet, die die Nutzungskamera des Bildes, den Zählerstand des gleichnamigen Bildes sowie die Markierungen für die Positionen der Zählerstände und die einzelnen Ziffern enthalten."
      ],
      "metadata": {
        "id": "IxAdNGvrGaa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Beispielhafte neu erstellte Struktur der Textdatei für das Bild 'meter0001.txt' :\n",
        "\n",
        "camera: LG G3 D855\n",
        "reading: 00001\n",
        "position: 879 1775 434 178\n",
        "\tdigit 1: 888 1788 84 146\n",
        "\tdigit 2: 978 1788 76 144\n",
        "\tdigit 3: 1057 1788 77 143\n",
        "\tdigit 4: 1140 1788 77 143\n",
        "\tdigit 5: 1229 1792 66 136"
      ],
      "metadata": {
        "id": "LNIsKWQdJCLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Counter Detection (Ordner \"counter detection\" im Repository)\n",
        "Bei der Durchsicht des Datensatzes haben wir festgestellt, dass die Zähler viele Textblöcke haben, die mit der Zähleranzeige verwechselt werden können. Der Zähler, der unsere Region of Interest (ROI) darstellt, nimmt normalerweise nur einen kleinen Teil des Bildes ein. Die Position des Zählers hängt von vielen Faktoren ab, wie z. B. dem Zählertyp, dem Abstand zum Zähler, dem Knöchel und so weiter. Daher zielt unser Ansatz darauf ab, zunächst die Zählerregionen zu extrahieren und das Bild entsprechend dem Zähler zuzuschneiden. Dadurch können die nächsten Schritte, die Zifferndetektion und -erkennung, innerhalb der zuvor identifizierten Zähler stattfinden, was die oben erwähnten Störfaktoren wie Textblöcke deutlich reduzieren sollte.\n",
        "\n",
        "Für unsere spezifischen Anforderungen haben wir uns entschieden, ein YOLOv5-Modell zu verwenden und es an unseren Datensatz anzupassen. YOLOv5 ist ein leistungsfähiger und effizienter Objektdetektor, ideal für die Echtzeit-Objekterkennung. Somit eignet es sich auch sehr gut für Bilder, die mit dem Smartphone aufgenommen werden.\n",
        "\n",
        "Das Yolov5-Modell wurde für unseren Projektschritt in Visual Studio Code im Terminal geklont und im Ordner 'detector' abgelegt.\n",
        "\n",
        "Voraussetzung für das Yolov5-Modell sind Koordinaten im Yolo-Format der Objekte, die für das Training relevant sind. Diese wurden im Schritt Data Preparation erstellt.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6yPcs09UKoBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Github YOLOv5-Modell klonen\n",
        "\n",
        "cd /Users/Julez/Downloads/detector\n",
        "git clone https://github.com/ultralytics/yolov5.git\n",
        "cd yolov5\n",
        "\n",
        "# Installation Python-Abhängigkeiten\n",
        "pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "DL0LFeFKQ8vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data Preparation:\n",
        "### 3.1.1 Umwandlung Yolo-Format\n",
        "Zuerst wurden die Originalbilder und Annotationen aus dem Datensatz durch die folgenden 3 Skripte ins Yolo-Format konvertiert.\n",
        "\n",
        "Die zugehörigen Dateien finden sich im Ordner: project-st24-nk_jr_js/counter_detection/data preparation"
      ],
      "metadata": {
        "id": "j7d56bwBNg9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### dataset_convert_annotations_test\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Verzeichnis mit Bildern und Annotationsdateien\n",
        "dataset_path = '/Users/julez/Downloads/detector/counter_detection/data_prep/testing'\n",
        "output_path = '/Users/julez/Downloads/detector/counter_detection/yolov5_data/test/labels'\n",
        "\n",
        "# Sicherstellen, dass das Ausgabe-Verzeichnis existiert\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Funktion zur Umwandlung ins YOLO-Format\n",
        "def convert_to_yolo_format(meter_x, meter_y, meter_w, meter_h, image_width, image_height):\n",
        "    x_center = (meter_x + meter_w / 2) / image_width\n",
        "    y_center = (meter_y + meter_h / 2) / image_height\n",
        "    width = meter_w / image_width\n",
        "    height = meter_h / image_height\n",
        "    return f\"0 {x_center} {y_center} {width} {height}\"\n",
        "\n",
        "# Durchlaufen aller Annotationsdateien im Verzeichnis\n",
        "for filename in os.listdir(dataset_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Entsprechende Bilddatei finden\n",
        "        image_filename = filename.replace('.txt', '.jpg')\n",
        "        image_path = os.path.join(dataset_path, image_filename)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Bilddatei {image_filename} nicht gefunden.\")\n",
        "            continue\n",
        "\n",
        "        # Bildgröße ermitteln\n",
        "        with Image.open(image_path) as img:\n",
        "            image_width, image_height = img.size\n",
        "\n",
        "        with open(os.path.join(dataset_path, filename), 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            for line in lines:\n",
        "                if line.startswith('position:'):\n",
        "                    parts = line.split()\n",
        "                    meter_x = int(parts[1])\n",
        "                    meter_y = int(parts[2])\n",
        "                    meter_w = int(parts[3])\n",
        "                    meter_h = int(parts[4])\n",
        "\n",
        "                    yolo_annotation = convert_to_yolo_format(\n",
        "                        meter_x, meter_y, meter_w, meter_h,\n",
        "                        image_width, image_height\n",
        "                    )\n",
        "\n",
        "                    # Speicherung der YOLO-Format-Annotationsdatei\n",
        "                    output_filename = os.path.join(output_path, filename)\n",
        "                    with open(output_filename, 'w') as output_file:\n",
        "                        output_file.write(yolo_annotation)\n",
        "\n",
        "print(\"Umwandlung abgeschlossen.\")\n"
      ],
      "metadata": {
        "id": "NvMnyoPVNJOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### dataset_convert_annotations_train\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Verzeichnis mit Bildern und Annotationsdateien\n",
        "dataset_path = '/Users/julez/Downloads/detector/counter_detection/data_prep/training'\n",
        "output_path = '/Users/julez/Downloads/detector/counter_detection/yolov5_data/train/labels'\n",
        "\n",
        "# Sicherstellen, dass das Ausgabe-Verzeichnis existiert\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Funktion zur Umwandlung ins YOLO-Format\n",
        "def convert_to_yolo_format(meter_x, meter_y, meter_w, meter_h, image_width, image_height):\n",
        "    x_center = (meter_x + meter_w / 2) / image_width\n",
        "    y_center = (meter_y + meter_h / 2) / image_height\n",
        "    width = meter_w / image_width\n",
        "    height = meter_h / image_height\n",
        "    return f\"0 {x_center} {y_center} {width} {height}\"\n",
        "\n",
        "# Durchlaufen aller Annotationsdateien im Verzeichnis\n",
        "for filename in os.listdir(dataset_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Entsprechende Bilddatei finden\n",
        "        image_filename = filename.replace('.txt', '.jpg')\n",
        "        image_path = os.path.join(dataset_path, image_filename)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Bilddatei {image_filename} nicht gefunden.\")\n",
        "            continue\n",
        "\n",
        "        # Bildgröße ermitteln\n",
        "        with Image.open(image_path) as img:\n",
        "            image_width, image_height = img.size\n",
        "\n",
        "        with open(os.path.join(dataset_path, filename), 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            for line in lines:\n",
        "                if line.startswith('position:'):\n",
        "                    parts = line.split()\n",
        "                    meter_x = int(parts[1])\n",
        "                    meter_y = int(parts[2])\n",
        "                    meter_w = int(parts[3])\n",
        "                    meter_h = int(parts[4])\n",
        "\n",
        "                    yolo_annotation = convert_to_yolo_format(\n",
        "                        meter_x, meter_y, meter_w, meter_h,\n",
        "                        image_width, image_height\n",
        "                    )\n",
        "\n",
        "                    # Speicherung der YOLO-Format-Annotationsdatei\n",
        "                    output_filename = os.path.join(output_path, filename)\n",
        "                    with open(output_filename, 'w') as output_file:\n",
        "                        output_file.write(yolo_annotation)\n",
        "\n",
        "print(\"Umwandlung abgeschlossen.\")\n"
      ],
      "metadata": {
        "id": "2ayFfAyANTdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### dataset_convert_annotations_val\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Verzeichnis mit Bildern und Annotationsdateien\n",
        "dataset_path = '/Users/julez/Downloads/detector/counter_detection/data_prep/validation'\n",
        "output_path = '/Users/julez/Downloads/detector/counter_detection/yolov5_data/val/labels'\n",
        "\n",
        "# Sicherstellen, dass das Ausgabe-Verzeichnis existiert\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Funktion zur Umwandlung ins YOLO-Format\n",
        "def convert_to_yolo_format(meter_x, meter_y, meter_w, meter_h, image_width, image_height):\n",
        "    x_center = (meter_x + meter_w / 2) / image_width\n",
        "    y_center = (meter_y + meter_h / 2) / image_height\n",
        "    width = meter_w / image_width\n",
        "    height = meter_h / image_height\n",
        "    return f\"0 {x_center} {y_center} {width} {height}\"\n",
        "\n",
        "# Durchlaufen aller Annotationsdateien im Verzeichnis\n",
        "for filename in os.listdir(dataset_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Entsprechende Bilddatei finden\n",
        "        image_filename = filename.replace('.txt', '.jpg')\n",
        "        image_path = os.path.join(dataset_path, image_filename)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Bilddatei {image_filename} nicht gefunden.\")\n",
        "            continue\n",
        "\n",
        "        # Bildgröße ermitteln\n",
        "        with Image.open(image_path) as img:\n",
        "            image_width, image_height = img.size\n",
        "\n",
        "        with open(os.path.join(dataset_path, filename), 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            for line in lines:\n",
        "                if line.startswith('position:'):\n",
        "                    parts = line.split()\n",
        "                    meter_x = int(parts[1])\n",
        "                    meter_y = int(parts[2])\n",
        "                    meter_w = int(parts[3])\n",
        "                    meter_h = int(parts[4])\n",
        "\n",
        "                    yolo_annotation = convert_to_yolo_format(\n",
        "                        meter_x, meter_y, meter_w, meter_h,\n",
        "                        image_width, image_height\n",
        "                    )\n",
        "\n",
        "                    # Speicherung der YOLO-Format-Annotationsdatei\n",
        "                    output_filename = os.path.join(output_path, filename)\n",
        "                    with open(output_filename, 'w') as output_file:\n",
        "                        output_file.write(yolo_annotation)\n",
        "\n",
        "print(\"Umwandlung abgeschlossen.\")"
      ],
      "metadata": {
        "id": "_AO755yJNTz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Beispielhafte neu erstellte Struktur der YOLO-Annotationsdatei für das Bild 'meter0001.txt' :\n",
        "\n",
        "0 0.4683760683760684 0.4480769230769231 0.18547008547008548 0.04278846153846154\n",
        "\n"
      ],
      "metadata": {
        "id": "zl9euWUrYZbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2. Erstellung Training-Datensatz\n",
        "\n",
        "Für den Trainingsdatensatz wurde der Ordner 'yolov5_data' erstellt. Die Bild- und Text-Dateien  wurden für unser weiteres Vorgehen hierzu händisch in Unterordner 'images' und 'labels' verschoben und in jeweils auf die Ordner 'train' (800 Bilder;  meter0001 - meter0800), 'test' (400 Bilder; meter0801 - meter1200) und 'val' (800 Bilder; meter1201 - meter2000) aufgeteilt."
      ],
      "metadata": {
        "id": "ThNlCjYaNkyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "/Users/julez/downloads/detector/counter_detection/yolov5_data\n",
        "├── train/\n",
        "│   ├── images/\n",
        "│   │   ├── meter0001.jpg\n",
        "│   │   ├── meter0002jpg\n",
        "│   │   └── ...\n",
        "│   └── labels/\n",
        "│       ├── meter0001.txt\n",
        "│       ├── meter0002.txt\n",
        "│       └── ...\n",
        "├── val/\n",
        "│   ├── images/\n",
        "│   │   ├── meter1201.jpg\n",
        "│   │   ├── meter1202.jpg\n",
        "│   │   └── ...\n",
        "│   └── labels/\n",
        "│       ├── meter1201.txt\n",
        "│       ├── meter1202.txt\n",
        "│       └── ...\n",
        "└── test/\n",
        "    ├── images/\n",
        "    │   ├── meter0801.jpg\n",
        "    │   ├── meter0802.jpg\n",
        "    │   └── ...\n",
        "    └── labels/\n",
        "        ├── meter0801.txt\n",
        "        ├── meter0802.txt\n",
        "        └── ...\n"
      ],
      "metadata": {
        "id": "U5aossrqNlTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Vorbereitung Trainingsprozess\n",
        "### 3.2.1 Erstellen einer 'dataset_counter.yaml'-Datei als Grundlage für den Trainingsprozess\n",
        "\n",
        "Ordner: project-st24-nk_jr_js/counter_detection/yaml files for yolov5\n",
        "\n"
      ],
      "metadata": {
        "id": "1M-QHE3CNlve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### dataset_counter.yaml\n",
        "\n",
        "train: /Users/julez/Downloads/detector/counter_detection/yolov5_data/train/images\n",
        "val: /Users/julez/Downloads/detector/counter_detection/yolov5_data/val/images\n",
        "test: /Users/julez/Downloads/detector/counter_detection/yolov5_data/test/labels\n",
        "\n",
        "# Classes\n",
        "nc: 1  # Anzahl der Klassen, passen Sie dies entsprechend Ihren Daten an\n",
        "names: ['counter']  # Namen der Klassen, passen Sie dies entsprechend Ihren Daten an\n",
        "\n"
      ],
      "metadata": {
        "id": "P67EucyhNmIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Training YOLOv5-Modell\n",
        "\n",
        "Das Modell wird durch den nachfolgendem Befehl mit dem Verweis auf die train.py Datei sowie der .yaml mit 50 Epochen gestartet. Dabei werden die Bilder auf 640x640 Pixel skaliert und das Training im Terminal gestartet. Die Ausgabe im Terminal während und nach dem Training sind im folgenden zu entnehmen. Der gespeicherte Trainings-Run befindet sich in der Datei:\n",
        "\n",
        "project-st24-nk_jr_js/counter_detection/train and val runs yolov5/train/counter_detection_yolov5_Abgabe2"
      ],
      "metadata": {
        "id": "MmfCosbXQKrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Startbefehl Training\n",
        "python3 /Users/Julez/Downloads/detector/yolov5/train.py --img 640 --batch 16 --epochs 50 --data /Users/Julez/Downloads/detector/counter_detection/dataset_counter.yaml --weights yolov5s.pt --name counter_detection_yolov5_Abgabe2\n"
      ],
      "metadata": {
        "id": "HO4Brt_SNmU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Terminal Abschluss Training\n",
        "\n",
        "train: weights=yolov5s.pt, cfg=, data=/Users/Julez/Downloads/detector/counter_detection/dataset_counter.yaml, hyp=../../../../Julez/Downloads/detector/yolov5/data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=../../../../Julez/Downloads/detector/yolov5/data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=../../../../Julez/Downloads/detector/yolov5/runs/train, name=counter_detection_yolov5_Abgabe, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
        "github: ⚠️ YOLOv5 is out of date by 6 commits. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
        "requirements: Ultralytics requirement ['thop>=0.1.1'] not found, attempting AutoUpdate...\n",
        "Requirement already satisfied: thop>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (0.1.1.post2209072238)\n",
        "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (from thop>=0.1.1) (2.3.1)\n",
        "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch->thop>=0.1.1) (3.13.1)\n",
        "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch->thop>=0.1.1) (4.9.0)\n",
        "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch->thop>=0.1.1) (1.12)\n",
        "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch->thop>=0.1.1) (3.1)\n",
        "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch->thop>=0.1.1) (3.1.3)\n",
        "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch->thop>=0.1.1) (2023.10.0)\n",
        "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch->thop>=0.1.1) (2.1.3)\n",
        "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch->thop>=0.1.1) (1.3.0)\n",
        "\n",
        "requirements: AutoUpdate success ✅ 1.8s, installed 1 package: ['thop>=0.1.1']\n",
        "requirements: ⚠️ Restart runtime or rerun command for updates to take effect\n",
        "\n",
        "YOLOv5 🚀 v7.0-339-g150a1a31 Python-3.12.4 torch-2.3.1 CPU\n",
        "\n",
        "hyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
        "Comet: run 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
        "TensorBoard: Start with 'tensorboard --logdir ../../../../Julez/Downloads/detector/yolov5/runs/train', view at http://localhost:6006/\n",
        "Overriding model.yaml nc=80 with nc=1\n",
        "\n",
        "                 from  n    params  module                                  arguments\n",
        "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]\n",
        "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]\n",
        "  2                -1  1     18816  models.common.C3                        [64, 64, 1]\n",
        "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]\n",
        "  4                -1  2    115712  models.common.C3                        [128, 128, 2]\n",
        "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]\n",
        "  6                -1  3    625152  models.common.C3                        [256, 256, 3]\n",
        "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]\n",
        "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]\n",
        "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]\n",
        " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]\n",
        " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']\n",
        " 12           [-1, 6]  1         0  models.common.Concat                    [1]\n",
        " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]\n",
        " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]\n",
        " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']\n",
        " 16           [-1, 4]  1         0  models.common.Concat                    [1]\n",
        " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]\n",
        " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]\n",
        " 19          [-1, 14]  1         0  models.common.Concat                    [1]\n",
        " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]\n",
        " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]\n",
        " 22          [-1, 10]  1         0  models.common.Concat                    [1]\n",
        " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]\n",
        " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
        "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
        "\n",
        "Transferred 343/349 items from yolov5s.pt\n",
        "optimizer: SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
        "train: Scanning /Users/julez/Downloads/detector/counter_detection/yolov5_data/train/labels.cache... 800 images, 0 backgrounds, 0 corrupt: 100%|██████████| 800/800 [00:00<?, ?it/s]\n",
        "val: Scanning /Users/julez/Downloads/detector/counter_detection/yolov5_data/val/labels.cache... 800 images, 0 backgrounds, 0 corrupt: 100%|██████████| 800/800 [00:00<?, ?it/s]\n",
        "\n",
        "AutoAnchor: 3.59 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
        "Plotting labels to ../../../../Julez/Downloads/detector/yolov5/runs/train/counter_detection_yolov5_Abgabe2/labels.jpg...\n",
        "Image sizes 640 train, 640 val\n",
        "Using 8 dataloader workers\n",
        "Logging results to ../../../../Julez/Downloads/detector/yolov5/runs/train/counter_detection_yolov5_Abgabe2\n",
        "Starting training for 50 epochs...\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "       0/49         0G     0.1034    0.02139          0         23        640: 100%|██████████| 50/50 [13:35<00:00, 16.32s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/25 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  16%|█▌        | 4/25 [00:49<04:16, 12.22s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  28%|██▊       | 7/25 [01:24<03:30, 11.68s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  44%|████▍     | 11/25 [02:11<02:46, 11.90s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  48%|████▊     | 12/25 [02:24<02:36, 12.02s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  52%|█████▏    | 13/25 [02:36<02:24, 12.08s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  56%|█████▌    | 14/25 [02:48<02:13, 12.15s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  64%|██████▍   | 16/25 [03:12<01:49, 12.12s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  68%|██████▊   | 17/25 [03:24<01:36, 12.10s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  72%|███████▏  | 18/25 [03:37<01:24, 12.11s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  76%|███████▌  | 19/25 [03:49<01:12, 12.10s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [04:50<00:00, 11.62s/it]\n",
        "                   all        800        800    0.00297      0.881      0.016    0.00611\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "       1/49         0G    0.07203     0.0165          0         26        640: 100%|██████████| 50/50 [13:28<00:00, 16.17s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:48<00:00,  9.15s/it]\n",
        "                   all        800        800    0.00393      0.986      0.233     0.0638\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "       2/49         0G    0.05991    0.01553          0         24        640: 100%|██████████| 50/50 [10:48<00:00, 12.97s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:26<00:00,  8.25s/it]\n",
        "                   all        800        800    0.00334          1      0.567      0.231\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "       3/49         0G     0.0516    0.01489          0         24        640: 100%|██████████| 50/50 [10:49<00:00, 13.00s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:24<00:00,  8.19s/it]\n",
        "                   all        800        800      0.696      0.622      0.719      0.259\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "       4/49         0G     0.0454    0.01423          0         38        640: 100%|██████████| 50/50 [10:51<00:00, 13.02s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:23<00:00,  8.14s/it]\n",
        "                   all        800        800      0.852      0.805      0.891      0.445\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "       5/49         0G    0.04028    0.01223          0         34        640: 100%|██████████| 50/50 [10:49<00:00, 12.99s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:23<00:00,  8.14s/it]\n",
        "                   all        800        800       0.91       0.93       0.96      0.408\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "       6/49         0G    0.03823    0.01031          0         36        640: 100%|██████████| 50/50 [10:47<00:00, 12.94s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:22<00:00,  8.11s/it]\n",
        "                   all        800        800      0.953      0.966      0.982      0.467\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "       7/49         0G    0.03576   0.008891          0         22        640: 100%|██████████| 50/50 [10:50<00:00, 13.01s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:22<00:00,  8.09s/it]\n",
        "                   all        800        800      0.963      0.954      0.973      0.422\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "       8/49         0G    0.03415   0.008109          0         35        640: 100%|██████████| 50/50 [10:48<00:00, 12.97s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:23<00:00,  8.15s/it]\n",
        "                   all        800        800      0.978       0.98      0.987      0.524\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "       9/49         0G    0.03323   0.007231          0         27        640: 100%|██████████| 50/50 [10:46<00:00, 12.92s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:22<00:00,  8.09s/it]\n",
        "                   all        800        800      0.985      0.987      0.991      0.633\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      10/49         0G    0.03181   0.006536          0         24        640: 100%|██████████| 50/50 [10:44<00:00, 12.88s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:22<00:00,  8.10s/it]\n",
        "                   all        800        800      0.978      0.986      0.991      0.635\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      11/49         0G    0.03194   0.006422          0         37        640: 100%|██████████| 50/50 [10:43<00:00, 12.88s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:22<00:00,  8.12s/it]\n",
        "                   all        800        800       0.98      0.994      0.993      0.571\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      12/49         0G    0.02959   0.005758          0         31        640: 100%|██████████| 50/50 [10:44<00:00, 12.90s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:23<00:00,  8.12s/it]\n",
        "                   all        800        800      0.982      0.994      0.985      0.622\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      13/49         0G    0.02832   0.005441          0         23        640: 100%|██████████| 50/50 [10:44<00:00, 12.88s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:22<00:00,  8.10s/it]\n",
        "                   all        800        800       0.98      0.995      0.988      0.608\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      14/49         0G     0.0274   0.005447          0         25        640: 100%|██████████| 50/50 [10:45<00:00, 12.92s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:21<00:00,  8.08s/it]\n",
        "                   all        800        800      0.978      0.989      0.989      0.641\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      15/49         0G    0.02809   0.005482          0         34        640: 100%|██████████| 50/50 [10:45<00:00, 12.90s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:22<00:00,  8.10s/it]\n",
        "                   all        800        800      0.983      0.994      0.992      0.697\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      16/49         0G    0.02679   0.005294          0         26        640: 100%|██████████| 50/50 [10:42<00:00, 12.86s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:22<00:00,  8.11s/it]\n",
        "                   all        800        800      0.984      0.995      0.993      0.657\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      17/49         0G     0.0265   0.004852          0         24        640: 100%|██████████| 50/50 [10:42<00:00, 12.84s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:21<00:00,  8.06s/it]\n",
        "                   all        800        800      0.985      0.993      0.992      0.693\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      18/49         0G    0.02571   0.004875          0         28        640: 100%|██████████| 50/50 [10:44<00:00, 12.89s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:23<00:00,  8.13s/it]\n",
        "                   all        800        800      0.984       0.99      0.987      0.687\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      19/49         0G    0.02558   0.004659          0         27        640: 100%|██████████| 50/50 [10:41<00:00, 12.83s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:22<00:00,  8.11s/it]\n",
        "                   all        800        800      0.985      0.994      0.993      0.718\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      20/49         0G    0.02465   0.004555          0         35        640: 100%|██████████| 50/50 [10:42<00:00, 12.85s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:20<00:00,  8.03s/it]\n",
        "                   all        800        800      0.983      0.997      0.993      0.614\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      21/49         0G    0.02468   0.004607          0         25        640: 100%|██████████| 50/50 [10:42<00:00, 12.85s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:21<00:00,  8.04s/it]\n",
        "                   all        800        800      0.983      0.999      0.988      0.725\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      22/49         0G    0.02321   0.004387          0         30        640: 100%|██████████| 50/50 [10:37<00:00, 12.75s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:21<00:00,  8.06s/it]\n",
        "                   all        800        800      0.985      0.997      0.985      0.741\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      23/49         0G    0.02371    0.00448          0         33        640: 100%|██████████| 50/50 [10:40<00:00, 12.81s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:20<00:00,  8.02s/it]\n",
        "                   all        800        800      0.985      0.998       0.99      0.741\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      24/49         0G    0.02242   0.004355          0         28        640: 100%|██████████| 50/50 [10:12<00:00, 12.25s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:14<00:00,  7.78s/it]\n",
        "                   all        800        800      0.985      0.997      0.991      0.719\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      25/49         0G    0.02326   0.004028          0         19        640: 100%|██████████| 50/50 [10:35<00:00, 12.70s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:22<00:00,  8.09s/it]\n",
        "                   all        800        800      0.985      0.999       0.99       0.71\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      26/49         0G     0.0217    0.00424          0         25        640: 100%|██████████| 50/50 [10:42<00:00, 12.85s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:20<00:00,  8.01s/it]\n",
        "                   all        800        800      0.985      0.999      0.991      0.729\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      27/49         0G     0.0215   0.004065          0         30        640: 100%|██████████| 50/50 [10:37<00:00, 12.76s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:20<00:00,  8.02s/it]\n",
        "                   all        800        800      0.983      0.998      0.988      0.704\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      28/49         0G    0.02165   0.004031          0         37        640: 100%|██████████| 50/50 [10:35<00:00, 12.72s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:20<00:00,  8.02s/it]\n",
        "                   all        800        800      0.984      0.995      0.988      0.733\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      29/49         0G    0.02016   0.003902          0         32        640: 100%|██████████| 50/50 [10:38<00:00, 12.77s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:20<00:00,  8.01s/it]\n",
        "                   all        800        800      0.985      0.999       0.99      0.766\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      30/49         0G    0.01982   0.003791          0         29        640: 100%|██████████| 50/50 [10:40<00:00, 12.80s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:22<00:00,  8.08s/it]\n",
        "                   all        800        800      0.985      0.997      0.992      0.756\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      31/49         0G    0.02049   0.003842          0         23        640: 100%|██████████| 50/50 [10:37<00:00, 12.75s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:21<00:00,  8.04s/it]\n",
        "                   all        800        800      0.985      0.999      0.992      0.742\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      32/49         0G    0.01905   0.003678          0         30        640: 100%|██████████| 50/50 [10:36<00:00, 12.74s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:20<00:00,  8.04s/it]\n",
        "                   all        800        800      0.985      0.999      0.992       0.77\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      33/49         0G    0.01938   0.003709          0         24        640: 100%|██████████| 50/50 [10:40<00:00, 12.81s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:20<00:00,  8.02s/it]\n",
        "                   all        800        800      0.985      0.999       0.99      0.736\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      34/49         0G     0.0187   0.003571          0         23        640: 100%|██████████| 50/50 [10:39<00:00, 12.79s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:20<00:00,  8.03s/it]\n",
        "                   all        800        800      0.985          1      0.991      0.758\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      35/49         0G    0.01861   0.003729          0         31        640: 100%|██████████| 50/50 [10:40<00:00, 12.80s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:20<00:00,  8.03s/it]\n",
        "                   all        800        800      0.985          1       0.99       0.77\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      36/49         0G    0.01877   0.003677          0         28        640: 100%|██████████| 50/50 [10:39<00:00, 12.78s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:19<00:00,  7.97s/it]\n",
        "                   all        800        800      0.985          1      0.991      0.724\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      37/49         0G    0.01764   0.003666          0         24        640: 100%|██████████| 50/50 [10:34<00:00, 12.70s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:18<00:00,  7.94s/it]\n",
        "                   all        800        800      0.985          1      0.991      0.776\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      38/49         0G    0.01823   0.003603          0         33        640: 100%|██████████| 50/50 [10:30<00:00, 12.62s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:17<00:00,  7.91s/it]\n",
        "                   all        800        800      0.985      0.996      0.991       0.77\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      39/49         0G     0.0179   0.003623          0         28        640: 100%|██████████| 50/50 [10:27<00:00, 12.56s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:17<00:00,  7.89s/it]\n",
        "                   all        800        800      0.985      0.998      0.992      0.773\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      40/49         0G    0.01733   0.003655          0         25        640: 100%|██████████| 50/50 [10:27<00:00, 12.55s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:16<00:00,  7.87s/it]\n",
        "                   all        800        800      0.985      0.998      0.993      0.777\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      41/49         0G      0.017   0.003626          0         32        640: 100%|██████████| 50/50 [10:30<00:00, 12.61s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:18<00:00,  7.96s/it]\n",
        "                   all        800        800      0.985          1      0.993      0.778\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      42/49         0G     0.0167   0.003481          0         33        640: 100%|██████████| 50/50 [10:27<00:00, 12.54s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:17<00:00,  7.90s/it]\n",
        "                   all        800        800      0.985      0.999      0.992      0.778\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      43/49         0G    0.01685   0.003505          0         31        640: 100%|██████████| 50/50 [10:27<00:00, 12.55s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:18<00:00,  7.95s/it]\n",
        "                   all        800        800      0.985          1      0.991      0.781\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      44/49         0G    0.01663   0.003583          0         31        640: 100%|██████████| 50/50 [10:30<00:00, 12.61s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:19<00:00,  7.98s/it]\n",
        "                   all        800        800      0.985          1      0.991      0.783\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      45/49         0G    0.01644   0.003617          0         32        640: 100%|██████████| 50/50 [10:30<00:00, 12.61s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:18<00:00,  7.94s/it]\n",
        "                   all        800        800      0.985      0.999      0.991      0.781\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      46/49         0G    0.01543   0.003375          0         34        640: 100%|██████████| 50/50 [10:32<00:00, 12.65s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:19<00:00,  7.99s/it]\n",
        "                   all        800        800      0.985      0.999      0.992      0.782\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      47/49         0G    0.01605    0.00342          0         20        640: 100%|██████████| 50/50 [10:34<00:00, 12.68s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:19<00:00,  7.98s/it]\n",
        "                   all        800        800      0.985      0.999      0.992      0.786\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      48/49         0G    0.01565   0.003366          0         28        640: 100%|██████████| 50/50 [10:38<00:00, 12.78s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:20<00:00,  8.03s/it]\n",
        "                   all        800        800      0.985          1      0.993      0.783\n",
        "\n",
        "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
        "      49/49         0G     0.0157   0.003458          0         29        640: 100%|██████████| 50/50 [10:40<00:00, 12.81s/it]\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:21<00:00,  8.04s/it]\n",
        "                   all        800        800      0.985          1      0.993       0.79\n",
        "\n",
        "50 epochs completed in 11.805 hours.\n",
        "Optimizer stripped from ../../../../Julez/Downloads/detector/yolov5/runs/train/counter_detection_yolov5_Abgabe2/weights/last.pt, 14.3MB\n",
        "Optimizer stripped from ../../../../Julez/Downloads/detector/yolov5/runs/train/counter_detection_yolov5_Abgabe2/weights/best.pt, 14.3MB\n",
        "\n",
        "Validating ../../../../Julez/Downloads/detector/yolov5/runs/train/counter_detection_yolov5_Abgabe2/weights/best.pt...\n",
        "Fusing layers...\n",
        "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
        "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 25/25 [03:14<00:00,  7.79s/it]\n",
        "                   all        800        800      0.985          1      0.993       0.79\n",
        "Results saved to ../../../../Julez/Downloads/detector/yolov5/runs/train/counter_detection_yolov5_Abgabe2/weights/best.pt"
      ],
      "metadata": {
        "id": "Y4fvhqQGTbn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Evaluation Training YOLOv5-Modell\n",
        "Nach dem Trainingsprozess haben wir das Training evaluiert. Hierzu wurde die Validierungsdatei des Yolov5 Modells verwendet und die Ergebnisse im Ordner\n",
        "\n",
        "project-st24-nk_jr_js/counter_detection/train and val runs yolov5/val hinterlegt.\n",
        "\n",
        "Darüber hinaus wurde die Entwicklung des Modells im Traing geplottet und gespeichert, um Rückschlüsse auf den Erfolg des Traingsprozesses ziehen zu können (mAP & Box Loss). Diese Ergebnisse wurden im Ordner \"plots\" gespeichert, welcher sich in \"counter_detection\" befindet."
      ],
      "metadata": {
        "id": "Kw_gt6TFNmpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python3 /Users/Julez/Downloads/detector/yolov5/val.py --weights /Users/Julez/Downloads/detector/yolov5/runs/train/counter_detection_yolov5_Abgabe2/weights/best.pt --data /Users/Julez/Downloads/detector/counter_detection/dataset_counter.yaml --img 640 --iou 0.6 --conf 0.001 --task test"
      ],
      "metadata": {
        "id": "6YJz0WsyTaXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die nachfolgenden Skripte und Ergebnisse liegen im Ordner:\n",
        "\n",
        "project-st24-nk_jr_js/counter_detection/plots for paper"
      ],
      "metadata": {
        "id": "31arr2Odt3Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Erstellen des mAP-Plots\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# CSV-Datei einlesen\n",
        "csv_path = '/Users/julez/Downloads/detector/yolov5/runs/train/counter_detection_yolov5_Abgabe2/results.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Spaltennamen bereinigen\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Spaltennamen ausgeben, um zu überprüfen, ob 'epoch' existiert\n",
        "print(\"Spaltennamen der bereinigten CSV-Datei:\", df.columns)\n",
        "\n",
        "# Diagramm erstellen, wenn 'epoch' existiert\n",
        "if 'epoch' in df.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Training und Validation MAE\n",
        "    plt.plot(df['epoch'], df['metrics/mAP_0.5'], label='Train mAP_0.5', color='blue', linestyle='-')\n",
        "    plt.plot(df['epoch'], df['metrics/mAP_0.5:0.95'], label='Validation mAP_0.5:0.95', color='red', linestyle='--')\n",
        "\n",
        "    plt.title('Model Performance Yolov5 Counter Detection')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('mAP')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Diagramm speichern und anzeigen\n",
        "    plt.savefig('/Users/julez/Downloads/detector/counter_detection/plots/model_performance_mae.png')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Die Spalte 'epoch' existiert nicht in der CSV-Datei.\")\n"
      ],
      "metadata": {
        "id": "49E3ru6OMsvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotten der Loss-Werte\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# CSV-Datei einlesen\n",
        "csv_path = '/Users/julez/Downloads/detector/yolov5/runs/train/counter_detection_yolov5_Abgabe2/results.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Spaltennamen bereinigen\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Spaltennamen ausgeben, um zu überprüfen, ob 'epoch' existiert\n",
        "print(\"Spaltennamen der bereinigten CSV-Datei:\", df.columns)\n",
        "\n",
        "# Diagramm erstellen, wenn 'epoch' existiert\n",
        "if 'epoch' in df.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Training und Validation Mean Absolute Error (MAE)\n",
        "    plt.plot(df['epoch'], df['train/box_loss'], label='Train Box Loss', color='blue', linestyle='-')\n",
        "    plt.plot(df['epoch'], df['val/box_loss'], label='Validation Box Loss', color='red', linestyle='--')\n",
        "\n",
        "    plt.title('Model Performance Yolov5 Counter Detection')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Diagramm speichern und anzeigen\n",
        "    plt.savefig('/Users/julez/Downloads/detector/counter_detection/plots/model_performance.png')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Die Spalte 'epoch' existiert nicht in der CSV-Datei.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bUAve5cbNATE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Test YOLOv5-Modell starten inkl. Ausgabe der vorhergesagten Koordinaten"
      ],
      "metadata": {
        "id": "mCA7WXTOUbQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um neue, bisher unbekannte Bilder zu testen kann der folgende Befehl ausgeführt werden. Jedoch muss der Ordnerpfad zu den zu testenden Dateien (--source) hierfür aktualisiert bzw. individuell angepasst werden. Gespeichert werden die vorhergesagten Counter sowie deren Annotationen im Ordner \"test_predictions\" unter dem Namen \"results_test_final\".\n",
        "\n",
        "Pfad: project-st24-nk_jr_js/counter_detection/test_predictions"
      ],
      "metadata": {
        "id": "8pXmd4FcFEZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python3 /Users/Julez/Downloads/detector/yolov5/detect.py --weights /Users/julez/Downloads/detector/yolov5/runs/train/counter_detection_yolov5_Abgabe2/weights/best.pt --source /Users/julez/Downloads/detector/counter_detection/yolov5_data/test/images --img 640 --conf 0.25 --save-txt --save-conf --project /Users/julez/Downloads/detector/counter_detection/test_predictions --name results_test_final"
      ],
      "metadata": {
        "id": "KP-driA0UhQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.6 Nachverarbeitung der Detection Ergebnisse"
      ],
      "metadata": {
        "id": "TiJ7CKVFFhjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Da in drei Fällen mehr als ein Counter erkannt wurde (was, wie sich am Ende herausgestellt hat auch richtig war), wurde ein Nachverarbeitungsskript erstellt, das sicherstellt, dass lediglich ein Counter pro Foto gefunden wird, da in der Praxis jedes erstellte Bild vorraussichtlich lediglich einen Counter haben wird. Das Ergebnis des Skripts wird im Ordner \"filtered_labels\" innerhalb des Ordners \"results_test_final\" gespeichert. Somit liegen nun bereinigte vorhergesagte Counter vor.\n",
        "\n",
        "Pfad zum Skript: project-st24-nk_jr_js/counter_detection/python scripts/Post_test_Processing.py\n",
        "\n",
        "Pfad zum Ergebnis: project-st24-nk_jr_js/counter_detection/test_predictions/results_test_final/labels/filtered_labels"
      ],
      "metadata": {
        "id": "YL33U3RFeR3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Pfade zu den Ausgabe- und Bildverzeichnissen\n",
        "output_dir = '/Users/julez/Downloads/detector/counter_detection/test_predictions/results_test_final/labels'\n",
        "filtered_output_dir = '/Users/julez/Downloads/detector/counter_detection/test_predictions/results_test_final/filtered_labels'\n",
        "\n",
        "# Erstellen Sie das Ausgabe-Verzeichnis, wenn es nicht existiert\n",
        "os.makedirs(filtered_output_dir, exist_ok=True)\n",
        "\n",
        "# Alle Ausgabe-Textdateien durchlaufen\n",
        "for label_file in os.listdir(output_dir):\n",
        "    if label_file.endswith('.txt'):\n",
        "        label_path = os.path.join(output_dir, label_file)\n",
        "\n",
        "        # Einlesen der Textdatei\n",
        "        with open(label_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Wenn keine Zeilen vorhanden sind, überspringen\n",
        "        if not lines:\n",
        "            continue\n",
        "\n",
        "        # Die Boxen und ihre Wahrscheinlichkeiten in einem DataFrame speichern\n",
        "        boxes = []\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            class_id, x_center, y_center, width, height, conf = parts\n",
        "            boxes.append([class_id, float(x_center), float(y_center), float(width), float(height), float(conf)])\n",
        "\n",
        "        df = pd.DataFrame(boxes, columns=['class_id', 'x_center', 'y_center', 'width', 'height', 'conf'])\n",
        "\n",
        "        # Die Box mit der höchsten Wahrscheinlichkeit auswählen\n",
        "        best_box = df.loc[df['conf'].idxmax()]\n",
        "\n",
        "        # In einer neuen Datei speichern\n",
        "        filtered_label_path = os.path.join(filtered_output_dir, label_file)\n",
        "        with open(filtered_label_path, 'w') as f:\n",
        "            f.write(f\"{best_box['class_id']} {best_box['x_center']} {best_box['y_center']} {best_box['width']} {best_box['height']} {best_box['conf']}\\n\")\n",
        "\n",
        "print(f'Filtered labels saved to: {filtered_output_dir}')\n"
      ],
      "metadata": {
        "id": "o6c1OqcRGBuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.7 Evaluation der Testergebnisse durch IoU"
      ],
      "metadata": {
        "id": "xidP2Sk-GbZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zur Evaluation der Ergebnisse, wurde ein Skript geschriben, welches die IoU für jede Vorhersage berechnet. Zudem wird die durchschnittliche, minimale und maximale IoU angegeben. Darüber hinaus zeigt das Ergebnis auch, wie viele Vorhersagen ein IoU unter 0.5 haben und somit nicht richtig erkannt wurden. Die drei betroffenen Bilder konnten anschließend händisch auf die Ursache untersucht werden.\n",
        "\n",
        "Pfad zum Skript: project-st24-nk_jr_js/counter_detection/python scripts/Calculating_IoU.py\n",
        "\n",
        "Pfad zum Ergebnis: counter_detection/test_predictions/results_test_final/iou_results.txt abgelegt."
      ],
      "metadata": {
        "id": "dTbLFs1xGuC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def yolo_to_bbox(yolo_bbox, img_width, img_height):\n",
        "    \"\"\"Convert YOLO format to bounding box (x_min, y_min, x_max, y_max).\"\"\"\n",
        "    x_center, y_center, width, height = yolo_bbox\n",
        "    x_center, y_center, width, height = float(x_center), float(y_center), float(width), float(height)\n",
        "\n",
        "    x_min = int((x_center - width / 2) * img_width)\n",
        "    y_min = int((y_center - height / 2) * img_height)\n",
        "    x_max = int((x_center + width / 2) * img_width)\n",
        "    y_max = int((y_center + height / 2) * img_height)\n",
        "\n",
        "    return x_min, y_min, x_max, y_max\n",
        "\n",
        "def compute_iou(box1, box2):\n",
        "    \"\"\"Compute IoU between two bounding boxes.\"\"\"\n",
        "    x_min1, y_min1, x_max1, y_max1 = box1\n",
        "    x_min2, y_min2, x_max2, y_max2 = box2\n",
        "\n",
        "    xi1 = max(x_min1, x_min2)\n",
        "    yi1 = max(y_min1, y_min2)\n",
        "    xi2 = min(x_max1, x_max2)\n",
        "    yi2 = min(y_max1, y_max2)\n",
        "\n",
        "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
        "    box1_area = (x_max1 - x_min1) * (y_max1 - y_min1)\n",
        "    box2_area = (x_max2 - x_min2) * (y_max2 - y_min2)\n",
        "\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "# Pfade zu den vorhergesagten und den originalen Label-Dateien\n",
        "predicted_labels_path = '/Users/julez/Downloads/detector/counter_detection/test_predictions/results_test_final/labels/filtered_labels'\n",
        "original_labels_path = '/Users/julez/Downloads/detector/counter_detection/Test_images_model_combination/labels'\n",
        "images_path = '/Users/julez/Downloads/detector/counter_detection/yolov5_data/test/labels'\n",
        "\n",
        "iou_results = []\n",
        "low_iou_files = []\n",
        "\n",
        "# Vorhergesagte Label-Dateien durchlaufen\n",
        "for label_file in os.listdir(predicted_labels_path):\n",
        "    if label_file.endswith('.txt'):\n",
        "        predicted_file_path = os.path.join(predicted_labels_path, label_file)\n",
        "        original_file_path = os.path.join(original_labels_path, label_file)\n",
        "\n",
        "        image_file = label_file.replace('.txt', '.jpg')  # Annahme: Bilddateien sind im JPG-Format\n",
        "        img_path = os.path.join(images_path, image_file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Warnung: Bild nicht gefunden oder kann nicht gelesen werden: {img_path}\")\n",
        "            continue\n",
        "\n",
        "        img_height, img_width = img.shape[:2]\n",
        "\n",
        "        with open(predicted_file_path, 'r') as f_pred, open(original_file_path, 'r') as f_orig:\n",
        "            pred_lines = f_pred.readlines()\n",
        "            orig_lines = f_orig.readlines()\n",
        "\n",
        "            for pred_line, orig_line in zip(pred_lines, orig_lines):\n",
        "                pred_parts = pred_line.strip().split()\n",
        "                orig_parts = orig_line.strip().split()\n",
        "\n",
        "                pred_bbox = yolo_to_bbox(pred_parts[1:5], img_width, img_height)\n",
        "                orig_bbox = yolo_to_bbox(orig_parts[1:5], img_width, img_height)\n",
        "\n",
        "                iou = compute_iou(pred_bbox, orig_bbox)\n",
        "                iou_results.append((pred_bbox, orig_bbox, iou, label_file))\n",
        "\n",
        "                if iou < 0.5:\n",
        "                    low_iou_files.append(label_file)\n",
        "\n",
        "# IoU Ergebnisse nach Meter-Nummer sortieren\n",
        "iou_results.sort(key=lambda x: x[3])\n",
        "\n",
        "# Durchschnittliche, minimale und maximale IoU berechnen\n",
        "iou_values = [iou for _, _, iou, _ in iou_results]\n",
        "average_iou = np.mean(iou_values)\n",
        "min_iou = np.min(iou_values)\n",
        "max_iou = np.max(iou_values)\n",
        "\n",
        "# Ergebnisse in eine Textdatei schreiben\n",
        "output_path = '/Users/julez/Downloads/detector/counter_detection/test_predictions/results_test_final/iou_results.txt'\n",
        "with open(output_path, 'w') as f:\n",
        "    for i, (pred_bbox, orig_bbox, iou, label_file) in enumerate(iou_results):\n",
        "        f.write(f\"{label_file}:\\n\")\n",
        "        f.write(f\"Vorhergesagte Koordinaten: {pred_bbox}\\n\")\n",
        "        f.write(f\"Tatsächliche Koordinaten: {orig_bbox}\\n\")\n",
        "        f.write(f\"IoU = {iou}\\n\\n\")\n",
        "    f.write(f\"Durchschnittliche IoU: {average_iou}\\n\")\n",
        "    f.write(f\"Minimale IoU: {min_iou}\\n\")\n",
        "    f.write(f\"Maximale IoU: {max_iou}\\n\")\n",
        "    f.write(f\"Anzahl der Dateien mit IoU < 0,5: {len(low_iou_files)}\\n\")\n",
        "    f.write(\"Dateien mit IoU < 0,5:\\n\")\n",
        "    for file in low_iou_files:\n",
        "        f.write(f\"{file}\\n\")\n",
        "\n",
        "print(f'Ergebnisse wurden in {output_path} gespeichert.')\n"
      ],
      "metadata": {
        "id": "_HEwebmvGnZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ergebnis der IoU-Auswertung (Gesamtes Testing Set)**\n",
        "\n",
        "Durchschnittliche IoU: 0.8635772997189792\n",
        "\n",
        "Minimale IoU: 0.0\n",
        "\n",
        "Maximale IoU: 0.9875325038623172\n",
        "\n",
        "Anzahl der Dateien mit IoU < 0,5: 3\n",
        "\n",
        "Dateien mit IoU < 0,5:\n",
        "\n",
        "meter0847.txt\n",
        "meter0809.txt\n",
        "meter0870.txt\n",
        "\n"
      ],
      "metadata": {
        "id": "ag5BsBWIsDcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ergebnis der IoU-Auswertung (Kombination der Modelle)**\n",
        "\n",
        "Durchschnittliche IoU: 0.8851765201650009\n",
        "\n",
        "Minimale IoU: 0.5772017045454545\n",
        "\n",
        "Maximale IoU: 0.9875325038623172\n",
        "\n",
        "Anzahl der Dateien mit IoU < 0,5: 0\n",
        "\n",
        "Dateien mit IoU < 0,5:"
      ],
      "metadata": {
        "id": "U561GjiosVvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.8 Bilder zuschneiden\n",
        "Wie bereits oben erwähnt zielt unser Ansatz neben dem Trainingsprozess für die Counter Detection darauf ab, die Zählerregionen zu extrahieren und das Bild entsprechend dem Zähler zuzuschneiden. Nachdem das Training abgeschlossen und getestet wurde, haben wir über den folgenden Befehl die Bildregionen der Zähler extrahiert. Hierbei ist zu erwähnen, dass wir den Code so geschrieben haben, dass auf die größe der vorhergesagten Bounding Boxen ein bestimmter Aufschlag berechnet werrden kann. Wir haben uns für eine Vergrößerung der Bounding Boxes um 20% entschieden. Die zugeschnittenen Bilder werden im Ordner \"test_output_cropped_img\" in \"counter_detection\"  abgespeichert. Die so zugeschnittenen Bilder wurden dann an das CR-Net-Modell zur Digit Recognition übergeben.\n",
        "\n",
        "Pfad zu den zugeschnittenen Bildern von test_final: project-st24-nk_jr_js/counter_detection/test_final_output_cropped_20p\n",
        "\n",
        "Pfad zu den zugeschnittenen Bildern von test_combined (Kombination der Modelle):\n",
        "\n",
        "*   Mit 20% Vergrößerung: project-st24-nk_jr_js/counter_detection/test_final_output_cropped_20p\n",
        "*  Originalgröße: project-st24-nk_jr_js/counter_detection/test_combined_output_cropped_original\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QUp-2fJSVbu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "def yolo_to_bbox(yolo_bbox, img_width, img_height, enlargement_factor=1.0):\n",
        "    \"\"\"Convert YOLO format to bounding box (x_min, y_min, x_max, y_max) and enlarge the box.\"\"\"\n",
        "    x_center, y_center, width, height = yolo_bbox\n",
        "    x_center, y_center, width, height = float(x_center), float(y_center), float(width), float(height)\n",
        "\n",
        "    width *= enlargement_factor\n",
        "    height *= enlargement_factor\n",
        "\n",
        "    x_min = int((x_center - width / 2) * img_width)\n",
        "    y_min = int((y_center - height / 2) * img_height)\n",
        "    x_max = int((x_center + width / 2) * img_width)\n",
        "    y_max = int((y_center + height / 2) * img_height)\n",
        "\n",
        "    # Sicherstellen, dass Koordinaten innerhalb der Bildgrenzen sind\n",
        "    x_min = max(0, x_min)\n",
        "    y_min = max(0, y_min)\n",
        "    x_max = min(img_width, x_max)\n",
        "    y_max = min(img_height, y_max)\n",
        "\n",
        "    return x_min, y_min, x_max, y_max\n",
        "\n",
        "# Pfade zu den vorhergesagten Label-Dateien und zu den Originalbildern\n",
        "predicted_labels_path = '/Users/julez/Downloads/detector/counter_detection/test_predictions/results_test_final/filtered_labels'\n",
        "images_path = '/Users/julez/Downloads/detector/counter_detection/yolov5_data/test/images'\n",
        "output_dir = '/Users/julez/Downloads/detector/counter_detection/test_output_cropped_img'\n",
        "enlargement_factor = 1.2  # Vergrößerungsfaktor für die Bounding Box\n",
        "\n",
        "# Sicherstellen, dass das Ausgabeverzeichnis existiert\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Vorhergesagte Label-Dateien durchlaufen\n",
        "for label_file in os.listdir(predicted_labels_path):\n",
        "    if label_file.endswith('.txt'):\n",
        "        with open(os.path.join(predicted_labels_path, label_file), 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Holen des zugehörigen Bildes\n",
        "        image_file = label_file.replace('.txt', '.jpg')  # Annahme: Bilddateien sind im JPG-Format\n",
        "        img_path = os.path.join(images_path, image_file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Warnung: Bild nicht gefunden oder kann nicht gelesen werden: {img_path}\")\n",
        "            continue\n",
        "\n",
        "        img_height, img_width = img.shape[:2]\n",
        "\n",
        "        # Bounding Box basierend auf den vorhergesagten Koordinaten zuschneiden\n",
        "        for i, line in enumerate(lines):\n",
        "            parts = line.strip().split()\n",
        "            yolo_bbox = [float(p) for p in parts[1:5]]  # [class, x_center, y_center, width, height]\n",
        "            x_min, y_min, x_max, y_max = yolo_to_bbox(yolo_bbox, img_width, img_height, enlargement_factor)\n",
        "\n",
        "            # Ausschneiden des Bildes\n",
        "            cropped_img = img[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            # Speichern des zugeschnittenen Bildes\n",
        "            cropped_img_filename = f\"{label_file.replace('.txt', '')}_cropped_{i+1}.jpg\"\n",
        "            cropped_img_path = os.path.join(output_dir, cropped_img_filename)\n",
        "            cv2.imwrite(cropped_img_path, cropped_img)\n",
        "            print(f'Gespeichert: {cropped_img_path}')\n"
      ],
      "metadata": {
        "id": "jgz1UBulUhcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.9 Vorbereitung Übergabe YOLOv5 -> CRNET\n",
        "Um die Übergabe vom YOLOv5-Modell zum nächsten Projektschritt, dem CRNET-Modell, problemlos zu gestalten, wurde der Ordner 'digit_detection' erstellt, in dem mit den folgenden Befehlen die YOLO-Annotationsdateien der einzelnen 5 Digits für das CRNET-Modell erstellt wurden. Die zugeschnittenen Bilder, sowie die neuen .txt Dateien wurden in den Ordner '/Users/janriedel/Downloads/detector/digit_detection/yolov5_data' kopiert."
      ],
      "metadata": {
        "id": "v0y0ocNQXjrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Testing Data Prep.py\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "# Funktion zum Ausschneiden der Bilder\n",
        "def crop_image(image_path, bbox):\n",
        "    img = cv2.imread(image_path)\n",
        "    x, y, w, h = bbox\n",
        "    cropped_img = img[y:y+h, x:x+w]\n",
        "    return cropped_img\n",
        "\n",
        "# Funktion zum Berechnen der neuen Koordinaten\n",
        "def adjust_bbox(original_bbox, counter_bbox):\n",
        "    x_min, y_min = counter_bbox[0], counter_bbox[1]\n",
        "    new_bbox = [\n",
        "        original_bbox[0] - x_min,\n",
        "        original_bbox[1] - y_min,\n",
        "        original_bbox[2],\n",
        "        original_bbox[3]\n",
        "    ]\n",
        "    return new_bbox\n",
        "\n",
        "# Funktion zum Konvertieren in YOLO-Format\n",
        "def convert_to_yolo_format(bbox, img_width, img_height):\n",
        "    x_min, y_min, width, height = bbox\n",
        "    x_center = (x_min + width / 2) / img_width\n",
        "    y_center = (y_min + height / 2) / img_height\n",
        "    width /= img_width\n",
        "    height /= img_height\n",
        "    return [x_center, y_center, width, height]\n",
        "\n",
        "# Pfade\n",
        "input_image_path = '/Users/julez/Downloads/detector/UFPR-AMR Dataset/testing'\n",
        "annotation_file_path = '/Users/julez/Downloads/detector/UFPR-AMR Dataset/testing'\n",
        "output_image_path = '/Users/julez/Downloads/detector/digit_detection/testing_data/images_crp'\n",
        "new_annotations_path = '/Users/julez/Downloads/detector/digit_detection/testing_data/annotations_crp'\n",
        "yolo_annotations_path = '/Users/julez/Downloads/detector/digit_detection/testing_data/annotations_yolo'\n",
        "\n",
        "# Erstelle die Output-Ordner, falls nicht vorhanden\n",
        "os.makedirs(output_image_path, exist_ok=True)\n",
        "os.makedirs(new_annotations_path, exist_ok=True)\n",
        "os.makedirs(yolo_annotations_path, exist_ok=True)\n",
        "\n",
        "# Lese Annotationsdateien, schneide die Bilder und berechne neue Koordinaten\n",
        "for annotation_file in os.listdir(annotation_file_path):\n",
        "    if annotation_file.endswith('.txt'):\n",
        "        with open(os.path.join(annotation_file_path, annotation_file), 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Lese die Counter-Box-Koordinaten\n",
        "        counter_line = lines[2].strip().split()\n",
        "        try:\n",
        "            counter_bbox = [int(counter_line[1]), int(counter_line[2]), int(counter_line[3]), int(counter_line[4])]\n",
        "        except ValueError as e:\n",
        "            print(f\"Fehler beim Verarbeiten der Zeile: {counter_line}. Fehler: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Hole den Bildpfad\n",
        "        image_file = annotation_file.replace('.txt', '.jpg')\n",
        "        image_path = os.path.join(input_image_path, image_file)\n",
        "\n",
        "        # Schneide das Bild aus\n",
        "        cropped_img = crop_image(image_path, counter_bbox)\n",
        "\n",
        "        # Speichere das ausgeschnittene Bild\n",
        "        output_file = os.path.join(output_image_path, image_file)\n",
        "        cv2.imwrite(output_file, cropped_img)\n",
        "\n",
        "        new_lines = []\n",
        "\n",
        "        # Berechne die neuen Koordinaten für jede Ziffer\n",
        "        for i in range(3, 8):\n",
        "            digit_line = lines[i].strip().split()\n",
        "            try:\n",
        "                original_bbox = [int(digit_line[j].replace(':', '')) for j in range(2, 6)]\n",
        "            except ValueError as e:\n",
        "                print(f\"Fehler beim Verarbeiten der Zeile: {digit_line}. Fehler: {e}\")\n",
        "                continue\n",
        "            new_bbox = adjust_bbox(original_bbox, counter_bbox)\n",
        "            new_lines.append(f'{i-3} {new_bbox[0]} {new_bbox[1]} {new_bbox[2]} {new_bbox[3]}\\n')\n",
        "\n",
        "        # Speichere die neuen Annotationsdateien\n",
        "        new_annotation_file = os.path.join(new_annotations_path, annotation_file)\n",
        "        with open(new_annotation_file, 'w') as f:\n",
        "            f.writelines(new_lines)\n",
        "\n",
        "# Konvertiere die neuen Koordinaten in YOLO-Format\n",
        "for annotation_file in os.listdir(new_annotations_path):\n",
        "    if annotation_file.endswith('.txt'):\n",
        "        with open(os.path.join(new_annotations_path, annotation_file), 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Hole den Bildpfad\n",
        "        image_file = annotation_file.replace('.txt', '.jpg')\n",
        "        img_path = os.path.join(output_image_path, image_file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Bildgröße des zugeschnittenen Bildes\n",
        "        img_height, img_width = img.shape[:2]\n",
        "\n",
        "        yolo_lines = []\n",
        "\n",
        "        for idx, line in enumerate(lines):\n",
        "            parts = line.strip().split()\n",
        "            try:\n",
        "                bbox = [int(parts[i]) for i in range(1, 5)]\n",
        "            except ValueError as e:\n",
        "                print(f\"Fehler beim Verarbeiten der Zeile: {parts}. Fehler: {e}\")\n",
        "                continue\n",
        "            yolo_bbox = convert_to_yolo_format(bbox, img_width, img_height)\n",
        "            yolo_lines.append(f'{parts[0]} {yolo_bbox[0]} {yolo_bbox[1]} {yolo_bbox[2]} {yolo_bbox[3]}\\n')\n",
        "\n",
        "        # Speichere die YOLO-Annotationsdateien\n",
        "        yolo_annotation_file = os.path.join(yolo_annotations_path, annotation_file)\n",
        "        with open(yolo_annotation_file, 'w') as f:\n",
        "            f.writelines(yolo_lines)\n",
        "\n"
      ],
      "metadata": {
        "id": "iE2Zz1HxVRox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training Data Prep.py\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "# Funktion zum Ausschneiden der Bilder\n",
        "def crop_image(image_path, bbox):\n",
        "    img = cv2.imread(image_path)\n",
        "    x, y, w, h = bbox\n",
        "    cropped_img = img[y:y+h, x:x+w]\n",
        "    return cropped_img\n",
        "\n",
        "# Funktion zum Berechnen der neuen Koordinaten\n",
        "def adjust_bbox(original_bbox, counter_bbox):\n",
        "    x_min, y_min = counter_bbox[0], counter_bbox[1]\n",
        "    new_bbox = [\n",
        "        original_bbox[0] - x_min,\n",
        "        original_bbox[1] - y_min,\n",
        "        original_bbox[2],\n",
        "        original_bbox[3]\n",
        "    ]\n",
        "    return new_bbox\n",
        "\n",
        "# Funktion zum Konvertieren in YOLO-Format\n",
        "def convert_to_yolo_format(bbox, img_width, img_height):\n",
        "    x_min, y_min, width, height = bbox\n",
        "    x_center = (x_min + width / 2) / img_width\n",
        "    y_center = (y_min + height / 2) / img_height\n",
        "    width /= img_width\n",
        "    height /= img_height\n",
        "    return [x_center, y_center, width, height]\n",
        "\n",
        "# Pfade\n",
        "input_image_path = '/Users/julez/Downloads/detector/UFPR-AMR Dataset/training'\n",
        "annotation_file_path = '/Users/julez/Downloads/detector/UFPR-AMR Dataset/training'\n",
        "output_image_path = '/Users/julez/Downloads/detector/digit_detection/training_data/images_crp'\n",
        "new_annotations_path = '/Users/julez/Downloads/detector/digit_detection/training_data/annotations_crp'\n",
        "yolo_annotations_path = '/Users/julez/Downloads/detector/digit_detection/training_data/annotations_yolo'\n",
        "\n",
        "# Erstelle die Output-Ordner, falls nicht vorhanden\n",
        "os.makedirs(output_image_path, exist_ok=True)\n",
        "os.makedirs(new_annotations_path, exist_ok=True)\n",
        "os.makedirs(yolo_annotations_path, exist_ok=True)\n",
        "\n",
        "# Lese Annotationsdateien, schneide die Bilder und berechne neue Koordinaten\n",
        "for annotation_file in os.listdir(annotation_file_path):\n",
        "    if annotation_file.endswith('.txt'):\n",
        "        with open(os.path.join(annotation_file_path, annotation_file), 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Lese die Counter-Box-Koordinaten\n",
        "        counter_line = lines[2].strip().split()\n",
        "        try:\n",
        "            counter_bbox = [int(counter_line[1]), int(counter_line[2]), int(counter_line[3]), int(counter_line[4])]\n",
        "        except ValueError as e:\n",
        "            print(f\"Fehler beim Verarbeiten der Zeile: {counter_line}. Fehler: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Hole den Bildpfad\n",
        "        image_file = annotation_file.replace('.txt', '.jpg')\n",
        "        image_path = os.path.join(input_image_path, image_file)\n",
        "\n",
        "        # Schneide das Bild aus\n",
        "        cropped_img = crop_image(image_path, counter_bbox)\n",
        "\n",
        "        # Speichere das ausgeschnittene Bild\n",
        "        output_file = os.path.join(output_image_path, image_file)\n",
        "        cv2.imwrite(output_file, cropped_img)\n",
        "\n",
        "        new_lines = []\n",
        "\n",
        "        # Berechne die neuen Koordinaten für jede Ziffer\n",
        "        for i in range(3, 8):\n",
        "            digit_line = lines[i].strip().split()\n",
        "            try:\n",
        "                original_bbox = [int(digit_line[j].replace(':', '')) for j in range(2, 6)]\n",
        "            except ValueError as e:\n",
        "                print(f\"Fehler beim Verarbeiten der Zeile: {digit_line}. Fehler: {e}\")\n",
        "                continue\n",
        "            new_bbox = adjust_bbox(original_bbox, counter_bbox)\n",
        "            new_lines.append(f'{i-3} {new_bbox[0]} {new_bbox[1]} {new_bbox[2]} {new_bbox[3]}\\n')\n",
        "\n",
        "        # Speichere die neuen Annotationsdateien\n",
        "        new_annotation_file = os.path.join(new_annotations_path, annotation_file)\n",
        "        with open(new_annotation_file, 'w') as f:\n",
        "            f.writelines(new_lines)\n",
        "\n",
        "# Konvertiere die neuen Koordinaten in YOLO-Format\n",
        "for annotation_file in os.listdir(new_annotations_path):\n",
        "    if annotation_file.endswith('.txt'):\n",
        "        with open(os.path.join(new_annotations_path, annotation_file), 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Hole den Bildpfad\n",
        "        image_file = annotation_file.replace('.txt', '.jpg')\n",
        "        img_path = os.path.join(output_image_path, image_file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Bildgröße des zugeschnittenen Bildes\n",
        "        img_height, img_width = img.shape[:2]\n",
        "\n",
        "        yolo_lines = []\n",
        "\n",
        "        for idx, line in enumerate(lines):\n",
        "            parts = line.strip().split()\n",
        "            try:\n",
        "                bbox = [int(parts[i]) for i in range(1, 5)]\n",
        "            except ValueError as e:\n",
        "                print(f\"Fehler beim Verarbeiten der Zeile: {parts}. Fehler: {e}\")\n",
        "                continue\n",
        "            yolo_bbox = convert_to_yolo_format(bbox, img_width, img_height)\n",
        "            yolo_lines.append(f'{parts[0]} {yolo_bbox[0]} {yolo_bbox[1]} {yolo_bbox[2]} {yolo_bbox[3]}\\n')\n",
        "\n",
        "        # Speichere die YOLO-Annotationsdateien\n",
        "        yolo_annotation_file = os.path.join(yolo_annotations_path, annotation_file)\n",
        "        with open(yolo_annotation_file, 'w') as f:\n",
        "            f.writelines(yolo_lines)\n"
      ],
      "metadata": {
        "id": "PcvQkek8Y17r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Validation Data Prep.py\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "# Funktion zum Ausschneiden der Bilder\n",
        "def crop_image(image_path, bbox):\n",
        "    img = cv2.imread(image_path)\n",
        "    x, y, w, h = bbox\n",
        "    cropped_img = img[y:y+h, x:x+w]\n",
        "    return cropped_img\n",
        "\n",
        "# Funktion zum Berechnen der neuen Koordinaten\n",
        "def adjust_bbox(original_bbox, counter_bbox):\n",
        "    x_min, y_min = counter_bbox[0], counter_bbox[1]\n",
        "    new_bbox = [\n",
        "        original_bbox[0] - x_min,\n",
        "        original_bbox[1] - y_min,\n",
        "        original_bbox[2],\n",
        "        original_bbox[3]\n",
        "    ]\n",
        "    return new_bbox\n",
        "\n",
        "# Funktion zum Konvertieren in YOLO-Format\n",
        "def convert_to_yolo_format(bbox, img_width, img_height):\n",
        "    x_min, y_min, width, height = bbox\n",
        "    x_center = (x_min + width / 2) / img_width\n",
        "    y_center = (y_min + height / 2) / img_height\n",
        "    width /= img_width\n",
        "    height /= img_height\n",
        "    return [x_center, y_center, width, height]\n",
        "\n",
        "# Pfade\n",
        "input_image_path = '/Users/julez/Downloads/detector/UFPR-AMR Dataset/validation'\n",
        "annotation_file_path = '/Users/julez/Downloads/detector/UFPR-AMR Dataset/validation'\n",
        "output_image_path = '/Users/julez/Downloads/detector/digit_detection/validation_data/images_crp'\n",
        "new_annotations_path = '/Users/julez/Downloads/detector/digit_detection/validation_data/annotations_crp'\n",
        "yolo_annotations_path = '/Users/julez/Downloads/detector/digit_detection/validation_data/annotations_yolo'\n",
        "\n",
        "# Erstelle die Output-Ordner, falls nicht vorhanden\n",
        "os.makedirs(output_image_path, exist_ok=True)\n",
        "os.makedirs(new_annotations_path, exist_ok=True)\n",
        "os.makedirs(yolo_annotations_path, exist_ok=True)\n",
        "\n",
        "# Lese Annotationsdateien, schneide die Bilder und berechne neue Koordinaten\n",
        "for annotation_file in os.listdir(annotation_file_path):\n",
        "    if annotation_file.endswith('.txt'):\n",
        "        with open(os.path.join(annotation_file_path, annotation_file), 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Lese die Counter-Box-Koordinaten\n",
        "        counter_line = lines[2].strip().split()\n",
        "        try:\n",
        "            counter_bbox = [int(counter_line[1]), int(counter_line[2]), int(counter_line[3]), int(counter_line[4])]\n",
        "        except ValueError as e:\n",
        "            print(f\"Fehler beim Verarbeiten der Zeile: {counter_line}. Fehler: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Hole den Bildpfad\n",
        "        image_file = annotation_file.replace('.txt', '.jpg')\n",
        "        image_path = os.path.join(input_image_path, image_file)\n",
        "\n",
        "        # Schneide das Bild aus\n",
        "        cropped_img = crop_image(image_path, counter_bbox)\n",
        "\n",
        "        # Speichere das ausgeschnittene Bild\n",
        "        output_file = os.path.join(output_image_path, image_file)\n",
        "        cv2.imwrite(output_file, cropped_img)\n",
        "\n",
        "        new_lines = []\n",
        "\n",
        "        # Berechne die neuen Koordinaten für jede Ziffer\n",
        "        for i in range(3, 8):\n",
        "            digit_line = lines[i].strip().split()\n",
        "            try:\n",
        "                original_bbox = [int(digit_line[j].replace(':', '')) for j in range(2, 6)]\n",
        "            except ValueError as e:\n",
        "                print(f\"Fehler beim Verarbeiten der Zeile: {digit_line}. Fehler: {e}\")\n",
        "                continue\n",
        "            new_bbox = adjust_bbox(original_bbox, counter_bbox)\n",
        "            new_lines.append(f'{i-3} {new_bbox[0]} {new_bbox[1]} {new_bbox[2]} {new_bbox[3]}\\n')\n",
        "\n",
        "        # Speichere die neuen Annotationsdateien\n",
        "        new_annotation_file = os.path.join(new_annotations_path, annotation_file)\n",
        "        with open(new_annotation_file, 'w') as f:\n",
        "            f.writelines(new_lines)\n",
        "\n",
        "# Konvertiere die neuen Koordinaten in YOLO-Format\n",
        "for annotation_file in os.listdir(new_annotations_path):\n",
        "    if annotation_file.endswith('.txt'):\n",
        "        with open(os.path.join(new_annotations_path, annotation_file), 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Hole den Bildpfad\n",
        "        image_file = annotation_file.replace('.txt', '.jpg')\n",
        "        img_path = os.path.join(output_image_path, image_file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Bildgröße des zugeschnittenen Bildes\n",
        "        img_height, img_width = img.shape[:2]\n",
        "\n",
        "        yolo_lines = []\n",
        "\n",
        "        for idx, line in enumerate(lines):\n",
        "            parts = line.strip().split()\n",
        "            try:\n",
        "                bbox = [int(parts[i]) for i in range(1, 5)]\n",
        "            except ValueError as e:\n",
        "                print(f\"Fehler beim Verarbeiten der Zeile: {parts}. Fehler: {e}\")\n",
        "                continue\n",
        "            yolo_bbox = convert_to_yolo_format(bbox, img_width, img_height)\n",
        "            yolo_lines.append(f'{parts[0]} {yolo_bbox[0]} {yolo_bbox[1]} {yolo_bbox[2]} {yolo_bbox[3]}\\n')\n",
        "\n",
        "        # Speichere die YOLO-Annotationsdateien\n",
        "        yolo_annotation_file = os.path.join(yolo_annotations_path, annotation_file)\n",
        "        with open(yolo_annotation_file, 'w') as f:\n",
        "            f.writelines(yolo_lines)\n",
        "\n"
      ],
      "metadata": {
        "id": "PR1Ua1bNY2Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Beispielhafte neu erstellte Struktur der YOLO-Annotationen für die jeweiligen 5 Digits für  'meter0001.txt'\n",
        "\n",
        "0 0.1175115207373272 0.48314606741573035 0.1935483870967742 0.8202247191011236\n",
        "1 0.315668202764977 0.47752808988764045 0.17511520737327188 0.8089887640449438\n",
        "2 0.4988479262672811 0.4747191011235955 0.1774193548387097 0.8033707865168539\n",
        "3 0.6900921658986175 0.4747191011235955 0.1774193548387097 0.8033707865168539\n",
        "4 0.8824884792626728 0.47752808988764045 0.15207373271889402 0.7640449438202247\n"
      ],
      "metadata": {
        "id": "Zw8iEQb6Y2de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. CRNET Digit Detection\n",
        "\n",
        "Im folgenden wird der Prozessschritt 'CRNET_digit_detection' beschrieben. Ziel war es, ein CRNET darauf zu trainieren, die jeweiligen Digits aus den gecroppten Bildern korrekt zu erkennen und vorherzusagen."
      ],
      "metadata": {
        "id": "n8L0cmskaDEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Data Preparation\n",
        "### 4.1.1 Übernahme Datensatz aus 3.9 Digit Detection\n",
        "Um den Traingsprozess zu starten, war ein erneutes Anpassen des Datensatzes notwendig. Hierfür wurde die erstellte Ordnerstruktur aus 3.9 ('/Users/janriedel/Downloads/detector/digit_detection/yolov5_data') in den folgenden Ordner kopiert: '/Users/janriedel/Downloads/detector/CRNET_digit_detection/data_preparation/image_crp'."
      ],
      "metadata": {
        "id": "Du_sMKTFafk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.2 Erstellen neuer Textdateien\n",
        "Das Trainingsmodell CRNET benötigt neben den bereits vorliegenden YOLO-Annotationsdaten der 5 Digits noch den IST-Wert des Zählerstandes auf dem Bild, um zuverlässig darauf trainiert zu werden, die richtigen Zählerstände zu predicten. Mit den folgenden Befehlen wurden zuerst die Reading-Scores aus den Originalen Textdateien des 'UFPR-AMR Dataset' extrahiert und zwischengespeichert, bevor die neuen Dateien mit den von Julian erstellten zusammengeführt wurden.\n"
      ],
      "metadata": {
        "id": "kS8NKRRGbHVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### extract_readings.py\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the source and target directories\n",
        "source_dir = '/Users/janriedel/Downloads/lbls/UFPR-AMR Dataset'\n",
        "target_dir = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/data_preparation/results/labels'\n",
        "\n",
        "# Define the subdirectories\n",
        "subdirs = ['training', 'testing', 'validation']\n",
        "\n",
        "def process_file(file_path, target_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            if line.startswith('reading:'):\n",
        "                reading_score = line.split(':')[1].strip()\n",
        "                if len(reading_score) == 5:\n",
        "                    with open(target_path, 'w') as output_file:\n",
        "                        for digit in reading_score:\n",
        "                            output_file.write(digit + '\\n')\n",
        "                break\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Process each file in the subdirectories\n",
        "for subdir in subdirs:\n",
        "    subdir_path = os.path.join(source_dir, subdir)\n",
        "    for file_name in os.listdir(subdir_path):\n",
        "        if file_name.endswith('.txt'):\n",
        "            file_path = os.path.join(subdir_path, file_name)\n",
        "            target_path = os.path.join(target_dir, file_name)\n",
        "            process_file(file_path, target_path)\n",
        "\n",
        "print(\"Processing complete!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rRuFnLu0cRoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Beispielhafte neu erstellte Struktur der Reading-Scores für den jeweiligen Zählerstand für 'meter0001.txt'\n",
        "\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "\n"
      ],
      "metadata": {
        "id": "349xG6ZwcR35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### labels_combined.py\n",
        "\n",
        "import os\n",
        "\n",
        "# Definiere die Verzeichnisse\n",
        "source_readings_dir = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/data_preparation/results/labels'\n",
        "source_yolo_dir = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/data_preparation/image_crp/yolov5_data'\n",
        "target_dir = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/data_preparation/newlabels'\n",
        "\n",
        "# Definiere die Subdirectories\n",
        "subdirs = ['train', 'test', 'val']\n",
        "\n",
        "def read_reading_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "def read_yolo_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "def write_combined_file(reading_lines, yolo_lines, target_path):\n",
        "    with open(target_path, 'w') as file:\n",
        "        for line in reading_lines:\n",
        "            file.write(line)\n",
        "        for line in yolo_lines:\n",
        "            file.write(line)\n",
        "\n",
        "# Erstelle das Zielverzeichnis, wenn es nicht existiert\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Prozessiere jede Datei in den Subdirectories\n",
        "for subdir in subdirs:\n",
        "    yolo_labels_path = os.path.join(source_yolo_dir, subdir, 'labels')\n",
        "    for file_name in os.listdir(yolo_labels_path):\n",
        "        if file_name.endswith('.txt'):\n",
        "            reading_file_path = os.path.join(source_readings_dir, file_name)\n",
        "            yolo_file_path = os.path.join(yolo_labels_path, file_name)\n",
        "            target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "            if os.path.exists(reading_file_path):\n",
        "                reading_lines = read_reading_file(reading_file_path)\n",
        "                yolo_lines = read_yolo_file(yolo_file_path)\n",
        "                write_combined_file(reading_lines, yolo_lines, target_file_path)\n",
        "\n",
        "print(\"Processing complete!\")\n"
      ],
      "metadata": {
        "id": "74ZBd2AKcSI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Beispielhafte neu erstellte Struktur der Reading-Scores für den jeweiligen Zählerstand inkl. YOLO-Annotation für 'meter0001.txt'\n",
        "\n",
        "0 0\n",
        "1 0\n",
        "2 0\n",
        "3 0\n",
        "4 1\n",
        "0 0.1175115207373272 0.48314606741573035 0.1935483870967742 0.8202247191011236\n",
        "1 0.315668202764977 0.47752808988764045 0.17511520737327188 0.8089887640449438\n",
        "2 0.4988479262672811 0.4747191011235955 0.1774193548387097 0.8033707865168539\n",
        "3 0.6900921658986175 0.4747191011235955 0.1774193548387097 0.8033707865168539\n",
        "4 0.8824884792626728 0.47752808988764045 0.15207373271889402 0.7640449438202247\n",
        "\n"
      ],
      "metadata": {
        "id": "JFd1BAlUcb1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.3 Erstellung CRNET Trainingsdatensatz 'yolov5_data'\n",
        "\n",
        "Nachdem somit alle benötigten Dateien erstellt wurden, wurde der Datensatz händisch neu aufgeteilt. Entsprechend der gängigen Praxis für einen CRNET-Trainingsprozess haben wir uns für die folgende Aufteilung entschieden:\n",
        "\n",
        "1. Training (75% der Bilder)\n",
        "2. Testing (12,5%)\n",
        "3. Validierung (12,5%)\n",
        "\n",
        "also\n",
        "\n",
        "\n",
        "1. Train: 1500 Bilder (meter0001 - meter0950 + meter1201 - meter1750)\n",
        "\n",
        "2. Val: 250 Bilder (meter1751 - meter2000)\n",
        "\n",
        "3. Test: 250 Bilder ( meter0951 - meter1200)\n",
        "\n",
        "Die zugehörigen Textdateien wurden analog der folgenden Verzeichnisstruktur in den Ordner 'yolov5_data' kopiert.\n",
        "\n"
      ],
      "metadata": {
        "id": "C-8u-Qy8dIV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "/Users/janriedel/Downloads/detector/CRNET_digit_detection/yolov5_data/\n",
        "├── train/\n",
        "│   ├── images/\n",
        "│   │   ├── meter0001.jpg\n",
        "│   │   ├── meter0002.jpg\n",
        "│   │   └── ...\n",
        "│   └── labels/\n",
        "│       ├── meter0001.txt\n",
        "│       ├── meter0002.txt\n",
        "│       └── ...\n",
        "├── val/\n",
        "│   ├── images/\n",
        "│   │   ├── meter1751.jpg\n",
        "│   │   ├── meter1752.jpg\n",
        "│   │   └── ...\n",
        "│   └── labels/\n",
        "│       ├── meter1751.txt\n",
        "│       ├── meter1752.txt\n",
        "│       └── ...\n",
        "└── test/\n",
        "    ├── images/\n",
        "    │   ├── meter0951.jpg\n",
        "    │   ├── meter0952.jpg\n",
        "    │   └── ...\n",
        "    └── labels/\n",
        "        ├── meter0951.txt\n",
        "        ├── meter0952.txt\n",
        "        └── ...\n"
      ],
      "metadata": {
        "id": "YxZ8g7yCc88J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Vorbereitung Trainingsprozess\n",
        "Für den Trainingsprozess wurden 2 Dateien vorbereitet, anhand derer das Training durchgeführt werden kann: 'crnet.py' & 'train.crnet.py'\n"
      ],
      "metadata": {
        "id": "izeNCKsjgPg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### crnet.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CRNET(nn.Module):\n",
        "    def __init__(self, num_classes=10, num_digits=5):\n",
        "        super(CRNET, self).__init__()\n",
        "        self.num_digits = num_digits\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifiers = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        ) for _ in range(num_digits)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        outputs = [classifier(x) for classifier in self.classifiers]\n",
        "        return outputs\n",
        "\n",
        "def crnet(num_classes=10, num_digits=5):\n",
        "    model = CRNET(num_classes=num_classes, num_digits=num_digits)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "zDCgAbIQgMSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### train.crnet.py\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CRNET(nn.Module):\n",
        "    def __init__(self, num_classes=10, num_digits=5):\n",
        "        super(CRNET, self).__init__()\n",
        "        self.num_digits = num_digits\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifiers = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        ) for _ in range(num_digits)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        outputs = [classifier(x) for classifier in self.classifiers]\n",
        "        return outputs\n",
        "\n",
        "def crnet(num_classes=10, num_digits=5):\n",
        "    model = CRNET(num_classes=num_classes, num_digits=num_digits)\n",
        "    return model\n",
        "\n",
        "class MeterDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Pfad zur Label-Datei\n",
        "        label_path = os.path.join(self.label_dir, os.path.splitext(os.path.basename(img_path))[0] + '.txt')\n",
        "\n",
        "        if not os.path.exists(label_path):\n",
        "            raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
        "\n",
        "        # Lese die Labels\n",
        "        with open(label_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            digits = [int(line.split()[1]) for line in lines[:5]]  # Erste 5 Zeilen sind die Ziffern-Labels\n",
        "\n",
        "        # Konvertieren zu einer Liste von Ziffern\n",
        "        label = torch.tensor(digits, dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def load_data(data_path, batch_size=32, img_size=224):\n",
        "    print(\"Loading data...\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = MeterDataset(os.path.join(data_path, 'train/images'), os.path.join(data_path, 'train/labels'), transform=transform)\n",
        "    val_dataset = MeterDataset(os.path.join(data_path, 'val/images'), os.path.join(data_path, 'val/labels'), transform=transform)\n",
        "    test_dataset = MeterDataset(os.path.join(data_path, 'test/images'), os.path.join(data_path, 'test/labels'), transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(\"Data loaded.\")\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, device='cuda'):\n",
        "    best_model_wts = model.state_dict()\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = sum(criterion(output, labels[:, i]) for i, output in enumerate(outputs))\n",
        "            _, preds = torch.max(torch.stack(outputs, dim=1), 2)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects.double() / (len(train_loader.dataset) * labels.size(1))\n",
        "\n",
        "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = sum(criterion(output, labels[:, i]) for i, output in enumerate(outputs))\n",
        "                _, preds = torch.max(torch.stack(outputs, dim=1), 2)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                val_corrects += torch.sum(preds == labels)\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_corrects.double() / (len(val_loader.dataset) * labels.size(1))\n",
        "\n",
        "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = model.state_dict()\n",
        "\n",
        "    print(f'Best val Acc: {best_acc:.4f}')\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    data_path = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/yolov5_data'\n",
        "    num_classes = 10\n",
        "    num_digits = 5\n",
        "    batch_size = 32\n",
        "    num_epochs = 200\n",
        "    learning_rate = 0.0001  # Adjusted learning rate\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    train_loader, val_loader, test_loader = load_data(data_path, batch_size=batch_size, img_size=224)\n",
        "\n",
        "    model = crnet(num_classes=num_classes, num_digits=num_digits)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs, device=device)\n",
        "\n",
        "    torch.save(model.state_dict(), 'crnet_dd.pth')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "SaQg9TEGf9u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Training CRNET\n",
        "\n",
        "Anhand der folgenden Parameter in train_crnet.py wurde das Modelltraining über 16 Stunden durchgeführt:  \n",
        "\n",
        "    data_path = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/yolov5_data'\n",
        "\n",
        "    num_classes = 10\n",
        "\n",
        "    num_digits = 5\n",
        "\n",
        "    batch_size = 32\n",
        "\n",
        "    num_epochs = 200\n",
        "\n",
        "    learning_rate = 0.0001"
      ],
      "metadata": {
        "id": "8SNUPSBjgx-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Ausführung train_crnet.py im Terminal\n",
        "\n",
        "\n",
        "/usr/local/bin/python3 /Users/janriedel/Downloads/detector/CRNET_digit_detection/crnet.py\n",
        "janriedel@MacBook-Pro-von-Jan-2 detector % /usr/local/bin/python3 /Users/janriedel/Downloads/detector/CRNET_digit_detection/crnet.p\n",
        "y\n",
        "janriedel@MacBook-Pro-von-Jan-2 detector % /usr/local/bin/python3 /Users/janriedel/Downloads/detector/CRNET_digit_detection/train.c\n",
        "rnet.py\n",
        "Loading data...\n",
        "Data loaded.\n",
        "Epoch 0/199\n",
        "----------\n",
        "Train Loss: 10.7458 Acc: 0.1773\n",
        "Val Loss: 10.9406 Acc: 0.1528\n",
        "Epoch 1/199\n",
        "----------\n",
        "Train Loss: 10.6240 Acc: 0.1843\n",
        "Val Loss: 10.8332 Acc: 0.1544\n",
        "Epoch 2/199\n",
        "----------\n",
        "Train Loss: 10.6068 Acc: 0.1907\n",
        "Val Loss: 10.8460 Acc: 0.1448\n",
        "Epoch 3/199\n",
        "----------\n",
        "Train Loss: 10.6055 Acc: 0.1853\n",
        "Val Loss: 10.8282 Acc: 0.1536\n",
        "Epoch 4/199\n",
        "----------\n",
        "Train Loss: 10.5754 Acc: 0.1892\n",
        "Val Loss: 10.9268 Acc: 0.1488\n",
        "Epoch 5/199\n",
        "----------\n",
        "Train Loss: 10.5541 Acc: 0.1959\n",
        "Val Loss: 10.9138 Acc: 0.1544\n",
        "Epoch 6/199\n",
        "----------\n",
        "Train Loss: 10.4971 Acc: 0.2028\n",
        "Val Loss: 10.9022 Acc: 0.1568\n",
        "Epoch 7/199\n",
        "----------\n",
        "Train Loss: 10.4685 Acc: 0.2108\n",
        "Val Loss: 10.7977 Acc: 0.1824\n",
        "Epoch 8/199\n",
        "----------\n",
        "Train Loss: 10.2027 Acc: 0.2384\n",
        "Val Loss: 10.5033 Acc: 0.2032\n",
        "Epoch 9/199\n",
        "----------\n",
        "Train Loss: 9.4858 Acc: 0.2976\n",
        "Val Loss: 9.8820 Acc: 0.2616\n",
        "Epoch 10/199\n",
        "----------\n",
        "Train Loss: 8.8061 Acc: 0.3521\n",
        "Val Loss: 9.1169 Acc: 0.3272\n",
        "Epoch 11/199\n",
        "----------\n",
        "Train Loss: 8.0959 Acc: 0.4120\n",
        "Val Loss: 8.6259 Acc: 0.3832\n",
        "Epoch 12/199\n",
        "----------\n",
        "Train Loss: 7.4224 Acc: 0.4660\n",
        "Val Loss: 8.0940 Acc: 0.4240\n",
        "Epoch 13/199\n",
        "----------\n",
        "Train Loss: 6.6098 Acc: 0.5295\n",
        "Val Loss: 7.2257 Acc: 0.4816\n",
        "Epoch 14/199\n",
        "----------\n",
        "Train Loss: 5.9469 Acc: 0.5839\n",
        "Val Loss: 6.4322 Acc: 0.5624\n",
        "Epoch 15/199\n",
        "----------\n",
        "Train Loss: 5.3427 Acc: 0.6253\n",
        "Val Loss: 5.8911 Acc: 0.6040\n",
        "Epoch 16/199\n",
        "----------\n",
        "Train Loss: 4.6693 Acc: 0.6812\n",
        "Val Loss: 5.7007 Acc: 0.6160\n",
        "Epoch 17/199\n",
        "----------\n",
        "Train Loss: 4.2100 Acc: 0.7128\n",
        "Val Loss: 4.7486 Acc: 0.6736\n",
        "Epoch 18/199\n",
        "----------\n",
        "Train Loss: 3.6133 Acc: 0.7523\n",
        "Val Loss: 4.4879 Acc: 0.6920\n",
        "Epoch 19/199\n",
        "----------\n",
        "Train Loss: 3.0733 Acc: 0.7921\n",
        "Val Loss: 4.2740 Acc: 0.7184\n",
        "Epoch 20/199\n",
        "----------\n",
        "Train Loss: 2.7909 Acc: 0.8136\n",
        "Val Loss: 4.0623 Acc: 0.7344\n",
        "Epoch 21/199\n",
        "----------\n",
        "Train Loss: 2.4001 Acc: 0.8388\n",
        "Val Loss: 3.9335 Acc: 0.7528\n",
        "Epoch 22/199\n",
        "----------\n",
        "Train Loss: 2.1371 Acc: 0.8599\n",
        "Val Loss: 3.6985 Acc: 0.7584\n",
        "Epoch 23/199\n",
        "----------\n",
        "Train Loss: 1.6999 Acc: 0.8873\n",
        "Val Loss: 3.5565 Acc: 0.7704\n",
        "Epoch 24/199\n",
        "----------\n",
        "Train Loss: 1.6144 Acc: 0.8921\n",
        "Val Loss: 3.5986 Acc: 0.7832\n",
        "Epoch 25/199\n",
        "----------\n",
        "Train Loss: 1.3646 Acc: 0.9125\n",
        "Val Loss: 3.3614 Acc: 0.8016\n",
        "Epoch 26/199\n",
        "----------\n",
        "Train Loss: 1.2160 Acc: 0.9188\n",
        "Val Loss: 3.4549 Acc: 0.8144\n",
        "Epoch 27/199\n",
        "----------\n",
        "Train Loss: 1.0439 Acc: 0.9329\n",
        "Val Loss: 3.5099 Acc: 0.7960\n",
        "Epoch 28/199\n",
        "----------\n",
        "Train Loss: 0.9885 Acc: 0.9356\n",
        "Val Loss: 3.4834 Acc: 0.8032\n",
        "Epoch 29/199\n",
        "----------\n",
        "Train Loss: 0.8180 Acc: 0.9460\n",
        "Val Loss: 3.6607 Acc: 0.7984\n",
        "Epoch 30/199\n",
        "----------\n",
        "Train Loss: 0.8031 Acc: 0.9465\n",
        "Val Loss: 3.2917 Acc: 0.8096\n",
        "Epoch 31/199\n",
        "----------\n",
        "Train Loss: 0.6922 Acc: 0.9564\n",
        "Val Loss: 3.3820 Acc: 0.8248\n",
        "Epoch 32/199\n",
        "----------\n",
        "Train Loss: 0.6239 Acc: 0.9603\n",
        "Val Loss: 3.2555 Acc: 0.8352\n",
        "Epoch 33/199\n",
        "----------\n",
        "Train Loss: 0.6350 Acc: 0.9581\n",
        "Val Loss: 3.8473 Acc: 0.8096\n",
        "Epoch 34/199\n",
        "----------\n",
        "Train Loss: 0.4916 Acc: 0.9671\n",
        "Val Loss: 3.7236 Acc: 0.8272\n",
        "Epoch 35/199\n",
        "----------\n",
        "Train Loss: 0.4553 Acc: 0.9680\n",
        "Val Loss: 3.4018 Acc: 0.8368\n",
        "Epoch 36/199\n",
        "----------\n",
        "Train Loss: 0.4518 Acc: 0.9708\n",
        "Val Loss: 3.7118 Acc: 0.8280\n",
        "Epoch 37/199\n",
        "----------\n",
        "Train Loss: 0.3910 Acc: 0.9753\n",
        "Val Loss: 3.5979 Acc: 0.8360\n",
        "Epoch 38/199\n",
        "----------\n",
        "Train Loss: 0.3686 Acc: 0.9763\n",
        "Val Loss: 3.3338 Acc: 0.8360\n",
        "Epoch 39/199\n",
        "----------\n",
        "Train Loss: 0.3641 Acc: 0.9761\n",
        "Val Loss: 3.4003 Acc: 0.8368\n",
        "Epoch 40/199\n",
        "----------\n",
        "Train Loss: 0.3242 Acc: 0.9805\n",
        "Val Loss: 3.6923 Acc: 0.8240\n",
        "Epoch 41/199\n",
        "----------\n",
        "Train Loss: 0.2745 Acc: 0.9821\n",
        "Val Loss: 3.5391 Acc: 0.8472\n",
        "Epoch 42/199\n",
        "----------\n",
        "Train Loss: 0.3329 Acc: 0.9769\n",
        "Val Loss: 4.0197 Acc: 0.8320\n",
        "Epoch 43/199\n",
        "----------\n",
        "Train Loss: 0.2815 Acc: 0.9815\n",
        "Val Loss: 3.6645 Acc: 0.8528\n",
        "Epoch 44/199\n",
        "----------\n",
        "Train Loss: 0.2964 Acc: 0.9819\n",
        "Val Loss: 3.5667 Acc: 0.8456\n",
        "Epoch 45/199\n",
        "----------\n",
        "Train Loss: 0.2985 Acc: 0.9808\n",
        "Val Loss: 3.7410 Acc: 0.8408\n",
        "Epoch 46/199\n",
        "----------\n",
        "Train Loss: 0.2892 Acc: 0.9819\n",
        "Val Loss: 3.5132 Acc: 0.8272\n",
        "Epoch 47/199\n",
        "----------\n",
        "Train Loss: 0.2268 Acc: 0.9857\n",
        "Val Loss: 3.8584 Acc: 0.8392\n",
        "Epoch 48/199\n",
        "----------\n",
        "Train Loss: 0.2229 Acc: 0.9859\n",
        "Val Loss: 3.6166 Acc: 0.8384\n",
        "Epoch 49/199\n",
        "----------\n",
        "Train Loss: 0.2662 Acc: 0.9829\n",
        "Val Loss: 3.6957 Acc: 0.8416\n",
        "Epoch 50/199\n",
        "----------\n",
        "Train Loss: 0.2235 Acc: 0.9863\n",
        "Val Loss: 3.7006 Acc: 0.8488\n",
        "Epoch 51/199\n",
        "----------\n",
        "Train Loss: 0.1975 Acc: 0.9876\n",
        "Val Loss: 3.4276 Acc: 0.8536\n",
        "Epoch 52/199\n",
        "----------\n",
        "Train Loss: 0.1975 Acc: 0.9879\n",
        "Val Loss: 3.8105 Acc: 0.8520\n",
        "Epoch 53/199\n",
        "----------\n",
        "Train Loss: 0.2006 Acc: 0.9867\n",
        "Val Loss: 3.2930 Acc: 0.8632\n",
        "Epoch 54/199\n",
        "----------\n",
        "Train Loss: 0.2063 Acc: 0.9860\n",
        "Val Loss: 3.9365 Acc: 0.8368\n",
        "Epoch 55/199\n",
        "----------\n",
        "Train Loss: 0.1705 Acc: 0.9895\n",
        "Val Loss: 3.4254 Acc: 0.8624\n",
        "Epoch 56/199\n",
        "----------\n",
        "Train Loss: 0.1853 Acc: 0.9872\n",
        "Val Loss: 3.5504 Acc: 0.8552\n",
        "Epoch 57/199\n",
        "----------\n",
        "Train Loss: 0.1701 Acc: 0.9887\n",
        "Val Loss: 3.8119 Acc: 0.8592\n",
        "Epoch 58/199\n",
        "----------\n",
        "Train Loss: 0.1721 Acc: 0.9883\n",
        "Val Loss: 3.8438 Acc: 0.8560\n",
        "Epoch 59/199\n",
        "----------\n",
        "Train Loss: 0.1563 Acc: 0.9901\n",
        "Val Loss: 4.1307 Acc: 0.8440\n",
        "Epoch 60/199\n",
        "----------\n",
        "Train Loss: 0.1439 Acc: 0.9915\n",
        "Val Loss: 3.9639 Acc: 0.8416\n",
        "Epoch 61/199\n",
        "----------\n",
        "Train Loss: 0.1708 Acc: 0.9877\n",
        "Val Loss: 3.5430 Acc: 0.8632\n",
        "Epoch 62/199\n",
        "----------\n",
        "Train Loss: 0.1600 Acc: 0.9896\n",
        "Val Loss: 3.7694 Acc: 0.8560\n",
        "Epoch 63/199\n",
        "----------\n",
        "Train Loss: 0.1590 Acc: 0.9897\n",
        "Val Loss: 3.5022 Acc: 0.8536\n",
        "Epoch 64/199\n",
        "----------\n",
        "Train Loss: 0.1842 Acc: 0.9891\n",
        "Val Loss: 3.4556 Acc: 0.8424\n",
        "Epoch 65/199\n",
        "----------\n",
        "Train Loss: 0.1663 Acc: 0.9891\n",
        "Val Loss: 3.8181 Acc: 0.8408\n",
        "Epoch 66/199\n",
        "----------\n",
        "Train Loss: 0.1573 Acc: 0.9908\n",
        "Val Loss: 3.6829 Acc: 0.8536\n",
        "Epoch 67/199\n",
        "----------\n",
        "Train Loss: 0.1383 Acc: 0.9913\n",
        "Val Loss: 3.7802 Acc: 0.8576\n",
        "Epoch 68/199\n",
        "----------\n",
        "Train Loss: 0.1092 Acc: 0.9927\n",
        "Val Loss: 3.9376 Acc: 0.8504\n",
        "Epoch 69/199\n",
        "----------\n",
        "Train Loss: 0.1140 Acc: 0.9941\n",
        "Val Loss: 3.5327 Acc: 0.8600\n",
        "Epoch 70/199\n",
        "----------\n",
        "Train Loss: 0.0894 Acc: 0.9941\n",
        "Val Loss: 3.8637 Acc: 0.8600\n",
        "Epoch 71/199\n",
        "----------\n",
        "Train Loss: 0.1313 Acc: 0.9917\n",
        "Val Loss: 4.2060 Acc: 0.8480\n",
        "Epoch 72/199\n",
        "----------\n",
        "Train Loss: 0.0973 Acc: 0.9941\n",
        "Val Loss: 4.4529 Acc: 0.8352\n",
        "Epoch 73/199\n",
        "----------\n",
        "Train Loss: 0.0831 Acc: 0.9952\n",
        "Val Loss: 4.3934 Acc: 0.8424\n",
        "Epoch 74/199\n",
        "----------\n",
        "Train Loss: 0.1028 Acc: 0.9944\n",
        "Val Loss: 4.1003 Acc: 0.8472\n",
        "Epoch 75/199\n",
        "----------\n",
        "Train Loss: 0.1209 Acc: 0.9927\n",
        "Val Loss: 3.9172 Acc: 0.8472\n",
        "Epoch 76/199\n",
        "----------\n",
        "Train Loss: 0.1063 Acc: 0.9932\n",
        "Val Loss: 4.2988 Acc: 0.8464\n",
        "Epoch 77/199\n",
        "----------\n",
        "Train Loss: 0.1102 Acc: 0.9936\n",
        "Val Loss: 3.6447 Acc: 0.8680\n",
        "Epoch 78/199\n",
        "----------\n",
        "Train Loss: 0.0825 Acc: 0.9951\n",
        "Val Loss: 4.5085 Acc: 0.8472\n",
        "Epoch 79/199\n",
        "----------\n",
        "Train Loss: 0.1338 Acc: 0.9915\n",
        "Val Loss: 3.6724 Acc: 0.8536\n",
        "Epoch 80/199\n",
        "----------\n",
        "Train Loss: 0.1146 Acc: 0.9929\n",
        "Val Loss: 4.7838 Acc: 0.8512\n",
        "Epoch 81/199\n",
        "----------\n",
        "Train Loss: 0.1181 Acc: 0.9925\n",
        "Val Loss: 4.0236 Acc: 0.8584\n",
        "Epoch 82/199\n",
        "----------\n",
        "Train Loss: 0.1120 Acc: 0.9935\n",
        "Val Loss: 3.6883 Acc: 0.8560\n",
        "Epoch 83/199\n",
        "----------\n",
        "Train Loss: 0.0894 Acc: 0.9941\n",
        "Val Loss: 4.0497 Acc: 0.8584\n",
        "Epoch 84/199\n",
        "----------\n",
        "Train Loss: 0.0833 Acc: 0.9947\n",
        "Val Loss: 4.4444 Acc: 0.8656\n",
        "Epoch 85/199\n",
        "----------\n",
        "Train Loss: 0.1172 Acc: 0.9925\n",
        "Val Loss: 4.0381 Acc: 0.8264\n",
        "Epoch 86/199\n",
        "----------\n",
        "Train Loss: 0.1344 Acc: 0.9916\n",
        "Val Loss: 4.3261 Acc: 0.8448\n",
        "Epoch 87/199\n",
        "----------\n",
        "Train Loss: 0.0942 Acc: 0.9952\n",
        "Val Loss: 4.1724 Acc: 0.8496\n",
        "Epoch 88/199\n",
        "----------\n",
        "Train Loss: 0.0874 Acc: 0.9951\n",
        "Val Loss: 4.3929 Acc: 0.8464\n",
        "Epoch 89/199\n",
        "----------\n",
        "Train Loss: 0.1036 Acc: 0.9940\n",
        "Val Loss: 4.2739 Acc: 0.8536\n",
        "Epoch 90/199\n",
        "----------\n",
        "Train Loss: 0.0898 Acc: 0.9947\n",
        "Val Loss: 3.6251 Acc: 0.8616\n",
        "Epoch 91/199\n",
        "----------\n",
        "Train Loss: 0.1470 Acc: 0.9909\n",
        "Val Loss: 4.1515 Acc: 0.8424\n",
        "Epoch 92/199\n",
        "----------\n",
        "Train Loss: 0.1257 Acc: 0.9931\n",
        "Val Loss: 3.8642 Acc: 0.8568\n",
        "Epoch 93/199\n",
        "----------\n",
        "Train Loss: 0.0706 Acc: 0.9960\n",
        "Val Loss: 3.5689 Acc: 0.8736\n",
        "Epoch 94/199\n",
        "----------\n",
        "Train Loss: 0.0643 Acc: 0.9960\n",
        "Val Loss: 4.4681 Acc: 0.8456\n",
        "Epoch 95/199\n",
        "----------\n",
        "Train Loss: 0.0560 Acc: 0.9963\n",
        "Val Loss: 3.9015 Acc: 0.8600\n",
        "Epoch 96/199\n",
        "----------\n",
        "Train Loss: 0.0714 Acc: 0.9956\n",
        "Val Loss: 3.9695 Acc: 0.8520\n",
        "Epoch 97/199\n",
        "----------\n",
        "Train Loss: 0.0697 Acc: 0.9959\n",
        "Val Loss: 4.3993 Acc: 0.8568\n",
        "Epoch 98/199\n",
        "----------\n",
        "Train Loss: 0.0525 Acc: 0.9964\n",
        "Val Loss: 4.3262 Acc: 0.8480\n",
        "Epoch 99/199\n",
        "----------\n",
        "Train Loss: 0.0715 Acc: 0.9948\n",
        "Val Loss: 4.5708 Acc: 0.8472\n",
        "Epoch 100/199\n",
        "----------\n",
        "Train Loss: 0.0531 Acc: 0.9969\n",
        "Val Loss: 3.6358 Acc: 0.8688\n",
        "Epoch 101/199\n",
        "----------\n",
        "Train Loss: 0.0405 Acc: 0.9981\n",
        "Val Loss: 4.3622 Acc: 0.8728\n",
        "Epoch 102/199\n",
        "----------\n",
        "Train Loss: 0.0809 Acc: 0.9951\n",
        "Val Loss: 3.6977 Acc: 0.8648\n",
        "Epoch 103/199\n",
        "----------\n",
        "Train Loss: 0.0428 Acc: 0.9972\n",
        "Val Loss: 3.9639 Acc: 0.8592\n",
        "Epoch 104/199\n",
        "----------\n",
        "Train Loss: 0.0685 Acc: 0.9964\n",
        "Val Loss: 3.6257 Acc: 0.8712\n",
        "Epoch 105/199\n",
        "----------\n",
        "Train Loss: 0.0716 Acc: 0.9956\n",
        "Val Loss: 5.1643 Acc: 0.8480\n",
        "Epoch 106/199\n",
        "----------\n",
        "Train Loss: 0.1130 Acc: 0.9932\n",
        "Val Loss: 4.3079 Acc: 0.8464\n",
        "Epoch 107/199\n",
        "----------\n",
        "Train Loss: 0.1521 Acc: 0.9909\n",
        "Val Loss: 3.7304 Acc: 0.8640\n",
        "Epoch 108/199\n",
        "----------\n",
        "Train Loss: 0.0744 Acc: 0.9948\n",
        "Val Loss: 4.0700 Acc: 0.8656\n",
        "Epoch 109/199\n",
        "----------\n",
        "Train Loss: 0.0454 Acc: 0.9972\n",
        "Val Loss: 3.7969 Acc: 0.8712\n",
        "Epoch 110/199\n",
        "----------\n",
        "Train Loss: 0.0716 Acc: 0.9964\n",
        "Val Loss: 4.0396 Acc: 0.8560\n",
        "Epoch 111/199\n",
        "----------\n",
        "Train Loss: 0.0470 Acc: 0.9973\n",
        "Val Loss: 4.4005 Acc: 0.8512\n",
        "Epoch 112/199\n",
        "----------\n",
        "Train Loss: 0.0589 Acc: 0.9961\n",
        "Val Loss: 3.8871 Acc: 0.8592\n",
        "Epoch 113/199\n",
        "----------\n",
        "Train Loss: 0.0578 Acc: 0.9955\n",
        "Val Loss: 4.2340 Acc: 0.8560\n",
        "Epoch 114/199\n",
        "----------\n",
        "Train Loss: 0.0683 Acc: 0.9956\n",
        "Val Loss: 3.6663 Acc: 0.8536\n",
        "Epoch 115/199\n",
        "----------\n",
        "Train Loss: 0.0731 Acc: 0.9953\n",
        "Val Loss: 5.2244 Acc: 0.8392\n",
        "Epoch 116/199\n",
        "----------\n",
        "Train Loss: 0.1139 Acc: 0.9936\n",
        "Val Loss: 3.6931 Acc: 0.8640\n",
        "Epoch 117/199\n",
        "----------\n",
        "Train Loss: 0.1225 Acc: 0.9924\n",
        "Val Loss: 3.5568 Acc: 0.8704\n",
        "Epoch 118/199\n",
        "----------\n",
        "Train Loss: 0.0655 Acc: 0.9952\n",
        "Val Loss: 3.7940 Acc: 0.8544\n",
        "Epoch 119/199\n",
        "----------\n",
        "Train Loss: 0.0519 Acc: 0.9969\n",
        "Val Loss: 3.8384 Acc: 0.8664\n",
        "Epoch 120/199\n",
        "----------\n",
        "Train Loss: 0.0893 Acc: 0.9951\n",
        "Val Loss: 4.3974 Acc: 0.8536\n",
        "Epoch 121/199\n",
        "----------\n",
        "Train Loss: 0.0714 Acc: 0.9945\n",
        "Val Loss: 3.2843 Acc: 0.8696\n",
        "Epoch 122/199\n",
        "----------\n",
        "Train Loss: 0.0366 Acc: 0.9979\n",
        "Val Loss: 3.9227 Acc: 0.8696\n",
        "Epoch 123/199\n",
        "----------\n",
        "Train Loss: 0.0445 Acc: 0.9971\n",
        "Val Loss: 3.8344 Acc: 0.8640\n",
        "Epoch 124/199\n",
        "----------\n",
        "Train Loss: 0.0412 Acc: 0.9975\n",
        "Val Loss: 3.5687 Acc: 0.8640\n",
        "Epoch 125/199\n",
        "----------\n",
        "Train Loss: 0.0764 Acc: 0.9951\n",
        "Val Loss: 4.1810 Acc: 0.8736\n",
        "Epoch 126/199\n",
        "----------\n",
        "Train Loss: 0.0628 Acc: 0.9961\n",
        "Val Loss: 4.9766 Acc: 0.8528\n",
        "Epoch 127/199\n",
        "----------\n",
        "Train Loss: 0.0727 Acc: 0.9955\n",
        "Val Loss: 3.9725 Acc: 0.8616\n",
        "Epoch 128/199\n",
        "----------\n",
        "Train Loss: 0.0548 Acc: 0.9968\n",
        "Val Loss: 3.7859 Acc: 0.8680\n",
        "Epoch 129/199\n",
        "----------\n",
        "Train Loss: 0.0718 Acc: 0.9951\n",
        "Val Loss: 3.7838 Acc: 0.8720\n",
        "Epoch 130/199\n",
        "----------\n",
        "Train Loss: 0.0709 Acc: 0.9956\n",
        "Val Loss: 3.8620 Acc: 0.8744\n",
        "Epoch 131/199\n",
        "----------\n",
        "Train Loss: 0.0457 Acc: 0.9977\n",
        "Val Loss: 4.1133 Acc: 0.8568\n",
        "Epoch 132/199\n",
        "----------\n",
        "Train Loss: 0.0467 Acc: 0.9977\n",
        "Val Loss: 4.1614 Acc: 0.8648\n",
        "Epoch 133/199\n",
        "----------\n",
        "Train Loss: 0.0276 Acc: 0.9983\n",
        "Val Loss: 3.8793 Acc: 0.8696\n",
        "Epoch 134/199\n",
        "----------\n",
        "Train Loss: 0.0450 Acc: 0.9975\n",
        "Val Loss: 4.2290 Acc: 0.8640\n",
        "Epoch 135/199\n",
        "----------\n",
        "Train Loss: 0.0498 Acc: 0.9971\n",
        "Val Loss: 3.7941 Acc: 0.8656\n",
        "Epoch 136/199\n",
        "----------\n",
        "Train Loss: 0.0427 Acc: 0.9968\n",
        "Val Loss: 3.7803 Acc: 0.8824\n",
        "Epoch 137/199\n",
        "----------\n",
        "Train Loss: 0.0625 Acc: 0.9965\n",
        "Val Loss: 4.4408 Acc: 0.8664\n",
        "Epoch 138/199\n",
        "----------\n",
        "Train Loss: 0.0827 Acc: 0.9943\n",
        "Val Loss: 3.7804 Acc: 0.8672\n",
        "Epoch 139/199\n",
        "----------\n",
        "Train Loss: 0.0618 Acc: 0.9959\n",
        "Val Loss: 4.7025 Acc: 0.8568\n",
        "Epoch 140/199\n",
        "----------\n",
        "Train Loss: 0.0630 Acc: 0.9968\n",
        "Val Loss: 4.1623 Acc: 0.8720\n",
        "Epoch 141/199\n",
        "----------\n",
        "Train Loss: 0.0704 Acc: 0.9959\n",
        "Val Loss: 3.8103 Acc: 0.8600\n",
        "Epoch 142/199\n",
        "----------\n",
        "Train Loss: 0.0568 Acc: 0.9963\n",
        "Val Loss: 3.8282 Acc: 0.8720\n",
        "Epoch 143/199\n",
        "----------\n",
        "Train Loss: 0.0508 Acc: 0.9971\n",
        "Val Loss: 3.2581 Acc: 0.8712\n",
        "Epoch 144/199\n",
        "----------\n",
        "Train Loss: 0.0500 Acc: 0.9976\n",
        "Val Loss: 4.0524 Acc: 0.8840\n",
        "Epoch 145/199\n",
        "----------\n",
        "Train Loss: 0.0601 Acc: 0.9961\n",
        "Val Loss: 3.7267 Acc: 0.8704\n",
        "Epoch 146/199\n",
        "----------\n",
        "Train Loss: 0.0608 Acc: 0.9963\n",
        "Val Loss: 3.6881 Acc: 0.8640\n",
        "Epoch 147/199\n",
        "----------\n",
        "Train Loss: 0.0771 Acc: 0.9961\n",
        "Val Loss: 4.5519 Acc: 0.8600\n",
        "Epoch 148/199\n",
        "----------\n",
        "Train Loss: 0.0414 Acc: 0.9971\n",
        "Val Loss: 4.1598 Acc: 0.8688\n",
        "Epoch 149/199\n",
        "----------\n",
        "Train Loss: 0.0359 Acc: 0.9977\n",
        "Val Loss: 4.1538 Acc: 0.8776\n",
        "Epoch 150/199\n",
        "----------\n",
        "Train Loss: 0.0677 Acc: 0.9960\n",
        "Val Loss: 4.0481 Acc: 0.8608\n",
        "Epoch 151/199\n",
        "----------\n",
        "Train Loss: 0.0648 Acc: 0.9961\n",
        "Val Loss: 4.2970 Acc: 0.8648\n",
        "Epoch 152/199\n",
        "----------\n",
        "Train Loss: 0.0569 Acc: 0.9967\n",
        "Val Loss: 4.2493 Acc: 0.8712\n",
        "Epoch 153/199\n",
        "----------\n",
        "Train Loss: 0.0506 Acc: 0.9964\n",
        "Val Loss: 3.9948 Acc: 0.8744\n",
        "Epoch 154/199\n",
        "----------\n",
        "Train Loss: 0.0686 Acc: 0.9952\n",
        "Val Loss: 4.4103 Acc: 0.8600\n",
        "Epoch 155/199\n",
        "----------\n",
        "Train Loss: 0.0673 Acc: 0.9963\n",
        "Val Loss: 3.9269 Acc: 0.8664\n",
        "Epoch 156/199\n",
        "----------\n",
        "Train Loss: 0.0473 Acc: 0.9965\n",
        "Val Loss: 3.8031 Acc: 0.8736\n",
        "Epoch 157/199\n",
        "----------\n",
        "Train Loss: 0.0511 Acc: 0.9975\n",
        "Val Loss: 3.9042 Acc: 0.8656\n",
        "Epoch 158/199\n",
        "----------\n",
        "Train Loss: 0.0378 Acc: 0.9980\n",
        "Val Loss: 3.8877 Acc: 0.8768\n",
        "Epoch 159/199\n",
        "----------\n",
        "Train Loss: 0.0764 Acc: 0.9967\n",
        "Val Loss: 3.7665 Acc: 0.8728\n",
        "Epoch 160/199\n",
        "----------\n",
        "Train Loss: 0.0606 Acc: 0.9967\n",
        "Val Loss: 3.9160 Acc: 0.8688\n",
        "Epoch 161/199\n",
        "----------\n",
        "Train Loss: 0.0464 Acc: 0.9973\n",
        "Val Loss: 4.1750 Acc: 0.8768\n",
        "Epoch 162/199\n",
        "----------\n",
        "Train Loss: 0.0500 Acc: 0.9972\n",
        "Val Loss: 3.9411 Acc: 0.8728\n",
        "Epoch 163/199\n",
        "----------\n",
        "Train Loss: 0.0344 Acc: 0.9984\n",
        "Val Loss: 3.9328 Acc: 0.8552\n",
        "Epoch 164/199\n",
        "----------\n",
        "Train Loss: 0.0355 Acc: 0.9972\n",
        "Val Loss: 3.8958 Acc: 0.8736\n",
        "Epoch 165/199\n",
        "----------\n",
        "Train Loss: 0.0334 Acc: 0.9980\n",
        "Val Loss: 3.7026 Acc: 0.8696\n",
        "Epoch 166/199\n",
        "----------\n",
        "Train Loss: 0.0374 Acc: 0.9973\n",
        "Val Loss: 4.3964 Acc: 0.8656\n",
        "Epoch 167/199\n",
        "----------\n",
        "Train Loss: 0.0649 Acc: 0.9967\n",
        "Val Loss: 4.2886 Acc: 0.8656\n",
        "Epoch 168/199\n",
        "----------\n",
        "Train Loss: 0.0398 Acc: 0.9971\n",
        "Val Loss: 4.5983 Acc: 0.8568\n",
        "Epoch 169/199\n",
        "----------\n",
        "Train Loss: 0.0386 Acc: 0.9977\n",
        "Val Loss: 4.2096 Acc: 0.8616\n",
        "Epoch 170/199\n",
        "----------\n",
        "Train Loss: 0.0536 Acc: 0.9967\n",
        "Val Loss: 4.2267 Acc: 0.8664\n",
        "Epoch 171/199\n",
        "----------\n",
        "Train Loss: 0.0766 Acc: 0.9955\n",
        "Val Loss: 4.1794 Acc: 0.8568\n",
        "Epoch 172/199\n",
        "----------\n",
        "Train Loss: 0.0549 Acc: 0.9976\n",
        "Val Loss: 3.5780 Acc: 0.8640\n",
        "Epoch 173/199\n",
        "----------\n",
        "Train Loss: 0.0482 Acc: 0.9975\n",
        "Val Loss: 3.9927 Acc: 0.8640\n",
        "Epoch 174/199\n",
        "----------\n",
        "Train Loss: 0.0469 Acc: 0.9969\n",
        "Val Loss: 4.3735 Acc: 0.8608\n",
        "Epoch 175/199\n",
        "----------\n",
        "Train Loss: 0.0585 Acc: 0.9971\n",
        "Val Loss: 3.7796 Acc: 0.8800\n",
        "Epoch 176/199\n",
        "----------\n",
        "Train Loss: 0.0360 Acc: 0.9976\n",
        "Val Loss: 3.8465 Acc: 0.8672\n",
        "Epoch 177/199\n",
        "----------\n",
        "Train Loss: 0.0754 Acc: 0.9953\n",
        "Val Loss: 3.7255 Acc: 0.8704\n",
        "Epoch 178/199\n",
        "----------\n",
        "Train Loss: 0.0521 Acc: 0.9973\n",
        "Val Loss: 3.4099 Acc: 0.8688\n",
        "Epoch 179/199\n",
        "----------\n",
        "Train Loss: 0.0438 Acc: 0.9972\n",
        "Val Loss: 3.9390 Acc: 0.8680\n",
        "Epoch 180/199\n",
        "----------\n",
        "Train Loss: 0.0390 Acc: 0.9969\n",
        "Val Loss: 3.8571 Acc: 0.8784\n",
        "Epoch 181/199\n",
        "----------\n",
        "Train Loss: 0.0653 Acc: 0.9967\n",
        "Val Loss: 3.6765 Acc: 0.8656\n",
        "Epoch 182/199\n",
        "----------\n",
        "Train Loss: 0.0663 Acc: 0.9965\n",
        "Val Loss: 4.1639 Acc: 0.8536\n",
        "Epoch 183/199\n",
        "----------\n",
        "Train Loss: 0.0617 Acc: 0.9968\n",
        "Val Loss: 3.6442 Acc: 0.8744\n",
        "Epoch 184/199\n",
        "----------\n",
        "Train Loss: 0.0390 Acc: 0.9975\n",
        "Val Loss: 3.6672 Acc: 0.8760\n",
        "Epoch 185/199\n",
        "----------\n",
        "Train Loss: 0.0470 Acc: 0.9971\n",
        "Val Loss: 4.3677 Acc: 0.8672\n",
        "Epoch 186/199\n",
        "----------\n",
        "Train Loss: 0.0451 Acc: 0.9973\n",
        "Val Loss: 4.0510 Acc: 0.8648\n",
        "Epoch 187/199\n",
        "----------\n",
        "Train Loss: 0.0632 Acc: 0.9961\n",
        "Val Loss: 3.4779 Acc: 0.8864\n",
        "Epoch 188/199\n",
        "----------\n",
        "Train Loss: 0.0433 Acc: 0.9973\n",
        "Val Loss: 4.0078 Acc: 0.8672\n",
        "Epoch 189/199\n",
        "----------\n",
        "Train Loss: 0.0566 Acc: 0.9976\n",
        "Val Loss: 3.6331 Acc: 0.8712\n",
        "Epoch 190/199\n",
        "----------\n",
        "Train Loss: 0.0438 Acc: 0.9973\n",
        "Val Loss: 3.6330 Acc: 0.8832\n",
        "Epoch 191/199\n",
        "----------\n",
        "Train Loss: 0.0245 Acc: 0.9980\n",
        "Val Loss: 4.1123 Acc: 0.8544\n",
        "Epoch 192/199\n",
        "----------\n",
        "Train Loss: 0.0239 Acc: 0.9985\n",
        "Val Loss: 3.6755 Acc: 0.8784\n",
        "Epoch 193/199\n",
        "----------\n",
        "Train Loss: 0.0356 Acc: 0.9984\n",
        "Val Loss: 3.7750 Acc: 0.8760\n",
        "Epoch 194/199\n",
        "----------\n",
        "Train Loss: 0.0283 Acc: 0.9976\n",
        "Val Loss: 4.2314 Acc: 0.8672\n",
        "Epoch 195/199\n",
        "----------\n",
        "Train Loss: 0.0367 Acc: 0.9976\n",
        "Val Loss: 3.9438 Acc: 0.8768\n",
        "Epoch 196/199\n",
        "----------\n",
        "Train Loss: 0.0232 Acc: 0.9987\n",
        "Val Loss: 4.0174 Acc: 0.8648\n",
        "Epoch 197/199\n",
        "----------\n",
        "Train Loss: 0.0547 Acc: 0.9980\n",
        "Val Loss: 3.5034 Acc: 0.8744\n",
        "Epoch 198/199\n",
        "----------\n",
        "Train Loss: 0.0161 Acc: 0.9989\n",
        "Val Loss: 4.1435 Acc: 0.8680\n",
        "Epoch 199/199\n",
        "----------\n",
        "Train Loss: 0.0219 Acc: 0.9987\n",
        "Val Loss: 3.9682 Acc: 0.8800\n",
        "Best val Acc: 0.8864"
      ],
      "metadata": {
        "id": "C16fVNExgxqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um die hier dargestellt Accuracy des Modells zu visualisieren, wurde das folgende Skript erstellt. Dieses basierte auf einer txt-Datei, die händisch anhand des o.g. Terminal-Outputs erstellt wurde."
      ],
      "metadata": {
        "id": "dyhEpkPROGN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pfad zur Datei\n",
        "file_path = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/crnet_train_result.txt'\n",
        "\n",
        "# Listen für Epochen, Trainingsgenauigkeit und Validierungsgenauigkeit\n",
        "epochs = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "# Datei einlesen und Werte extrahieren\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        if 'Epoch' in line:\n",
        "            epoch = int(line.strip().split()[1].split('/')[0])\n",
        "            if epoch > 0:  # Epoche 0 überspringen\n",
        "                epochs.append(epoch)\n",
        "        elif 'Train Loss' in line:\n",
        "            train_accuracy = float(line.strip().split('Acc: ')[-1])\n",
        "            train_acc.append(train_accuracy)\n",
        "        elif 'Val Loss' in line:\n",
        "            val_accuracy = float(line.strip().split('Acc: ')[-1])\n",
        "            val_acc.append(val_accuracy)\n",
        "\n",
        "# Sicherstellen, dass die Länge der Listen 199 beträgt\n",
        "assert len(epochs) == 199, \"Epochs list length is not 199.\"\n",
        "assert len(train_acc) == 199, \"Train accuracy list length is not 199.\"\n",
        "assert len(val_acc) == 199, \"Validation accuracy list length is not 199.\"\n",
        "\n",
        "# Plot erstellen\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_acc, 'b', label='Train Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r--', label='Validation Accuracy')\n",
        "plt.title('Model Accuracy CR-Net')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim([0, 1])\n",
        "plt.grid(True)\n",
        "\n",
        "# Diagramm speichern\n",
        "plt.savefig('/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/model_accuracy_CRNet.png')\n",
        "\n",
        "# Diagramm anzeigen\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kWT0m3IcONwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Hochladen der Modelldatei drnet_dd.pth\n",
        "###Um die Datei 'crnet_dd.pth' ('/Users/janriedel/Downloads/detector/crnet_dd.pth') mit 2,4 GB zur Abgabe zur Verfügung stellen zu können, wurde die Erweiterung 'Git Large Files' (https://git-lfs.com/ -> '/Users/janriedel/Downloads/detector/git-lfs-3.5.1') installiert und ausgeführt.\n",
        "\n",
        "\n",
        "cd /Users/janriedel/Documents/GitHub/project-st24-nk_jr_js\n",
        "\n",
        "git lfs track \"*.pth\"\n",
        "\n",
        "git add .gitattributes\n",
        "git commit -m \"Track .pth files with LFS\"\n",
        "\n",
        "cp /Users/janriedel/Downloads/detector/crnet_dd.pth /Users/janriedel/Documents/GitHub/project-st24-nk_jr_js\n",
        "\n",
        "git add crnet_dd.pth\n",
        "git commit -m \"Add crnet_dd.pth file\"\n",
        "git push origin main\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jca2O14N9_Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Predictions Neue Bilder / Evaluation Trainingsmodell\n",
        "\n",
        "Das trainierte Modell wurde durch das folgende Skript getestet. Testgrundlage waren die Testbilder im Ordner 'yolov5_data'."
      ],
      "metadata": {
        "id": "OtwF7m5lhERW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Predictions Bilder Test und speichern der Predictions 'prediction_test.py'\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Definiere die CRNET-Klasse und die crnet-Funktion\n",
        "class CRNET(nn.Module):\n",
        "    def __init__(self, num_classes=10, num_digits=5):\n",
        "        super(CRNET, self).__init__()\n",
        "        self.num_digits = num_digits\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifiers = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        ) for _ in range(num_digits)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        outputs = [classifier(x) for classifier in self.classifiers]\n",
        "        return outputs\n",
        "\n",
        "def crnet(num_classes=10, num_digits=5):\n",
        "    model = CRNET(num_classes=num_classes, num_digits=num_digits)\n",
        "    return model\n",
        "\n",
        "# Funktion zum Laden des Modells\n",
        "def load_model(model_path, num_classes=10, num_digits=5):\n",
        "    model = crnet(num_classes=num_classes, num_digits=num_digits)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    return model\n",
        "\n",
        "# Funktion zur Vorhersage des Zählerstands\n",
        "def predict_meter_reading(model, image_path, device='cpu'):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        predictions = [torch.argmax(output, dim=1).item() for output in outputs]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Pfade\n",
        "model_path = '/Users/janriedel/Downloads/detector/crnet_dd.pth'\n",
        "test_images_dir = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/yolov5_data/test/images'\n",
        "output_file = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/predictions.txt'\n",
        "\n",
        "# Laden des Modells\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Vorhersagen für alle Testbilder\n",
        "results = []\n",
        "for image_name in os.listdir(test_images_dir):\n",
        "    if image_name.endswith(('.jpg', '.jpeg', '.png')):\n",
        "        image_path = os.path.join(test_images_dir, image_name)\n",
        "        predictions = predict_meter_reading(model, image_path)\n",
        "        results.append((image_name, predictions))\n",
        "\n",
        "# Speichern der Ergebnisse\n",
        "with open(output_file, 'w') as f:\n",
        "    for image_name, predictions in results:\n",
        "        f.write(f\"{image_name}: {''.join(map(str, predictions))}\\n\")\n",
        "\n",
        "print(f\"Predictions saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "Wcume8u-hJu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Sortierung der Predictions 'sort_predictions_test.py'\n",
        "\n",
        "# Datei zum Lesen und Sortieren der Vorhersagen\n",
        "input_file = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/predictions.txt'\n",
        "output_file = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/predictions_sorted.txt'\n",
        "\n",
        "# Funktion zum Extrahieren der Nummer aus dem Dateinamen\n",
        "def extract_number(filename):\n",
        "    import re\n",
        "    match = re.search(r'meter(\\d+)', filename)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "# Lesen der Vorhersagen aus der Datei\n",
        "with open(input_file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Sortieren der Vorhersagen nach Dateinummer\n",
        "sorted_lines = sorted(lines, key=lambda x: extract_number(x.split(':')[0]))\n",
        "\n",
        "# Schreiben der sortierten Vorhersagen in die Ausgabedatei\n",
        "with open(output_file, 'w') as f:\n",
        "    for line in sorted_lines:\n",
        "        f.write(line)\n",
        "\n",
        "print(f\"Sorted predictions saved to {output_file}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1DolniRKhKNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Abgleich der Predictions 'compare_predictions_test.py'\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Pfade definieren\n",
        "predictions_file = '/Users/janriedel/Downloads/detector/results/predictions_sorted.txt'\n",
        "labels_dir = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/yolov5_data/test/labels'\n",
        "output_file = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/results_check.txt'\n",
        "\n",
        "# Funktion zum Extrahieren der Nummer aus dem Dateinamen\n",
        "def extract_number(filename):\n",
        "    match = re.search(r'meter(\\d+)', filename)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "# Funktion zum Lesen der tatsächlichen Labels aus einer Label-Datei\n",
        "def read_labels(label_file):\n",
        "    with open(label_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    labels = [line.split()[1] for line in lines[:5]]  # Nur die ersten 5 Zeilen\n",
        "    return labels\n",
        "\n",
        "# Lesen der Vorhersagen aus der Datei\n",
        "with open(predictions_file, 'r') as f:\n",
        "    predictions = f.readlines()\n",
        "\n",
        "total_predictions = 0\n",
        "correct_predictions = 0\n",
        "\n",
        "# Ergebnisvergleich und Schreiben in die Ausgabedatei\n",
        "with open(output_file, 'w') as f:\n",
        "    for line in predictions:\n",
        "        image_name, predicted_reading = line.strip().split(': ')\n",
        "        predicted_digits = list(predicted_reading)\n",
        "\n",
        "        # Pfad zur entsprechenden Label-Datei\n",
        "        label_file = os.path.join(labels_dir, image_name.replace('.jpg', '.txt'))\n",
        "\n",
        "        if os.path.exists(label_file):\n",
        "            actual_digits = read_labels(label_file)\n",
        "            result = f\"{image_name}: Predicted - {predicted_digits}, Actual - {actual_digits}\"\n",
        "\n",
        "            # Überprüfung der Korrektheit\n",
        "            if predicted_digits == actual_digits:\n",
        "                result += \" - Correct\\n\"\n",
        "                correct_predictions += 1\n",
        "            else:\n",
        "                incorrect_digits = [f\"digit {i+1}\" for i in range(len(predicted_digits)) if predicted_digits[i] != actual_digits[i]]\n",
        "                result += f\" - Incorrect ({', '.join(incorrect_digits)})\\n\"\n",
        "            total_predictions += 1\n",
        "        else:\n",
        "            result = f\"{image_name}: Predicted - {predicted_digits}, Actual - File not found\\n\"\n",
        "\n",
        "        f.write(result)\n",
        "\n",
        "# Berechnung des Prozentsatzes\n",
        "accuracy_percentage = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
        "\n",
        "# Schreiben der Zusammenfassung in die Ausgabedatei\n",
        "with open(output_file, 'a') as f:  # 'a' für append\n",
        "    f.write(f\"\\nTotal correct predictions: {correct_predictions}\\n\")\n",
        "    f.write(f\"Accuracy: {accuracy_percentage:.2f}%\\n\")\n",
        "\n",
        "print(f\"Results saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "I2pc0flrhRHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Ergebnisse Test CRNET-Trainingsmodell"
      ],
      "metadata": {
        "id": "duvLtPRRh96r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### check_results.txt\n",
        "\n",
        "\n",
        "meter0951.jpg: Predicted - ['1', '2', '7', '2', '8'], Actual - ['1', '2', '7', '2', '8'] - Correct\n",
        "meter0952.jpg: Predicted - ['3', '3', '8', '8', '5'], Actual - ['3', '3', '8', '8', '5'] - Correct\n",
        "meter0953.jpg: Predicted - ['1', '3', '0', '2', '5'], Actual - ['1', '9', '0', '2', '5'] - Incorrect (digit 2)\n",
        "meter0954.jpg: Predicted - ['0', '3', '6', '8', '9'], Actual - ['0', '3', '6', '8', '9'] - Correct\n",
        "meter0955.jpg: Predicted - ['0', '0', '0', '0', '1'], Actual - ['0', '0', '0', '0', '1'] - Correct\n",
        "meter0956.jpg: Predicted - ['1', '6', '6', '8', '5'], Actual - ['1', '6', '6', '8', '5'] - Correct\n",
        "meter0957.jpg: Predicted - ['0', '3', '0', '2', '3'], Actual - ['0', '3', '0', '2', '3'] - Correct\n",
        "meter0958.jpg: Predicted - ['0', '8', '4', '9', '1'], Actual - ['0', '8', '4', '9', '1'] - Correct\n",
        "meter0959.jpg: Predicted - ['0', '4', '2', '2', '0'], Actual - ['0', '4', '2', '2', '0'] - Correct\n",
        "meter0960.jpg: Predicted - ['2', '6', '1', '4', '2'], Actual - ['2', '6', '7', '4', '3'] - Incorrect (digit 3, digit 5)\n",
        "meter0961.jpg: Predicted - ['0', '7', '3', '5', '3'], Actual - ['0', '7', '3', '5', '3'] - Correct\n",
        "meter0962.jpg: Predicted - ['1', '2', '8', '4', '0'], Actual - ['1', '2', '8', '4', '0'] - Correct\n",
        "meter0963.jpg: Predicted - ['1', '3', '1', '8', '4'], Actual - ['1', '3', '7', '8', '6'] - Incorrect (digit 3, digit 5)\n",
        "meter0964.jpg: Predicted - ['1', '3', '8', '3', '3'], Actual - ['1', '3', '8', '3', '3'] - Correct\n",
        "meter0965.jpg: Predicted - ['0', '8', '3', '6', '2'], Actual - ['0', '8', '3', '0', '2'] - Incorrect (digit 4)\n",
        "meter0966.jpg: Predicted - ['1', '6', '1', '7', '8'], Actual - ['1', '6', '1', '7', '9'] - Incorrect (digit 5)\n",
        "meter0967.jpg: Predicted - ['0', '8', '2', '7', '6'], Actual - ['0', '8', '2', '7', '6'] - Correct\n",
        "meter0968.jpg: Predicted - ['0', '5', '3', '2', '2'], Actual - ['0', '5', '3', '2', '2'] - Correct\n",
        "meter0969.jpg: Predicted - ['4', '1', '8', '4', '3'], Actual - ['4', '1', '8', '4', '3'] - Correct\n",
        "meter0970.jpg: Predicted - ['1', '7', '4', '8', '5'], Actual - ['7', '7', '4', '5', '1'] - Incorrect (digit 1, digit 4, digit 5)\n",
        "meter0971.jpg: Predicted - ['1', '0', '1', '9', '9'], Actual - ['1', '0', '7', '9', '9'] - Incorrect (digit 3)\n",
        "meter0972.jpg: Predicted - ['1', '0', '6', '1', '3'], Actual - ['1', '0', '6', '1', '3'] - Correct\n",
        "meter0973.jpg: Predicted - ['0', '5', '9', '6', '2'], Actual - ['0', '5', '9', '6', '2'] - Correct\n",
        "meter0974.jpg: Predicted - ['0', '6', '3', '9', '1'], Actual - ['0', '6', '3', '9', '1'] - Correct\n",
        "meter0975.jpg: Predicted - ['1', '5', '5', '5', '4'], Actual - ['4', '5', '5', '5', '4'] - Incorrect (digit 1)\n",
        "meter0976.jpg: Predicted - ['0', '0', '0', '0', '3'], Actual - ['0', '0', '9', '0', '3'] - Incorrect (digit 3)\n",
        "meter0977.jpg: Predicted - ['1', '5', '6', '6', '5'], Actual - ['1', '5', '6', '0', '5'] - Incorrect (digit 4)\n",
        "meter0978.jpg: Predicted - ['0', '0', '0', '9', '4'], Actual - ['0', '5', '2', '6', '5'] - Incorrect (digit 2, digit 3, digit 4, digit 5)\n",
        "meter0979.jpg: Predicted - ['0', '0', '6', '8', '9'], Actual - ['0', '0', '8', '8', '9'] - Incorrect (digit 3)\n",
        "meter0980.jpg: Predicted - ['0', '1', '4', '5', '8'], Actual - ['0', '1', '4', '5', '9'] - Incorrect (digit 5)\n",
        "meter0981.jpg: Predicted - ['0', '3', '1', '3', '4'], Actual - ['0', '3', '1', '3', '4'] - Correct\n",
        "meter0982.jpg: Predicted - ['0', '6', '9', '6', '8'], Actual - ['0', '6', '9', '6', '8'] - Correct\n",
        "meter0983.jpg: Predicted - ['1', '9', '0', '1', '4'], Actual - ['0', '9', '0', '1', '4'] - Incorrect (digit 1)\n",
        "meter0984.jpg: Predicted - ['1', '9', '1', '8', '6'], Actual - ['0', '9', '1', '6', '5'] - Incorrect (digit 1, digit 4, digit 5)\n",
        "meter0985.jpg: Predicted - ['0', '0', '1', '9', '6'], Actual - ['0', '0', '1', '9', '4'] - Incorrect (digit 5)\n",
        "meter0986.jpg: Predicted - ['0', '6', '2', '4', '4'], Actual - ['0', '6', '7', '3', '3'] - Incorrect (digit 3, digit 4, digit 5)\n",
        "meter0987.jpg: Predicted - ['2', '2', '2', '3', '6'], Actual - ['2', '2', '2', '3', '6'] - Correct\n",
        "meter0988.jpg: Predicted - ['1', '7', '1', '1', '9'], Actual - ['1', '7', '1', '7', '8'] - Incorrect (digit 4, digit 5)\n",
        "meter0989.jpg: Predicted - ['0', '5', '3', '4', '6'], Actual - ['0', '5', '3', '4', '6'] - Correct\n",
        "meter0990.jpg: Predicted - ['1', '4', '6', '6', '2'], Actual - ['1', '4', '6', '6', '2'] - Correct\n",
        "meter0991.jpg: Predicted - ['1', '4', '3', '7', '4'], Actual - ['1', '4', '3', '7', '4'] - Correct\n",
        "meter0992.jpg: Predicted - ['2', '4', '9', '5', '1'], Actual - ['2', '4', '9', '5', '1'] - Correct\n",
        "meter0993.jpg: Predicted - ['1', '0', '3', '0', '3'], Actual - ['1', '0', '3', '0', '3'] - Correct\n",
        "meter0994.jpg: Predicted - ['1', '3', '1', '7', '8'], Actual - ['1', '3', '1', '7', '8'] - Correct\n",
        "meter0995.jpg: Predicted - ['0', '0', '0', '0', '4'], Actual - ['4', '0', '0', '8', '0'] - Incorrect (digit 1, digit 4, digit 5)\n",
        "meter0996.jpg: Predicted - ['0', '7', '1', '9', '6'], Actual - ['0', '1', '1', '9', '5'] - Incorrect (digit 2, digit 5)\n",
        "meter0997.jpg: Predicted - ['0', '5', '7', '3', '3'], Actual - ['0', '5', '7', '3', '3'] - Correct\n",
        "meter0998.jpg: Predicted - ['0', '3', '0', '5', '9'], Actual - ['0', '2', '0', '5', '3'] - Incorrect (digit 2, digit 5)\n",
        "meter0999.jpg: Predicted - ['1', '7', '5', '5', '3'], Actual - ['1', '1', '5', '5', '3'] - Incorrect (digit 2)\n",
        "meter1000.jpg: Predicted - ['0', '9', '6', '9', '1'], Actual - ['0', '9', '6', '9', '1'] - Correct\n",
        "meter1001.jpg: Predicted - ['0', '5', '2', '4', '6'], Actual - ['0', '5', '2', '4', '6'] - Correct\n",
        "meter1002.jpg: Predicted - ['0', '5', '3', '8', '1'], Actual - ['0', '5', '3', '8', '2'] - Incorrect (digit 5)\n",
        "meter1003.jpg: Predicted - ['0', '5', '2', '4', '6'], Actual - ['0', '5', '2', '4', '6'] - Correct\n",
        "meter1004.jpg: Predicted - ['1', '0', '2', '1', '1'], Actual - ['1', '0', '2', '1', '3'] - Incorrect (digit 5)\n",
        "meter1005.jpg: Predicted - ['0', '9', '6', '9', '9'], Actual - ['0', '9', '6', '9', '9'] - Correct\n",
        "meter1006.jpg: Predicted - ['0', '9', '4', '4', '4'], Actual - ['0', '9', '4', '4', '4'] - Correct\n",
        "meter1007.jpg: Predicted - ['0', '9', '0', '7', '2'], Actual - ['0', '9', '0', '7', '2'] - Correct\n",
        "meter1008.jpg: Predicted - ['1', '1', '9', '8', '2'], Actual - ['1', '1', '9', '8', '2'] - Correct\n",
        "meter1009.jpg: Predicted - ['1', '3', '8', '4', '7'], Actual - ['1', '3', '8', '4', '7'] - Correct\n",
        "meter1010.jpg: Predicted - ['1', '7', '5', '1', '7'], Actual - ['1', '0', '3', '1', '7'] - Incorrect (digit 2, digit 3)\n",
        "meter1011.jpg: Predicted - ['2', '5', '1', '3', '5'], Actual - ['2', '2', '1', '3', '4'] - Incorrect (digit 2, digit 5)\n",
        "meter1012.jpg: Predicted - ['2', '2', '1', '9', '5'], Actual - ['2', '2', '7', '9', '5'] - Incorrect (digit 3)\n",
        "meter1013.jpg: Predicted - ['1', '1', '4', '6', '1'], Actual - ['3', '1', '4', '5', '1'] - Incorrect (digit 1, digit 4)\n",
        "meter1014.jpg: Predicted - ['1', '4', '5', '3', '1'], Actual - ['1', '4', '5', '3', '1'] - Correct\n",
        "meter1015.jpg: Predicted - ['1', '8', '8', '5', '6'], Actual - ['1', '8', '8', '5', '8'] - Incorrect (digit 5)\n",
        "meter1016.jpg: Predicted - ['1', '0', '1', '2', '8'], Actual - ['1', '0', '1', '3', '9'] - Incorrect (digit 4, digit 5)\n",
        "meter1017.jpg: Predicted - ['0', '8', '2', '8', '9'], Actual - ['0', '8', '2', '0', '9'] - Incorrect (digit 4)\n",
        "meter1018.jpg: Predicted - ['4', '7', '6', '1', '4'], Actual - ['4', '7', '6', '0', '5'] - Incorrect (digit 4, digit 5)\n",
        "meter1019.jpg: Predicted - ['1', '0', '5', '9', '2'], Actual - ['6', '0', '5', '9', '2'] - Incorrect (digit 1)\n",
        "meter1020.jpg: Predicted - ['3', '1', '6', '5', '4'], Actual - ['3', '1', '6', '5', '7'] - Incorrect (digit 5)\n",
        "meter1021.jpg: Predicted - ['0', '7', '0', '5', '8'], Actual - ['8', '7', '0', '5', '0'] - Incorrect (digit 1, digit 5)\n",
        "meter1022.jpg: Predicted - ['2', '3', '3', '2', '2'], Actual - ['2', '3', '3', '2', '2'] - Correct\n",
        "meter1023.jpg: Predicted - ['0', '7', '8', '6', '1'], Actual - ['0', '7', '8', '6', '7'] - Incorrect (digit 5)\n",
        "meter1024.jpg: Predicted - ['0', '3', '3', '4', '8'], Actual - ['0', '3', '3', '8', '6'] - Incorrect (digit 4, digit 5)\n",
        "meter1025.jpg: Predicted - ['1', '0', '9', '4', '6'], Actual - ['1', '0', '9', '4', '6'] - Correct\n",
        "meter1026.jpg: Predicted - ['1', '9', '9', '2', '0'], Actual - ['1', '9', '9', '2', '0'] - Correct\n",
        "meter1027.jpg: Predicted - ['0', '6', '0', '6', '0'], Actual - ['0', '6', '0', '6', '0'] - Correct\n",
        "meter1028.jpg: Predicted - ['1', '4', '2', '0', '6'], Actual - ['1', '4', '2', '0', '6'] - Correct\n",
        "meter1029.jpg: Predicted - ['0', '9', '2', '8', '3'], Actual - ['0', '3', '3', '8', '3'] - Incorrect (digit 2, digit 3)\n",
        "meter1030.jpg: Predicted - ['0', '6', '1', '9', '2'], Actual - ['0', '6', '1', '9', '7'] - Incorrect (digit 5)\n",
        "meter1031.jpg: Predicted - ['0', '6', '5', '6', '9'], Actual - ['0', '6', '5', '6', '9'] - Correct\n",
        "meter1032.jpg: Predicted - ['1', '4', '2', '7', '9'], Actual - ['1', '4', '2', '7', '9'] - Correct\n",
        "meter1033.jpg: Predicted - ['1', '0', '9', '2', '2'], Actual - ['1', '0', '9', '2', '2'] - Correct\n",
        "meter1034.jpg: Predicted - ['1', '1', '7', '7', '5'], Actual - ['1', '1', '7', '7', '5'] - Correct\n",
        "meter1035.jpg: Predicted - ['1', '4', '3', '7', '5'], Actual - ['1', '4', '3', '7', '5'] - Correct\n",
        "meter1036.jpg: Predicted - ['1', '0', '9', '3', '5'], Actual - ['2', '0', '9', '3', '5'] - Incorrect (digit 1)\n",
        "meter1037.jpg: Predicted - ['0', '7', '9', '5', '0'], Actual - ['0', '7', '0', '5', '1'] - Incorrect (digit 3, digit 5)\n",
        "meter1038.jpg: Predicted - ['1', '7', '4', '6', '1'], Actual - ['1', '7', '4', '6', '1'] - Correct\n",
        "meter1039.jpg: Predicted - ['0', '5', '7', '2', '9'], Actual - ['0', '5', '7', '2', '9'] - Correct\n",
        "meter1040.jpg: Predicted - ['0', '9', '9', '3', '1'], Actual - ['0', '9', '9', '3', '1'] - Correct\n",
        "meter1041.jpg: Predicted - ['0', '5', '6', '2', '5'], Actual - ['0', '9', '6', '3', '5'] - Incorrect (digit 2, digit 4)\n",
        "meter1042.jpg: Predicted - ['0', '9', '8', '6', '5'], Actual - ['0', '9', '8', '6', '5'] - Correct\n",
        "meter1043.jpg: Predicted - ['0', '9', '5', '0', '4'], Actual - ['0', '9', '5', '0', '9'] - Incorrect (digit 5)\n",
        "meter1044.jpg: Predicted - ['0', '8', '9', '7', '9'], Actual - ['0', '8', '9', '7', '9'] - Correct\n",
        "meter1045.jpg: Predicted - ['0', '6', '0', '2', '8'], Actual - ['0', '8', '0', '2', '6'] - Incorrect (digit 2, digit 5)\n",
        "meter1046.jpg: Predicted - ['0', '7', '3', '0', '3'], Actual - ['0', '7', '3', '0', '3'] - Correct\n",
        "meter1047.jpg: Predicted - ['0', '7', '9', '1', '3'], Actual - ['0', '7', '9', '1', '3'] - Correct\n",
        "meter1048.jpg: Predicted - ['1', '1', '7', '1', '9'], Actual - ['1', '1', '7', '1', '9'] - Correct\n",
        "meter1049.jpg: Predicted - ['1', '3', '4', '7', '4'], Actual - ['1', '3', '4', '7', '4'] - Correct\n",
        "meter1050.jpg: Predicted - ['1', '0', '5', '4', '6'], Actual - ['1', '8', '5', '4', '6'] - Incorrect (digit 2)\n",
        "meter1051.jpg: Predicted - ['0', '4', '7', '7', '5'], Actual - ['0', '4', '7', '7', '5'] - Correct\n",
        "meter1052.jpg: Predicted - ['0', '8', '9', '5', '6'], Actual - ['0', '8', '9', '5', '6'] - Correct\n",
        "meter1053.jpg: Predicted - ['0', '0', '9', '8', '3'], Actual - ['0', '8', '9', '0', '3'] - Incorrect (digit 2, digit 4)\n",
        "meter1054.jpg: Predicted - ['1', '2', '0', '1', '8'], Actual - ['1', '2', '0', '1', '8'] - Correct\n",
        "meter1055.jpg: Predicted - ['0', '4', '1', '7', '8'], Actual - ['0', '4', '1', '7', '8'] - Correct\n",
        "meter1056.jpg: Predicted - ['0', '4', '1', '8', '4'], Actual - ['0', '4', '1', '8', '4'] - Correct\n",
        "meter1057.jpg: Predicted - ['0', '3', '9', '3', '1'], Actual - ['0', '3', '9', '3', '1'] - Correct\n",
        "meter1058.jpg: Predicted - ['1', '4', '4', '4', '3'], Actual - ['1', '4', '4', '4', '3'] - Correct\n",
        "meter1059.jpg: Predicted - ['1', '0', '0', '9', '3'], Actual - ['1', '0', '0', '9', '3'] - Correct\n",
        "meter1060.jpg: Predicted - ['0', '3', '5', '0', '2'], Actual - ['0', '3', '3', '0', '2'] - Incorrect (digit 3)\n",
        "meter1061.jpg: Predicted - ['0', '0', '5', '3', '8'], Actual - ['0', '0', '5', '3', '8'] - Correct\n",
        "meter1062.jpg: Predicted - ['0', '8', '6', '4', '5'], Actual - ['0', '8', '6', '4', '5'] - Correct\n",
        "meter1063.jpg: Predicted - ['0', '5', '6', '5', '4'], Actual - ['0', '5', '6', '5', '4'] - Correct\n",
        "meter1064.jpg: Predicted - ['1', '3', '3', '7', '5'], Actual - ['1', '3', '3', '7', '5'] - Correct\n",
        "meter1065.jpg: Predicted - ['1', '2', '3', '2', '7'], Actual - ['1', '2', '3', '2', '6'] - Incorrect (digit 5)\n",
        "meter1066.jpg: Predicted - ['0', '6', '0', '4', '4'], Actual - ['0', '6', '0', '4', '4'] - Correct\n",
        "meter1067.jpg: Predicted - ['0', '4', '4', '8', '8'], Actual - ['0', '4', '4', '6', '8'] - Incorrect (digit 4)\n",
        "meter1068.jpg: Predicted - ['0', '7', '7', '9', '4'], Actual - ['0', '7', '7', '9', '2'] - Incorrect (digit 5)\n",
        "meter1069.jpg: Predicted - ['5', '7', '7', '3', '3'], Actual - ['5', '7', '7', '3', '3'] - Correct\n",
        "meter1070.jpg: Predicted - ['0', '0', '0', '6', '3'], Actual - ['5', '8', '3', '6', '3'] - Incorrect (digit 1, digit 2, digit 3)\n",
        "meter1071.jpg: Predicted - ['2', '0', '3', '2', '5'], Actual - ['2', '0', '3', '2', '5'] - Correct\n",
        "meter1072.jpg: Predicted - ['0', '8', '8', '5', '6'], Actual - ['0', '8', '8', '5', '4'] - Incorrect (digit 5)\n",
        "meter1073.jpg: Predicted - ['0', '4', '1', '5', '7'], Actual - ['0', '4', '1', '5', '7'] - Correct\n",
        "meter1074.jpg: Predicted - ['0', '1', '6', '5', '6'], Actual - ['0', '3', '6', '5', '6'] - Incorrect (digit 2)\n",
        "meter1075.jpg: Predicted - ['0', '1', '9', '5', '7'], Actual - ['0', '1', '9', '5', '7'] - Correct\n",
        "meter1076.jpg: Predicted - ['3', '6', '9', '6', '3'], Actual - ['3', '6', '9', '6', '3'] - Correct\n",
        "meter1077.jpg: Predicted - ['0', '2', '3', '6', '3'], Actual - ['0', '8', '5', '5', '1'] - Incorrect (digit 2, digit 3, digit 4, digit 5)\n",
        "meter1078.jpg: Predicted - ['3', '2', '7', '8', '3'], Actual - ['3', '2', '7', '8', '3'] - Correct\n",
        "meter1079.jpg: Predicted - ['0', '6', '0', '0', '5'], Actual - ['0', '6', '0', '0', '5'] - Correct\n",
        "meter1080.jpg: Predicted - ['2', '1', '7', '0', '2'], Actual - ['2', '1', '7', '0', '2'] - Correct\n",
        "meter1081.jpg: Predicted - ['1', '3', '2', '0', '6'], Actual - ['1', '4', '2', '0', '6'] - Incorrect (digit 2)\n",
        "meter1082.jpg: Predicted - ['1', '6', '8', '1', '6'], Actual - ['1', '6', '8', '1', '6'] - Correct\n",
        "meter1083.jpg: Predicted - ['0', '3', '9', '5', '7'], Actual - ['0', '3', '9', '5', '7'] - Correct\n",
        "meter1084.jpg: Predicted - ['1', '9', '1', '5', '2'], Actual - ['1', '9', '1', '5', '2'] - Correct\n",
        "meter1085.jpg: Predicted - ['0', '1', '2', '7', '1'], Actual - ['0', '1', '2', '7', '1'] - Correct\n",
        "meter1086.jpg: Predicted - ['1', '7', '7', '4', '6'], Actual - ['0', '7', '7', '4', '6'] - Incorrect (digit 1)\n",
        "meter1087.jpg: Predicted - ['0', '8', '6', '7', '3'], Actual - ['0', '8', '6', '7', '3'] - Correct\n",
        "meter1088.jpg: Predicted - ['1', '3', '6', '5', '2'], Actual - ['1', '3', '6', '5', '2'] - Correct\n",
        "meter1089.jpg: Predicted - ['0', '5', '4', '9', '0'], Actual - ['0', '5', '4', '9', '0'] - Correct\n",
        "meter1090.jpg: Predicted - ['1', '6', '3', '4', '8'], Actual - ['1', '6', '3', '4', '8'] - Correct\n",
        "meter1091.jpg: Predicted - ['1', '4', '9', '5', '5'], Actual - ['1', '4', '9', '5', '5'] - Correct\n",
        "meter1092.jpg: Predicted - ['0', '8', '8', '1', '6'], Actual - ['0', '8', '8', '1', '6'] - Correct\n",
        "meter1093.jpg: Predicted - ['0', '7', '3', '6', '3'], Actual - ['0', '7', '3', '6', '3'] - Correct\n",
        "meter1094.jpg: Predicted - ['0', '9', '5', '0', '9'], Actual - ['0', '9', '5', '0', '9'] - Correct\n",
        "meter1095.jpg: Predicted - ['1', '2', '5', '7', '4'], Actual - ['1', '2', '5', '2', '4'] - Incorrect (digit 4)\n",
        "meter1096.jpg: Predicted - ['1', '0', '0', '6', '8'], Actual - ['1', '3', '0', '6', '8'] - Incorrect (digit 2)\n",
        "meter1097.jpg: Predicted - ['0', '8', '9', '5', '6'], Actual - ['0', '8', '9', '5', '6'] - Correct\n",
        "meter1098.jpg: Predicted - ['0', '6', '3', '2', '8'], Actual - ['0', '6', '3', '2', '8'] - Correct\n",
        "meter1099.jpg: Predicted - ['0', '0', '0', '7', '9'], Actual - ['0', '8', '0', '7', '9'] - Incorrect (digit 2)\n",
        "meter1100.jpg: Predicted - ['1', '0', '3', '1', '7'], Actual - ['1', '0', '3', '1', '7'] - Correct\n",
        "meter1101.jpg: Predicted - ['2', '4', '2', '1', '7'], Actual - ['2', '4', '2', '1', '7'] - Correct\n",
        "meter1102.jpg: Predicted - ['1', '7', '1', '5', '9'], Actual - ['1', '7', '1', '5', '9'] - Correct\n",
        "meter1103.jpg: Predicted - ['2', '5', '8', '9', '9'], Actual - ['2', '5', '8', '9', '9'] - Correct\n",
        "meter1104.jpg: Predicted - ['0', '8', '0', '9', '6'], Actual - ['0', '8', '0', '9', '6'] - Correct\n",
        "meter1105.jpg: Predicted - ['0', '5', '4', '4', '4'], Actual - ['0', '5', '4', '4', '4'] - Correct\n",
        "meter1106.jpg: Predicted - ['2', '2', '2', '3', '6'], Actual - ['2', '2', '2', '3', '6'] - Correct\n",
        "meter1107.jpg: Predicted - ['2', '9', '7', '2', '1'], Actual - ['2', '9', '7', '2', '1'] - Correct\n",
        "meter1108.jpg: Predicted - ['6', '2', '1', '3', '4'], Actual - ['9', '2', '1', '3', '4'] - Incorrect (digit 1)\n",
        "meter1109.jpg: Predicted - ['2', '1', '7', '6', '5'], Actual - ['2', '1', '7', '6', '5'] - Correct\n",
        "meter1110.jpg: Predicted - ['1', '2', '3', '6', '1'], Actual - ['1', '2', '3', '6', '1'] - Correct\n",
        "meter1111.jpg: Predicted - ['2', '7', '6', '0', '3'], Actual - ['2', '7', '6', '0', '3'] - Correct\n",
        "meter1112.jpg: Predicted - ['0', '4', '5', '9', '4'], Actual - ['6', '4', '5', '9', '4'] - Incorrect (digit 1)\n",
        "meter1113.jpg: Predicted - ['1', '4', '9', '8', '7'], Actual - ['1', '4', '9', '3', '7'] - Incorrect (digit 4)\n",
        "meter1114.jpg: Predicted - ['0', '5', '4', '5', '5'], Actual - ['0', '5', '4', '5', '5'] - Correct\n",
        "meter1115.jpg: Predicted - ['0', '3', '4', '1', '1'], Actual - ['0', '3', '4', '1', '1'] - Correct\n",
        "meter1116.jpg: Predicted - ['2', '6', '4', '4', '5'], Actual - ['2', '6', '4', '4', '5'] - Correct\n",
        "meter1117.jpg: Predicted - ['0', '1', '9', '3', '5'], Actual - ['0', '1', '9', '3', '5'] - Correct\n",
        "meter1118.jpg: Predicted - ['1', '2', '2', '7', '0'], Actual - ['1', '2', '2', '7', '0'] - Correct\n",
        "meter1119.jpg: Predicted - ['0', '0', '7', '0', '6'], Actual - ['0', '0', '7', '0', '6'] - Correct\n",
        "meter1120.jpg: Predicted - ['0', '5', '5', '2', '8'], Actual - ['0', '5', '5', '2', '8'] - Correct\n",
        "meter1121.jpg: Predicted - ['0', '3', '5', '4', '8'], Actual - ['0', '3', '5', '4', '6'] - Incorrect (digit 5)\n",
        "meter1122.jpg: Predicted - ['1', '4', '6', '0', '7'], Actual - ['1', '4', '6', '0', '7'] - Correct\n",
        "meter1123.jpg: Predicted - ['2', '2', '7', '5', '7'], Actual - ['2', '2', '7', '5', '7'] - Correct\n",
        "meter1124.jpg: Predicted - ['0', '3', '2', '8', '8'], Actual - ['0', '3', '2', '8', '8'] - Correct\n",
        "meter1125.jpg: Predicted - ['4', '6', '5', '8', '8'], Actual - ['4', '4', '5', '8', '9'] - Incorrect (digit 2, digit 5)\n",
        "meter1126.jpg: Predicted - ['2', '7', '6', '4', '1'], Actual - ['2', '7', '6', '4', '1'] - Correct\n",
        "meter1127.jpg: Predicted - ['0', '5', '2', '4', '2'], Actual - ['0', '5', '2', '4', '2'] - Correct\n",
        "meter1128.jpg: Predicted - ['1', '0', '3', '9', '1'], Actual - ['1', '0', '3', '9', '1'] - Correct\n",
        "meter1129.jpg: Predicted - ['4', '3', '8', '3', '3'], Actual - ['4', '3', '8', '3', '3'] - Correct\n",
        "meter1130.jpg: Predicted - ['0', '1', '1', '2', '5'], Actual - ['0', '1', '1', '2', '5'] - Correct\n",
        "meter1131.jpg: Predicted - ['3', '3', '1', '9', '0'], Actual - ['3', '3', '1', '9', '0'] - Correct\n",
        "meter1132.jpg: Predicted - ['1', '1', '1', '2', '7'], Actual - ['1', '1', '1', '2', '1'] - Incorrect (digit 5)\n",
        "meter1133.jpg: Predicted - ['0', '0', '5', '9', '8'], Actual - ['8', '9', '5', '9', '8'] - Incorrect (digit 1, digit 2)\n",
        "meter1134.jpg: Predicted - ['0', '3', '8', '6', '8'], Actual - ['0', '3', '8', '6', '8'] - Correct\n",
        "meter1135.jpg: Predicted - ['0', '4', '9', '2', '6'], Actual - ['0', '4', '9', '2', '6'] - Correct\n",
        "meter1136.jpg: Predicted - ['2', '6', '3', '7', '1'], Actual - ['1', '8', '3', '7', '1'] - Incorrect (digit 1, digit 2)\n",
        "meter1137.jpg: Predicted - ['1', '6', '5', '9', '0'], Actual - ['1', '6', '5', '9', '0'] - Correct\n",
        "meter1138.jpg: Predicted - ['0', '5', '8', '3', '8'], Actual - ['9', '5', '8', '3', '0'] - Incorrect (digit 1, digit 5)\n",
        "meter1139.jpg: Predicted - ['2', '4', '2', '1', '2'], Actual - ['2', '4', '2', '1', '2'] - Correct\n",
        "meter1140.jpg: Predicted - ['5', '0', '6', '8', '9'], Actual - ['5', '0', '6', '8', '9'] - Correct\n",
        "meter1141.jpg: Predicted - ['0', '2', '2', '9', '0'], Actual - ['4', '2', '2', '4', '0'] - Incorrect (digit 1, digit 4)\n",
        "meter1142.jpg: Predicted - ['0', '9', '7', '2', '6'], Actual - ['6', '9', '7', '2', '6'] - Incorrect (digit 1)\n",
        "meter1143.jpg: Predicted - ['4', '7', '8', '1', '4'], Actual - ['4', '8', '8', '1', '0'] - Incorrect (digit 2, digit 5)\n",
        "meter1144.jpg: Predicted - ['0', '3', '1', '4', '4'], Actual - ['0', '3', '1', '4', '4'] - Correct\n",
        "meter1145.jpg: Predicted - ['0', '8', '1', '6', '6'], Actual - ['0', '8', '1', '0', '6'] - Incorrect (digit 4)\n",
        "meter1146.jpg: Predicted - ['1', '8', '5', '9', '5'], Actual - ['7', '8', '5', '9', '5'] - Incorrect (digit 1)\n",
        "meter1147.jpg: Predicted - ['2', '1', '1', '2', '1'], Actual - ['2', '1', '1', '2', '1'] - Correct\n",
        "meter1148.jpg: Predicted - ['0', '6', '1', '5', '1'], Actual - ['6', '6', '1', '5', '1'] - Incorrect (digit 1)\n",
        "meter1149.jpg: Predicted - ['1', '2', '3', '2', '5'], Actual - ['1', '2', '3', '2', '5'] - Correct\n",
        "meter1150.jpg: Predicted - ['0', '3', '9', '8', '6'], Actual - ['0', '3', '9', '8', '6'] - Correct\n",
        "meter1151.jpg: Predicted - ['4', '8', '1', '4', '8'], Actual - ['4', '8', '1', '4', '9'] - Incorrect (digit 5)\n",
        "meter1152.jpg: Predicted - ['0', '6', '9', '1', '8'], Actual - ['0', '6', '9', '1', '8'] - Correct\n",
        "meter1153.jpg: Predicted - ['3', '5', '2', '7', '0'], Actual - ['7', '5', '2', '7', '0'] - Incorrect (digit 1)\n",
        "meter1154.jpg: Predicted - ['1', '0', '8', '8', '1'], Actual - ['1', '6', '8', '0', '0'] - Incorrect (digit 2, digit 4, digit 5)\n",
        "meter1155.jpg: Predicted - ['3', '9', '9', '4', '8'], Actual - ['9', '9', '9', '4', '0'] - Incorrect (digit 1, digit 5)\n",
        "meter1156.jpg: Predicted - ['0', '7', '5', '1', '1'], Actual - ['0', '7', '5', '1', '1'] - Correct\n",
        "meter1157.jpg: Predicted - ['1', '0', '0', '0', '1'], Actual - ['6', '0', '0', '9', '1'] - Incorrect (digit 1, digit 4)\n",
        "meter1158.jpg: Predicted - ['1', '5', '6', '0', '9'], Actual - ['4', '5', '6', '0', '9'] - Incorrect (digit 1)\n",
        "meter1159.jpg: Predicted - ['1', '2', '6', '3', '0'], Actual - ['1', '2', '6', '3', '0'] - Correct\n",
        "meter1160.jpg: Predicted - ['6', '7', '5', '4', '8'], Actual - ['6', '1', '5', '4', '8'] - Incorrect (digit 2)\n",
        "meter1161.jpg: Predicted - ['3', '4', '1', '7', '9'], Actual - ['3', '4', '1', '7', '6'] - Incorrect (digit 5)\n",
        "meter1162.jpg: Predicted - ['0', '4', '9', '3', '8'], Actual - ['0', '4', '9', '3', '8'] - Correct\n",
        "meter1163.jpg: Predicted - ['0', '6', '1', '5', '8'], Actual - ['0', '6', '1', '5', '8'] - Correct\n",
        "meter1164.jpg: Predicted - ['1', '3', '4', '9', '0'], Actual - ['1', '3', '4', '9', '0'] - Correct\n",
        "meter1165.jpg: Predicted - ['1', '7', '2', '2', '7'], Actual - ['1', '7', '2', '2', '7'] - Correct\n",
        "meter1166.jpg: Predicted - ['0', '9', '9', '3', '1'], Actual - ['0', '9', '9', '3', '1'] - Correct\n",
        "meter1167.jpg: Predicted - ['0', '6', '9', '4', '7'], Actual - ['0', '6', '9', '4', '2'] - Incorrect (digit 5)\n",
        "meter1168.jpg: Predicted - ['0', '8', '4', '1', '2'], Actual - ['0', '8', '4', '1', '2'] - Correct\n",
        "meter1169.jpg: Predicted - ['0', '5', '7', '2', '9'], Actual - ['0', '5', '7', '2', '9'] - Correct\n",
        "meter1170.jpg: Predicted - ['1', '2', '3', '5', '3'], Actual - ['1', '2', '3', '5', '3'] - Correct\n",
        "meter1171.jpg: Predicted - ['0', '4', '4', '2', '5'], Actual - ['0', '4', '4', '2', '5'] - Correct\n",
        "meter1172.jpg: Predicted - ['1', '0', '5', '4', '7'], Actual - ['1', '0', '5', '4', '7'] - Correct\n",
        "meter1173.jpg: Predicted - ['0', '7', '0', '5', '1'], Actual - ['0', '7', '0', '5', '1'] - Correct\n",
        "meter1174.jpg: Predicted - ['0', '6', '2', '1', '0'], Actual - ['0', '6', '2', '1', '0'] - Correct\n",
        "meter1175.jpg: Predicted - ['1', '1', '4', '9', '6'], Actual - ['1', '1', '4', '9', '6'] - Correct\n",
        "meter1176.jpg: Predicted - ['0', '8', '0', '2', '6'], Actual - ['0', '8', '0', '2', '6'] - Correct\n",
        "meter1177.jpg: Predicted - ['1', '0', '0', '9', '7'], Actual - ['1', '0', '0', '9', '3'] - Incorrect (digit 5)\n",
        "meter1178.jpg: Predicted - ['1', '4', '4', '2', '1'], Actual - ['1', '4', '4', '2', '5'] - Incorrect (digit 5)\n",
        "meter1179.jpg: Predicted - ['1', '2', '8', '3', '7'], Actual - ['1', '2', '8', '3', '7'] - Correct\n",
        "meter1180.jpg: Predicted - ['1', '0', '4', '1', '2'], Actual - ['1', '0', '4', '1', '2'] - Correct\n",
        "meter1181.jpg: Predicted - ['0', '8', '7', '6', '4'], Actual - ['0', '8', '7', '6', '4'] - Correct\n",
        "meter1182.jpg: Predicted - ['1', '3', '0', '0', '1'], Actual - ['1', '4', '0', '0', '1'] - Incorrect (digit 2)\n",
        "meter1183.jpg: Predicted - ['0', '8', '9', '8', '3'], Actual - ['0', '8', '9', '0', '3'] - Incorrect (digit 4)\n",
        "meter1184.jpg: Predicted - ['0', '4', '1', '7', '8'], Actual - ['0', '4', '1', '7', '8'] - Correct\n",
        "meter1185.jpg: Predicted - ['1', '4', '9', '7', '9'], Actual - ['1', '4', '9', '7', '9'] - Correct\n",
        "meter1186.jpg: Predicted - ['0', '0', '0', '0', '0'], Actual - ['0', '0', '0', '0', '0'] - Correct\n",
        "meter1187.jpg: Predicted - ['0', '9', '8', '3', '9'], Actual - ['0', '9', '8', '3', '9'] - Correct\n",
        "meter1188.jpg: Predicted - ['0', '2', '7', '9', '1'], Actual - ['0', '2', '7', '9', '1'] - Correct\n",
        "meter1189.jpg: Predicted - ['0', '2', '2', '2', '1'], Actual - ['0', '3', '2', '2', '3'] - Incorrect (digit 2, digit 5)\n",
        "meter1190.jpg: Predicted - ['0', '8', '7', '8', '8'], Actual - ['0', '1', '7', '8', '8'] - Incorrect (digit 2)\n",
        "meter1191.jpg: Predicted - ['2', '0', '4', '6', '9'], Actual - ['2', '0', '4', '6', '9'] - Correct\n",
        "meter1192.jpg: Predicted - ['1', '3', '0', '0', '4'], Actual - ['1', '3', '0', '0', '4'] - Correct\n",
        "meter1193.jpg: Predicted - ['0', '5', '0', '1', '8'], Actual - ['0', '6', '0', '1', '8'] - Incorrect (digit 2)\n",
        "meter1194.jpg: Predicted - ['1', '8', '3', '9', '5'], Actual - ['1', '8', '3', '9', '5'] - Correct\n",
        "meter1195.jpg: Predicted - ['0', '5', '5', '2', '5'], Actual - ['0', '5', '5', '2', '5'] - Correct\n",
        "meter1196.jpg: Predicted - ['3', '7', '1', '7', '8'], Actual - ['3', '7', '1', '7', '0'] - Incorrect (digit 5)\n",
        "meter1197.jpg: Predicted - ['0', '6', '6', '7', '3'], Actual - ['0', '6', '6', '7', '3'] - Correct\n",
        "meter1198.jpg: Predicted - ['0', '1', '5', '2', '1'], Actual - ['0', '1', '5', '2', '1'] - Correct\n",
        "meter1199.jpg: Predicted - ['2', '2', '2', '4', '7'], Actual - ['2', '2', '2', '4', '7'] - Correct\n",
        "meter1200.jpg: Predicted - ['0', '7', '3', '1', '7'], Actual - ['0', '7', '3', '1', '7'] - Correct\n",
        "\n",
        "Total correct predictions: 159\n",
        "Accuracy: 63.60%\n"
      ],
      "metadata": {
        "id": "nNDpo9yWh9Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diese Textdatei wurde anhand des folgenden Skriptes um neue Berechnungen ergänzt, die sich auf die Predictions für die einzelnen Ziffern beziehen."
      ],
      "metadata": {
        "id": "j7iQehWMOhSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datei-Pfad\n",
        "file_path = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/results_check.txt'\n",
        "\n",
        "# Variablen initialisieren\n",
        "total_digits = 0\n",
        "correct_digits = 0\n",
        "digit_correct_count = [0, 0, 0, 0, 0]\n",
        "digit_total_count = [0, 0, 0, 0, 0]\n",
        "\n",
        "# Datei einlesen und Werte extrahieren\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Daten verarbeiten\n",
        "for line in lines:\n",
        "    if \"Predicted\" in line and \"Actual\" in line:\n",
        "        predicted = line.strip().split(\"Predicted - [\")[1].split(\"]\")[0].split(\", \")\n",
        "        actual = line.strip().split(\"Actual - [\")[1].split(\"]\")[0].split(\", \")\n",
        "        for i in range(5):\n",
        "            digit_total_count[i] += 1\n",
        "            if predicted[i] == actual[i]:\n",
        "                correct_digits += 1\n",
        "                digit_correct_count[i] += 1\n",
        "        total_digits += 5\n",
        "\n",
        "# Gesamtanzahl korrekt erkannter Ziffern\n",
        "overall_accuracy = (correct_digits / total_digits) * 100\n",
        "\n",
        "# Prozentsatz der korrekt erkannten Ziffern pro Ziffernposition\n",
        "digit_accuracy = [(count / digit_total_count[i]) * 100 for i, count in enumerate(digit_correct_count)]\n",
        "\n",
        "# Ergebnisse zur Datei hinzufügen\n",
        "with open(file_path, 'a') as file:\n",
        "    file.write(f\"\\nTotal correct digits: {correct_digits}\\n\")\n",
        "    file.write(f\"Overall digit accuracy: {overall_accuracy:.2f}%\\n\")\n",
        "    for i, accuracy in enumerate(digit_accuracy):\n",
        "        file.write(f\"Digit {i+1} accuracy: {accuracy:.2f}%\\n\")\n",
        "\n",
        "    # Prozentsatz der insgesamt richtig erkannten Ziffern berechnen\n",
        "    total_readings = len([line for line in lines if \"Predicted\" in line and \"Actual\" in line])\n",
        "    correct_readings = sum(1 for line in lines if \"Predicted\" in line and \"Actual\" in line and line.strip().split(\"Predicted - [\")[1].split(\"]\")[0].split(\", \") == line.strip().split(\"Actual - [\")[1].split(\"]\")[0].split(\", \"))\n",
        "    total_digit_accuracy = (correct_digits / (total_readings * 5)) * 100\n",
        "\n",
        "    file.write(f\"Total digit accuracy across all readings: {total_digit_accuracy:.2f}%\\n\")\n",
        "\n",
        "print(\"Evaluation complete. Results added to the file.\")\n"
      ],
      "metadata": {
        "id": "AC9ee8DKOsfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Ergänzung Digit Predictions\n",
        "\n",
        "Total correct predictions: 159\n",
        "Accuracy: 63.60%\n",
        "\n",
        "Total correct digits: 1115\n",
        "Overall digit accuracy: 89.20%\n",
        "Digit 1 accuracy: 90.40%\n",
        "Digit 2 accuracy: 88.80%\n",
        "Digit 3 accuracy: 94.40%\n",
        "Digit 4 accuracy: 90.40%\n",
        "Digit 5 accuracy: 82.00%\n",
        "Total digit accuracy across all readings: 89.20%\n"
      ],
      "metadata": {
        "id": "cGas_S0M4N6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 Test mit Predicted Images\n",
        "Um die beiden trainierten Modelle (Yolov5 & CRNET) zu verbinden, wurde händisch der neue Ordner 'test_crp' erstellt, der die vom YoloV5-Modell vorhergesagten Counter in 'images' und den Zählerstand als Textdateien in 'labels' enthält. Anschließend wurden die Prozessschritte analog 4.4 & 4.5 durchgeführt.\n"
      ],
      "metadata": {
        "id": "4BE86WkZ50M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### prediction_test.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Definiere die CRNET-Klasse und die crnet-Funktion\n",
        "class CRNET(nn.Module):\n",
        "    def __init__(self, num_classes=10, num_digits=5):\n",
        "        super(CRNET, self).__init__()\n",
        "        self.num_digits = num_digits\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifiers = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        ) for _ in range(num_digits)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        outputs = [classifier(x) for classifier in self.classifiers]\n",
        "        return outputs\n",
        "\n",
        "def crnet(num_classes=10, num_digits=5):\n",
        "    model = CRNET(num_classes=num_classes, num_digits=num_digits)\n",
        "    return model\n",
        "\n",
        "# Funktion zum Laden des Modells\n",
        "def load_model(model_path, num_classes=10, num_digits=5):\n",
        "    model = crnet(num_classes=num_classes, num_digits=num_digits)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    return model\n",
        "\n",
        "# Funktion zur Vorhersage des Zählerstands\n",
        "def predict_meter_reading(model, image_path, device='cpu'):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        predictions = [torch.argmax(output, dim=1).item() for output in outputs]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Pfade\n",
        "model_path = '/Users/janriedel/Downloads/detector/crnet_dd.pth'\n",
        "test_images_dir = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/test_crp/images'\n",
        "output_file = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/predictions_crp.txt'\n",
        "\n",
        "# Laden des Modells\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Vorhersagen für alle Testbilder\n",
        "results = []\n",
        "for image_name in os.listdir(test_images_dir):\n",
        "    if image_name.endswith(('.jpg', '.jpeg', '.png')):\n",
        "        image_path = os.path.join(test_images_dir, image_name)\n",
        "        predictions = predict_meter_reading(model, image_path)\n",
        "        results.append((image_name, predictions))\n",
        "\n",
        "# Speichern der Ergebnisse\n",
        "with open(output_file, 'w') as f:\n",
        "    for image_name, predictions in results:\n",
        "        f.write(f\"{image_name}: {''.join(map(str, predictions))}\\n\")\n",
        "\n",
        "print(f\"Predictions saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "O_heSRKs8F9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### sort_predictions_test.py\n",
        "\n",
        "# Datei zum Lesen und Sortieren der Vorhersagen\n",
        "input_file = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/predictions_crp.txt'\n",
        "output_file = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/predictions_sorted_crp.txt'\n",
        "\n",
        "# Funktion zum Extrahieren der Nummer aus dem Dateinamen\n",
        "def extract_number(filename):\n",
        "    import re\n",
        "    match = re.search(r'meter(\\d+)', filename)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "# Lesen der Vorhersagen aus der Datei\n",
        "with open(input_file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Sortieren der Vorhersagen nach Dateinummer\n",
        "sorted_lines = sorted(lines, key=lambda x: extract_number(x.split(':')[0]))\n",
        "\n",
        "# Schreiben der sortierten Vorhersagen in die Ausgabedatei\n",
        "with open(output_file, 'w') as f:\n",
        "    for line in sorted_lines:\n",
        "        f.write(line)\n",
        "\n",
        "print(f\"Sorted predictions saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "hQJuWEbl8Puv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### compare_predictions_test.py\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Pfade definieren\n",
        "predictions_file = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/predictions_sorted_crp.txt'\n",
        "labels_dir = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/test_crp/labels'\n",
        "output_file = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/results_check_crp.txt'\n",
        "\n",
        "# Funktion zum Extrahieren der Nummer aus dem Dateinamen\n",
        "def extract_number(filename):\n",
        "    match = re.search(r'meter(\\d+)', filename)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "# Funktion zum Lesen der tatsächlichen Labels aus einer Label-Datei\n",
        "def read_labels(label_file):\n",
        "    with open(label_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    labels = [line.split()[1] for line in lines[:5]]  # Nur die ersten 5 Zeilen\n",
        "    return labels\n",
        "\n",
        "# Lesen der Vorhersagen aus der Datei\n",
        "with open(predictions_file, 'r') as f:\n",
        "    predictions = f.readlines()\n",
        "\n",
        "total_predictions = 0\n",
        "correct_predictions = 0\n",
        "\n",
        "# Ergebnisvergleich und Schreiben in die Ausgabedatei\n",
        "with open(output_file, 'w') as f:\n",
        "    for line in predictions:\n",
        "        image_name, predicted_reading = line.strip().split(': ')\n",
        "        predicted_digits = list(predicted_reading)\n",
        "\n",
        "        # Pfad zur entsprechenden Label-Datei\n",
        "        label_file = os.path.join(labels_dir, image_name.replace('.jpg', '.txt'))\n",
        "\n",
        "        if os.path.exists(label_file):\n",
        "            actual_digits = read_labels(label_file)\n",
        "            result = f\"{image_name}: Predicted - {predicted_digits}, Actual - {actual_digits}\"\n",
        "\n",
        "            # Überprüfung der Korrektheit\n",
        "            if predicted_digits == actual_digits:\n",
        "                result += \" - Correct\\n\"\n",
        "                correct_predictions += 1\n",
        "            else:\n",
        "                incorrect_digits = [f\"digit {i+1}\" for i in range(len(predicted_digits)) if predicted_digits[i] != actual_digits[i]]\n",
        "                result += f\" - Incorrect ({', '.join(incorrect_digits)})\\n\"\n",
        "            total_predictions += 1\n",
        "        else:\n",
        "            result = f\"{image_name}: Predicted - {predicted_digits}, Actual - File not found\\n\"\n",
        "\n",
        "        f.write(result)\n",
        "\n",
        "# Berechnung des Prozentsatzes\n",
        "accuracy_percentage = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
        "\n",
        "# Schreiben der Zusammenfassung in die Ausgabedatei\n",
        "with open(output_file, 'a') as f:  # 'a' für append\n",
        "    f.write(f\"\\nTotal correct predictions: {correct_predictions}\\n\")\n",
        "    f.write(f\"Accuracy: {accuracy_percentage:.2f}%\\n\")\n",
        "\n",
        "print(f\"Results saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "PLc2vyGu8Vtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### CRNet results_new_calculations_crp.py\n",
        "\n",
        "# Datei-Pfad\n",
        "file_path = '/Users/janriedel/Downloads/detector/CRNET_digit_detection/results/results_check_crp.txt'\n",
        "\n",
        "# Variablen initialisieren\n",
        "total_digits = 0\n",
        "correct_digits = 0\n",
        "digit_correct_count = [0, 0, 0, 0, 0]\n",
        "digit_total_count = [0, 0, 0, 0, 0]\n",
        "\n",
        "# Datei einlesen und Werte extrahieren\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Daten verarbeiten\n",
        "for line in lines:\n",
        "    if \"Predicted\" in line and \"Actual\" in line:\n",
        "        predicted = line.strip().split(\"Predicted - [\")[1].split(\"]\")[0].split(\", \")\n",
        "        actual = line.strip().split(\"Actual - [\")[1].split(\"]\")[0].split(\", \")\n",
        "        for i in range(5):\n",
        "            digit_total_count[i] += 1\n",
        "            if predicted[i] == actual[i]:\n",
        "                correct_digits += 1\n",
        "                digit_correct_count[i] += 1\n",
        "        total_digits += 5\n",
        "\n",
        "# Gesamtanzahl korrekt erkannter Ziffern\n",
        "overall_accuracy = (correct_digits / total_digits) * 100\n",
        "\n",
        "# Prozentsatz der korrekt erkannten Ziffern pro Ziffernposition\n",
        "digit_accuracy = [(count / digit_total_count[i]) * 100 for i, count in enumerate(digit_correct_count)]\n",
        "\n",
        "# Ergebnisse zur Datei hinzufügen\n",
        "with open(file_path, 'a') as file:\n",
        "    file.write(f\"\\nTotal correct digits: {correct_digits}\\n\")\n",
        "    file.write(f\"Overall digit accuracy: {overall_accuracy:.2f}%\\n\")\n",
        "    for i, accuracy in enumerate(digit_accuracy):\n",
        "        file.write(f\"Digit {i+1} accuracy: {accuracy:.2f}%\\n\")\n",
        "\n",
        "    # Prozentsatz der insgesamt richtig erkannten Ziffern berechnen\n",
        "    total_readings = len([line for line in lines if \"Predicted\" in line and \"Actual\" in line])\n",
        "    correct_readings = sum(1 for line in lines if \"Predicted\" in line and \"Actual\" in line and line.strip().split(\"Predicted - [\")[1].split(\"]\")[0].split(\", \") == line.strip().split(\"Actual - [\")[1].split(\"]\")[0].split(\", \"))\n",
        "    total_digit_accuracy = (correct_digits / (total_readings * 5)) * 100\n",
        "\n",
        "    file.write(f\"Total digit accuracy across all readings: {total_digit_accuracy:.2f}%\\n\")\n",
        "\n",
        "print(\"Evaluation complete. Results added to the file.\")\n"
      ],
      "metadata": {
        "id": "0ji4Okm-8jFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### check_results_crp.txt\n",
        "\n",
        "\n",
        "meter0951.jpg: Predicted - ['1', '2', '7', '2', '8'], Actual - ['1', '2', '7', '2', '8'] - Correct\n",
        "meter0952.jpg: Predicted - ['1', '3', '8', '8', '5'], Actual - ['3', '3', '8', '8', '5'] - Incorrect (digit 1)\n",
        "meter0953.jpg: Predicted - ['1', '9', '0', '2', '5'], Actual - ['1', '9', '0', '2', '5'] - Correct\n",
        "meter0954.jpg: Predicted - ['0', '3', '6', '8', '9'], Actual - ['0', '3', '6', '8', '9'] - Correct\n",
        "meter0955.jpg: Predicted - ['0', '0', '0', '0', '1'], Actual - ['0', '0', '0', '0', '1'] - Correct\n",
        "meter0956.jpg: Predicted - ['1', '6', '6', '8', '5'], Actual - ['1', '6', '6', '8', '5'] - Correct\n",
        "meter0957.jpg: Predicted - ['0', '3', '0', '2', '3'], Actual - ['0', '3', '0', '2', '3'] - Correct\n",
        "meter0958.jpg: Predicted - ['0', '8', '4', '9', '1'], Actual - ['0', '8', '4', '9', '1'] - Correct\n",
        "meter0959.jpg: Predicted - ['0', '4', '2', '2', '0'], Actual - ['0', '4', '2', '2', '0'] - Correct\n",
        "meter0960.jpg: Predicted - ['2', '6', '1', '4', '5'], Actual - ['2', '6', '7', '4', '3'] - Incorrect (digit 3, digit 5)\n",
        "meter0961.jpg: Predicted - ['0', '7', '3', '5', '3'], Actual - ['0', '7', '3', '5', '3'] - Correct\n",
        "meter0962.jpg: Predicted - ['1', '2', '8', '4', '0'], Actual - ['1', '2', '8', '4', '0'] - Correct\n",
        "meter0963.jpg: Predicted - ['1', '3', '1', '8', '4'], Actual - ['1', '3', '7', '8', '6'] - Incorrect (digit 3, digit 5)\n",
        "meter0964.jpg: Predicted - ['1', '3', '8', '3', '3'], Actual - ['1', '3', '8', '3', '3'] - Correct\n",
        "meter0965.jpg: Predicted - ['0', '8', '3', '8', '2'], Actual - ['0', '8', '3', '0', '2'] - Incorrect (digit 4)\n",
        "meter0966.jpg: Predicted - ['1', '6', '1', '7', '8'], Actual - ['1', '6', '1', '7', '9'] - Incorrect (digit 5)\n",
        "meter0967.jpg: Predicted - ['0', '8', '2', '7', '6'], Actual - ['0', '8', '2', '7', '6'] - Correct\n",
        "meter0968.jpg: Predicted - ['0', '5', '3', '2', '2'], Actual - ['0', '5', '3', '2', '2'] - Correct\n",
        "meter0969.jpg: Predicted - ['4', '1', '8', '4', '3'], Actual - ['4', '1', '8', '4', '3'] - Correct\n",
        "meter0970.jpg: Predicted - ['1', '7', '4', '5', '1'], Actual - ['7', '7', '4', '5', '1'] - Incorrect (digit 1)\n",
        "meter0971.jpg: Predicted - ['1', '0', '1', '9', '9'], Actual - ['1', '0', '7', '9', '9'] - Incorrect (digit 3)\n",
        "meter0972.jpg: Predicted - ['1', '0', '6', '1', '3'], Actual - ['1', '0', '6', '1', '3'] - Correct\n",
        "meter0973.jpg: Predicted - ['0', '5', '9', '6', '2'], Actual - ['0', '5', '9', '6', '2'] - Correct\n",
        "meter0974.jpg: Predicted - ['0', '6', '6', '9', '1'], Actual - ['0', '6', '3', '9', '1'] - Incorrect (digit 3)\n",
        "meter0975.jpg: Predicted - ['0', '5', '5', '5', '4'], Actual - ['4', '5', '5', '5', '4'] - Incorrect (digit 1)\n",
        "meter0976.jpg: Predicted - ['0', '0', '0', '0', '3'], Actual - ['0', '0', '9', '0', '3'] - Incorrect (digit 3)\n",
        "meter0977.jpg: Predicted - ['1', '5', '0', '6', '5'], Actual - ['1', '5', '6', '0', '5'] - Incorrect (digit 3, digit 4)\n",
        "meter0978.jpg: Predicted - ['0', '1', '7', '9', '4'], Actual - ['0', '5', '2', '6', '5'] - Incorrect (digit 2, digit 3, digit 4, digit 5)\n",
        "meter0979.jpg: Predicted - ['0', '0', '6', '8', '9'], Actual - ['0', '0', '8', '8', '9'] - Incorrect (digit 3)\n",
        "meter0980.jpg: Predicted - ['0', '1', '4', '5', '9'], Actual - ['0', '1', '4', '5', '9'] - Correct\n",
        "meter0981.jpg: Predicted - ['0', '3', '1', '3', '4'], Actual - ['0', '3', '1', '3', '4'] - Correct\n",
        "meter0982.jpg: Predicted - ['0', '6', '9', '6', '8'], Actual - ['0', '6', '9', '6', '8'] - Correct\n",
        "meter0983.jpg: Predicted - ['1', '4', '0', '1', '4'], Actual - ['0', '9', '0', '1', '4'] - Incorrect (digit 1, digit 2)\n",
        "meter0984.jpg: Predicted - ['1', '9', '1', '6', '5'], Actual - ['0', '9', '1', '6', '5'] - Incorrect (digit 1)\n",
        "meter0985.jpg: Predicted - ['0', '0', '1', '9', '6'], Actual - ['0', '0', '1', '9', '4'] - Incorrect (digit 5)\n",
        "meter0986.jpg: Predicted - ['0', '6', '2', '3', '3'], Actual - ['0', '6', '7', '3', '3'] - Incorrect (digit 3)\n",
        "meter0987.jpg: Predicted - ['2', '2', '2', '3', '6'], Actual - ['2', '2', '2', '3', '6'] - Correct\n",
        "meter0988.jpg: Predicted - ['1', '7', '1', '1', '9'], Actual - ['1', '7', '1', '7', '8'] - Incorrect (digit 4, digit 5)\n",
        "meter0989.jpg: Predicted - ['0', '5', '3', '4', '8'], Actual - ['0', '5', '3', '4', '6'] - Incorrect (digit 5)\n",
        "meter0990.jpg: Predicted - ['1', '4', '6', '6', '2'], Actual - ['1', '4', '6', '6', '2'] - Correct\n",
        "meter0991.jpg: Predicted - ['1', '4', '3', '7', '4'], Actual - ['1', '4', '3', '7', '4'] - Correct\n",
        "meter0992.jpg: Predicted - ['2', '4', '9', '5', '1'], Actual - ['2', '4', '9', '5', '1'] - Correct\n",
        "meter0993.jpg: Predicted - ['1', '0', '3', '0', '3'], Actual - ['1', '0', '3', '0', '3'] - Correct\n",
        "meter0994.jpg: Predicted - ['1', '3', '0', '7', '8'], Actual - ['1', '3', '1', '7', '8'] - Incorrect (digit 3)\n",
        "meter0995.jpg: Predicted - ['0', '0', '0', '8', '4'], Actual - ['4', '0', '0', '8', '0'] - Incorrect (digit 1, digit 5)\n",
        "meter0996.jpg: Predicted - ['0', '7', '1', '9', '6'], Actual - ['0', '1', '1', '9', '5'] - Incorrect (digit 2, digit 5)\n",
        "meter0997.jpg: Predicted - ['0', '5', '7', '2', '3'], Actual - ['0', '5', '7', '3', '3'] - Incorrect (digit 4)\n",
        "meter0998.jpg: Predicted - ['0', '4', '0', '5', '9'], Actual - ['0', '2', '0', '5', '3'] - Incorrect (digit 2, digit 5)\n",
        "meter0999.jpg: Predicted - ['1', '7', '5', '5', '3'], Actual - ['1', '1', '5', '5', '3'] - Incorrect (digit 2)\n",
        "meter1000.jpg: Predicted - ['0', '9', '6', '9', '1'], Actual - ['0', '9', '6', '9', '1'] - Correct\n",
        "meter1001.jpg: Predicted - ['0', '4', '1', '4', '6'], Actual - ['0', '5', '2', '4', '6'] - Incorrect (digit 2, digit 3)\n",
        "meter1002.jpg: Predicted - ['0', '5', '3', '8', '1'], Actual - ['0', '5', '3', '8', '2'] - Incorrect (digit 5)\n",
        "meter1003.jpg: Predicted - ['0', '4', '2', '4', '0'], Actual - ['0', '5', '2', '4', '6'] - Incorrect (digit 2, digit 5)\n",
        "meter1004.jpg: Predicted - ['1', '0', '2', '1', '3'], Actual - ['1', '0', '2', '1', '3'] - Correct\n",
        "meter1005.jpg: Predicted - ['0', '9', '6', '9', '9'], Actual - ['0', '9', '6', '9', '9'] - Correct\n",
        "meter1006.jpg: Predicted - ['0', '9', '4', '4', '4'], Actual - ['0', '9', '4', '4', '4'] - Correct\n",
        "meter1007.jpg: Predicted - ['0', '9', '0', '7', '2'], Actual - ['0', '9', '0', '7', '2'] - Correct\n",
        "meter1008.jpg: Predicted - ['1', '1', '9', '8', '2'], Actual - ['1', '1', '9', '8', '2'] - Correct\n",
        "meter1009.jpg: Predicted - ['1', '3', '8', '4', '7'], Actual - ['1', '3', '8', '4', '7'] - Correct\n",
        "meter1010.jpg: Predicted - ['1', '0', '3', '1', '6'], Actual - ['1', '0', '3', '1', '7'] - Incorrect (digit 5)\n",
        "meter1011.jpg: Predicted - ['1', '5', '1', '3', '4'], Actual - ['2', '2', '1', '3', '4'] - Incorrect (digit 1, digit 2)\n",
        "meter1012.jpg: Predicted - ['2', '2', '1', '9', '5'], Actual - ['2', '2', '7', '9', '5'] - Incorrect (digit 3)\n",
        "meter1013.jpg: Predicted - ['1', '1', '4', '6', '1'], Actual - ['3', '1', '4', '5', '1'] - Incorrect (digit 1, digit 4)\n",
        "meter1014.jpg: Predicted - ['1', '4', '5', '3', '1'], Actual - ['1', '4', '5', '3', '1'] - Correct\n",
        "meter1015.jpg: Predicted - ['1', '8', '8', '5', '0'], Actual - ['1', '8', '8', '5', '8'] - Incorrect (digit 5)\n",
        "meter1016.jpg: Predicted - ['1', '0', '1', '2', '8'], Actual - ['1', '0', '1', '3', '9'] - Incorrect (digit 4, digit 5)\n",
        "meter1017.jpg: Predicted - ['0', '8', '2', '8', '9'], Actual - ['0', '8', '2', '0', '9'] - Incorrect (digit 4)\n",
        "meter1018.jpg: Predicted - ['4', '7', '6', '1', '2'], Actual - ['4', '7', '6', '0', '5'] - Incorrect (digit 4, digit 5)\n",
        "meter1019.jpg: Predicted - ['6', '0', '5', '9', '2'], Actual - ['6', '0', '5', '9', '2'] - Correct\n",
        "meter1020.jpg: Predicted - ['1', '1', '8', '4', '7'], Actual - ['3', '1', '6', '5', '7'] - Incorrect (digit 1, digit 3, digit 4)\n",
        "meter1021.jpg: Predicted - ['0', '7', '0', '5', '8'], Actual - ['8', '7', '0', '5', '0'] - Incorrect (digit 1, digit 5)\n",
        "meter1022.jpg: Predicted - ['2', '3', '3', '2', '2'], Actual - ['2', '3', '3', '2', '2'] - Correct\n",
        "meter1023.jpg: Predicted - ['0', '7', '8', '6', '7'], Actual - ['0', '7', '8', '6', '7'] - Correct\n",
        "meter1024.jpg: Predicted - ['0', '3', '3', '4', '8'], Actual - ['0', '3', '3', '8', '6'] - Incorrect (digit 4, digit 5)\n",
        "meter1025.jpg: Predicted - ['1', '0', '9', '4', '6'], Actual - ['1', '0', '9', '4', '6'] - Correct\n",
        "meter1026.jpg: Predicted - ['1', '9', '9', '2', '0'], Actual - ['1', '9', '9', '2', '0'] - Correct\n",
        "meter1027.jpg: Predicted - ['0', '6', '0', '6', '0'], Actual - ['0', '6', '0', '6', '0'] - Correct\n",
        "meter1028.jpg: Predicted - ['1', '4', '7', '0', '6'], Actual - ['1', '4', '2', '0', '6'] - Incorrect (digit 3)\n",
        "meter1029.jpg: Predicted - ['0', '9', '2', '8', '3'], Actual - ['0', '3', '3', '8', '3'] - Incorrect (digit 2, digit 3)\n",
        "meter1030.jpg: Predicted - ['0', '6', '1', '9', '2'], Actual - ['0', '6', '1', '9', '7'] - Incorrect (digit 5)\n",
        "meter1031.jpg: Predicted - ['0', '6', '5', '6', '9'], Actual - ['0', '6', '5', '6', '9'] - Correct\n",
        "meter1032.jpg: Predicted - ['1', '4', '2', '7', '9'], Actual - ['1', '4', '2', '7', '9'] - Correct\n",
        "meter1033.jpg: Predicted - ['1', '0', '9', '2', '2'], Actual - ['1', '0', '9', '2', '2'] - Correct\n",
        "meter1034.jpg: Predicted - ['0', '7', '7', '7', '5'], Actual - ['1', '1', '7', '7', '5'] - Incorrect (digit 1, digit 2)\n",
        "meter1035.jpg: Predicted - ['1', '4', '3', '7', '5'], Actual - ['1', '4', '3', '7', '5'] - Correct\n",
        "meter1036.jpg: Predicted - ['0', '0', '9', '3', '5'], Actual - ['2', '0', '9', '3', '5'] - Incorrect (digit 1)\n",
        "meter1037.jpg: Predicted - ['0', '7', '9', '5', '6'], Actual - ['0', '7', '0', '5', '1'] - Incorrect (digit 3, digit 5)\n",
        "meter1038.jpg: Predicted - ['1', '7', '4', '6', '1'], Actual - ['1', '7', '4', '6', '1'] - Correct\n",
        "meter1039.jpg: Predicted - ['0', '5', '7', '2', '9'], Actual - ['0', '5', '7', '2', '9'] - Correct\n",
        "meter1040.jpg: Predicted - ['0', '9', '9', '3', '1'], Actual - ['0', '9', '9', '3', '1'] - Correct\n",
        "meter1041.jpg: Predicted - ['0', '5', '6', '1', '2'], Actual - ['0', '9', '6', '3', '5'] - Incorrect (digit 2, digit 4, digit 5)\n",
        "meter1042.jpg: Predicted - ['0', '9', '8', '6', '5'], Actual - ['0', '9', '8', '6', '5'] - Correct\n",
        "meter1043.jpg: Predicted - ['0', '9', '5', '0', '4'], Actual - ['0', '9', '5', '0', '9'] - Incorrect (digit 5)\n",
        "meter1044.jpg: Predicted - ['0', '8', '9', '7', '9'], Actual - ['0', '8', '9', '7', '9'] - Correct\n",
        "meter1045.jpg: Predicted - ['1', '0', '0', '2', '6'], Actual - ['0', '8', '0', '2', '6'] - Incorrect (digit 1, digit 2)\n",
        "meter1046.jpg: Predicted - ['0', '7', '3', '0', '3'], Actual - ['0', '7', '3', '0', '3'] - Correct\n",
        "meter1047.jpg: Predicted - ['0', '7', '9', '1', '7'], Actual - ['0', '7', '9', '1', '3'] - Incorrect (digit 5)\n",
        "meter1048.jpg: Predicted - ['1', '1', '7', '1', '9'], Actual - ['1', '1', '7', '1', '9'] - Correct\n",
        "meter1049.jpg: Predicted - ['1', '3', '4', '7', '4'], Actual - ['1', '3', '4', '7', '4'] - Correct\n",
        "meter1050.jpg: Predicted - ['1', '0', '5', '4', '6'], Actual - ['1', '8', '5', '4', '6'] - Incorrect (digit 2)\n",
        "meter1051.jpg: Predicted - ['0', '4', '7', '7', '5'], Actual - ['0', '4', '7', '7', '5'] - Correct\n",
        "meter1052.jpg: Predicted - ['0', '8', '9', '5', '6'], Actual - ['0', '8', '9', '5', '6'] - Correct\n",
        "meter1053.jpg: Predicted - ['1', '0', '9', '8', '3'], Actual - ['0', '8', '9', '0', '3'] - Incorrect (digit 1, digit 2, digit 4)\n",
        "meter1054.jpg: Predicted - ['1', '1', '0', '1', '8'], Actual - ['1', '2', '0', '1', '8'] - Incorrect (digit 2)\n",
        "meter1055.jpg: Predicted - ['0', '4', '1', '7', '8'], Actual - ['0', '4', '1', '7', '8'] - Correct\n",
        "meter1056.jpg: Predicted - ['0', '8', '1', '8', '4'], Actual - ['0', '4', '1', '8', '4'] - Incorrect (digit 2)\n",
        "meter1057.jpg: Predicted - ['0', '3', '1', '9', '4'], Actual - ['0', '3', '9', '3', '1'] - Incorrect (digit 3, digit 4, digit 5)\n",
        "meter1058.jpg: Predicted - ['1', '4', '4', '4', '3'], Actual - ['1', '4', '4', '4', '3'] - Correct\n",
        "meter1059.jpg: Predicted - ['1', '0', '0', '9', '3'], Actual - ['1', '0', '0', '9', '3'] - Correct\n",
        "meter1060.jpg: Predicted - ['0', '3', '3', '0', '2'], Actual - ['0', '3', '3', '0', '2'] - Correct\n",
        "meter1061.jpg: Predicted - ['0', '0', '5', '3', '8'], Actual - ['0', '0', '5', '3', '8'] - Correct\n",
        "meter1062.jpg: Predicted - ['0', '8', '6', '4', '5'], Actual - ['0', '8', '6', '4', '5'] - Correct\n",
        "meter1063.jpg: Predicted - ['0', '5', '6', '5', '4'], Actual - ['0', '5', '6', '5', '4'] - Correct\n",
        "meter1064.jpg: Predicted - ['1', '3', '3', '7', '5'], Actual - ['1', '3', '3', '7', '5'] - Correct\n",
        "meter1065.jpg: Predicted - ['1', '2', '3', '2', '7'], Actual - ['1', '2', '3', '2', '6'] - Incorrect (digit 5)\n",
        "meter1066.jpg: Predicted - ['0', '6', '0', '4', '4'], Actual - ['0', '6', '0', '4', '4'] - Correct\n",
        "meter1067.jpg: Predicted - ['0', '4', '4', '6', '8'], Actual - ['0', '4', '4', '6', '8'] - Correct\n",
        "meter1068.jpg: Predicted - ['0', '1', '1', '9', '7'], Actual - ['0', '7', '7', '9', '2'] - Incorrect (digit 2, digit 3, digit 5)\n",
        "meter1069.jpg: Predicted - ['5', '7', '7', '3', '3'], Actual - ['5', '7', '7', '3', '3'] - Correct\n",
        "meter1070.jpg: Predicted - ['3', '0', '3', '6', '3'], Actual - ['5', '8', '3', '6', '3'] - Incorrect (digit 1, digit 2)\n",
        "meter1071.jpg: Predicted - ['2', '0', '3', '2', '5'], Actual - ['2', '0', '3', '2', '5'] - Correct\n",
        "meter1072.jpg: Predicted - ['0', '8', '8', '5', '4'], Actual - ['0', '8', '8', '5', '4'] - Correct\n",
        "meter1073.jpg: Predicted - ['0', '4', '1', '5', '7'], Actual - ['0', '4', '1', '5', '7'] - Correct\n",
        "meter1074.jpg: Predicted - ['0', '3', '4', '5', '6'], Actual - ['0', '3', '6', '5', '6'] - Incorrect (digit 3)\n",
        "meter1075.jpg: Predicted - ['0', '1', '9', '5', '7'], Actual - ['0', '1', '9', '5', '7'] - Correct\n",
        "meter1076.jpg: Predicted - ['3', '6', '9', '6', '3'], Actual - ['3', '6', '9', '6', '3'] - Correct\n",
        "meter1077.jpg: Predicted - ['0', '2', '3', '6', '3'], Actual - ['0', '8', '5', '5', '1'] - Incorrect (digit 2, digit 3, digit 4, digit 5)\n",
        "meter1078.jpg: Predicted - ['3', '2', '7', '8', '3'], Actual - ['3', '2', '7', '8', '3'] - Correct\n",
        "meter1079.jpg: Predicted - ['0', '6', '0', '0', '5'], Actual - ['0', '6', '0', '0', '5'] - Correct\n",
        "meter1080.jpg: Predicted - ['2', '1', '7', '0', '2'], Actual - ['2', '1', '7', '0', '2'] - Correct\n",
        "meter1081.jpg: Predicted - ['1', '3', '1', '0', '6'], Actual - ['1', '4', '2', '0', '6'] - Incorrect (digit 2, digit 3)\n",
        "meter1082.jpg: Predicted - ['1', '6', '8', '1', '6'], Actual - ['1', '6', '8', '1', '6'] - Correct\n",
        "meter1083.jpg: Predicted - ['0', '3', '9', '5', '7'], Actual - ['0', '3', '9', '5', '7'] - Correct\n",
        "meter1084.jpg: Predicted - ['1', '9', '1', '5', '2'], Actual - ['1', '9', '1', '5', '2'] - Correct\n",
        "meter1085.jpg: Predicted - ['0', '1', '2', '7', '7'], Actual - ['0', '1', '2', '7', '1'] - Incorrect (digit 5)\n",
        "meter1086.jpg: Predicted - ['0', '7', '7', '4', '6'], Actual - ['0', '7', '7', '4', '6'] - Correct\n",
        "meter1087.jpg: Predicted - ['0', '8', '6', '7', '3'], Actual - ['0', '8', '6', '7', '3'] - Correct\n",
        "meter1088.jpg: Predicted - ['1', '3', '6', '6', '2'], Actual - ['1', '3', '6', '5', '2'] - Incorrect (digit 4)\n",
        "meter1089.jpg: Predicted - ['0', '5', '4', '9', '0'], Actual - ['0', '5', '4', '9', '0'] - Correct\n",
        "meter1090.jpg: Predicted - ['1', '6', '3', '4', '8'], Actual - ['1', '6', '3', '4', '8'] - Correct\n",
        "meter1091.jpg: Predicted - ['1', '4', '9', '5', '5'], Actual - ['1', '4', '9', '5', '5'] - Correct\n",
        "meter1092.jpg: Predicted - ['0', '8', '8', '1', '6'], Actual - ['0', '8', '8', '1', '6'] - Correct\n",
        "meter1093.jpg: Predicted - ['0', '7', '3', '6', '3'], Actual - ['0', '7', '3', '6', '3'] - Correct\n",
        "meter1094.jpg: Predicted - ['0', '9', '5', '0', '9'], Actual - ['0', '9', '5', '0', '9'] - Correct\n",
        "meter1095.jpg: Predicted - ['1', '2', '5', '2', '7'], Actual - ['1', '2', '5', '2', '4'] - Incorrect (digit 5)\n",
        "meter1096.jpg: Predicted - ['1', '0', '0', '6', '8'], Actual - ['1', '3', '0', '6', '8'] - Incorrect (digit 2)\n",
        "meter1097.jpg: Predicted - ['0', '8', '9', '5', '6'], Actual - ['0', '8', '9', '5', '6'] - Correct\n",
        "meter1098.jpg: Predicted - ['0', '6', '3', '2', '8'], Actual - ['0', '6', '3', '2', '8'] - Correct\n",
        "meter1099.jpg: Predicted - ['0', '0', '0', '7', '9'], Actual - ['0', '8', '0', '7', '9'] - Incorrect (digit 2)\n",
        "meter1100.jpg: Predicted - ['1', '0', '3', '1', '6'], Actual - ['1', '0', '3', '1', '7'] - Incorrect (digit 5)\n",
        "meter1101.jpg: Predicted - ['2', '4', '2', '1', '7'], Actual - ['2', '4', '2', '1', '7'] - Correct\n",
        "meter1102.jpg: Predicted - ['1', '7', '1', '5', '9'], Actual - ['1', '7', '1', '5', '9'] - Correct\n",
        "meter1103.jpg: Predicted - ['2', '5', '8', '9', '9'], Actual - ['2', '5', '8', '9', '9'] - Correct\n",
        "meter1104.jpg: Predicted - ['0', '8', '0', '9', '6'], Actual - ['0', '8', '0', '9', '6'] - Correct\n",
        "meter1105.jpg: Predicted - ['0', '5', '4', '4', '4'], Actual - ['0', '5', '4', '4', '4'] - Correct\n",
        "meter1106.jpg: Predicted - ['2', '2', '7', '3', '6'], Actual - ['2', '2', '2', '3', '6'] - Incorrect (digit 3)\n",
        "meter1107.jpg: Predicted - ['2', '9', '7', '2', '1'], Actual - ['2', '9', '7', '2', '1'] - Correct\n",
        "meter1108.jpg: Predicted - ['0', '2', '1', '3', '8'], Actual - ['9', '2', '1', '3', '4'] - Incorrect (digit 1, digit 5)\n",
        "meter1109.jpg: Predicted - ['2', '1', '7', '6', '5'], Actual - ['2', '1', '7', '6', '5'] - Correct\n",
        "meter1110.jpg: Predicted - ['1', '2', '3', '6', '7'], Actual - ['1', '2', '3', '6', '1'] - Incorrect (digit 5)\n",
        "meter1111.jpg: Predicted - ['2', '7', '6', '0', '3'], Actual - ['2', '7', '6', '0', '3'] - Correct\n",
        "meter1112.jpg: Predicted - ['0', '4', '5', '9', '9'], Actual - ['6', '4', '5', '9', '4'] - Incorrect (digit 1, digit 5)\n",
        "meter1113.jpg: Predicted - ['1', '4', '9', '8', '7'], Actual - ['1', '4', '9', '3', '7'] - Incorrect (digit 4)\n",
        "meter1114.jpg: Predicted - ['0', '5', '4', '5', '5'], Actual - ['0', '5', '4', '5', '5'] - Correct\n",
        "meter1115.jpg: Predicted - ['0', '3', '4', '1', '1'], Actual - ['0', '3', '4', '1', '1'] - Correct\n",
        "meter1116.jpg: Predicted - ['2', '6', '4', '4', '5'], Actual - ['2', '6', '4', '4', '5'] - Correct\n",
        "meter1117.jpg: Predicted - ['0', '1', '9', '3', '5'], Actual - ['0', '1', '9', '3', '5'] - Correct\n",
        "meter1118.jpg: Predicted - ['1', '2', '2', '7', '0'], Actual - ['1', '2', '2', '7', '0'] - Correct\n",
        "meter1119.jpg: Predicted - ['0', '0', '7', '0', '6'], Actual - ['0', '0', '7', '0', '6'] - Correct\n",
        "meter1120.jpg: Predicted - ['0', '5', '5', '2', '8'], Actual - ['0', '5', '5', '2', '8'] - Correct\n",
        "meter1121.jpg: Predicted - ['0', '3', '5', '4', '8'], Actual - ['0', '3', '5', '4', '6'] - Incorrect (digit 5)\n",
        "meter1122.jpg: Predicted - ['1', '4', '6', '0', '7'], Actual - ['1', '4', '6', '0', '7'] - Correct\n",
        "meter1123.jpg: Predicted - ['2', '2', '7', '5', '7'], Actual - ['2', '2', '7', '5', '7'] - Correct\n",
        "meter1124.jpg: Predicted - ['0', '3', '2', '8', '8'], Actual - ['0', '3', '2', '8', '8'] - Correct\n",
        "meter1125.jpg: Predicted - ['1', '8', '5', '8', '8'], Actual - ['4', '4', '5', '8', '9'] - Incorrect (digit 1, digit 2, digit 5)\n",
        "meter1126.jpg: Predicted - ['2', '7', '8', '4', '1'], Actual - ['2', '7', '6', '4', '1'] - Incorrect (digit 3)\n",
        "meter1127.jpg: Predicted - ['0', '5', '2', '4', '2'], Actual - ['0', '5', '2', '4', '2'] - Correct\n",
        "meter1128.jpg: Predicted - ['0', '0', '3', '9', '1'], Actual - ['1', '0', '3', '9', '1'] - Incorrect (digit 1)\n",
        "meter1129.jpg: Predicted - ['4', '3', '8', '3', '3'], Actual - ['4', '3', '8', '3', '3'] - Correct\n",
        "meter1130.jpg: Predicted - ['0', '1', '1', '2', '5'], Actual - ['0', '1', '1', '2', '5'] - Correct\n",
        "meter1131.jpg: Predicted - ['3', '3', '1', '9', '0'], Actual - ['3', '3', '1', '9', '0'] - Correct\n",
        "meter1132.jpg: Predicted - ['2', '1', '1', '2', '1'], Actual - ['1', '1', '1', '2', '1'] - Incorrect (digit 1)\n",
        "meter1133.jpg: Predicted - ['0', '0', '5', '9', '8'], Actual - ['8', '9', '5', '9', '8'] - Incorrect (digit 1, digit 2)\n",
        "meter1134.jpg: Predicted - ['0', '3', '8', '6', '8'], Actual - ['0', '3', '8', '6', '8'] - Correct\n",
        "meter1135.jpg: Predicted - ['0', '4', '9', '2', '6'], Actual - ['0', '4', '9', '2', '6'] - Correct\n",
        "meter1136.jpg: Predicted - ['2', '0', '3', '7', '1'], Actual - ['1', '8', '3', '7', '1'] - Incorrect (digit 1, digit 2)\n",
        "meter1137.jpg: Predicted - ['1', '6', '5', '9', '0'], Actual - ['1', '6', '5', '9', '0'] - Correct\n",
        "meter1138.jpg: Predicted - ['0', '5', '8', '3', '8'], Actual - ['9', '5', '8', '3', '0'] - Incorrect (digit 1, digit 5)\n",
        "meter1139.jpg: Predicted - ['2', '4', '2', '1', '2'], Actual - ['2', '4', '2', '1', '2'] - Correct\n",
        "meter1140.jpg: Predicted - ['5', '0', '6', '8', '9'], Actual - ['5', '0', '6', '8', '9'] - Correct\n",
        "meter1141.jpg: Predicted - ['0', '2', '2', '4', '8'], Actual - ['4', '2', '2', '4', '0'] - Incorrect (digit 1, digit 5)\n",
        "meter1142.jpg: Predicted - ['0', '9', '7', '2', '6'], Actual - ['6', '9', '7', '2', '6'] - Incorrect (digit 1)\n",
        "meter1143.jpg: Predicted - ['4', '1', '8', '8', '4'], Actual - ['4', '8', '8', '1', '0'] - Incorrect (digit 2, digit 4, digit 5)\n",
        "meter1144.jpg: Predicted - ['0', '3', '1', '4', '4'], Actual - ['0', '3', '1', '4', '4'] - Correct\n",
        "meter1145.jpg: Predicted - ['0', '0', '1', '6', '6'], Actual - ['0', '8', '1', '0', '6'] - Incorrect (digit 2, digit 4)\n",
        "meter1146.jpg: Predicted - ['1', '8', '5', '9', '5'], Actual - ['7', '8', '5', '9', '5'] - Incorrect (digit 1)\n",
        "meter1147.jpg: Predicted - ['2', '1', '1', '2', '1'], Actual - ['2', '1', '1', '2', '1'] - Correct\n",
        "meter1148.jpg: Predicted - ['0', '6', '1', '5', '6'], Actual - ['6', '6', '1', '5', '1'] - Incorrect (digit 1, digit 5)\n",
        "meter1149.jpg: Predicted - ['1', '2', '3', '2', '5'], Actual - ['1', '2', '3', '2', '5'] - Correct\n",
        "meter1150.jpg: Predicted - ['0', '3', '9', '8', '6'], Actual - ['0', '3', '9', '8', '6'] - Correct\n",
        "meter1151.jpg: Predicted - ['4', '8', '1', '4', '9'], Actual - ['4', '8', '1', '4', '9'] - Correct\n",
        "meter1152.jpg: Predicted - ['0', '8', '9', '1', '8'], Actual - ['0', '6', '9', '1', '8'] - Incorrect (digit 2)\n",
        "meter1153.jpg: Predicted - ['0', '0', '2', '7', '1'], Actual - ['7', '5', '2', '7', '0'] - Incorrect (digit 1, digit 2, digit 5)\n",
        "meter1154.jpg: Predicted - ['0', '0', '8', '8', '1'], Actual - ['1', '6', '8', '0', '0'] - Incorrect (digit 1, digit 2, digit 4, digit 5)\n",
        "meter1155.jpg: Predicted - ['3', '9', '9', '4', '8'], Actual - ['9', '9', '9', '4', '0'] - Incorrect (digit 1, digit 5)\n",
        "meter1156.jpg: Predicted - ['0', '7', '5', '1', '1'], Actual - ['0', '7', '5', '1', '1'] - Correct\n",
        "meter1157.jpg: Predicted - ['1', '0', '0', '9', '1'], Actual - ['6', '0', '0', '9', '1'] - Incorrect (digit 1)\n",
        "meter1158.jpg: Predicted - ['1', '5', '6', '0', '9'], Actual - ['4', '5', '6', '0', '9'] - Incorrect (digit 1)\n",
        "meter1159.jpg: Predicted - ['1', '2', '6', '3', '0'], Actual - ['1', '2', '6', '3', '0'] - Correct\n",
        "meter1160.jpg: Predicted - ['6', '7', '5', '4', '8'], Actual - ['6', '1', '5', '4', '8'] - Incorrect (digit 2)\n",
        "meter1161.jpg: Predicted - ['3', '4', '1', '7', '4'], Actual - ['3', '4', '1', '7', '6'] - Incorrect (digit 5)\n",
        "meter1162.jpg: Predicted - ['0', '4', '9', '3', '8'], Actual - ['0', '4', '9', '3', '8'] - Correct\n",
        "meter1163.jpg: Predicted - ['0', '6', '1', '5', '8'], Actual - ['0', '6', '1', '5', '8'] - Correct\n",
        "meter1164.jpg: Predicted - ['1', '3', '4', '9', '0'], Actual - ['1', '3', '4', '9', '0'] - Correct\n",
        "meter1165.jpg: Predicted - ['1', '7', '2', '2', '1'], Actual - ['1', '7', '2', '2', '7'] - Incorrect (digit 5)\n",
        "meter1166.jpg: Predicted - ['0', '9', '9', '3', '1'], Actual - ['0', '9', '9', '3', '1'] - Correct\n",
        "meter1167.jpg: Predicted - ['0', '6', '9', '4', '2'], Actual - ['0', '6', '9', '4', '2'] - Correct\n",
        "meter1168.jpg: Predicted - ['0', '8', '4', '1', '2'], Actual - ['0', '8', '4', '1', '2'] - Correct\n",
        "meter1169.jpg: Predicted - ['0', '5', '7', '2', '9'], Actual - ['0', '5', '7', '2', '9'] - Correct\n",
        "meter1170.jpg: Predicted - ['1', '2', '3', '5', '3'], Actual - ['1', '2', '3', '5', '3'] - Correct\n",
        "meter1171.jpg: Predicted - ['0', '4', '4', '2', '5'], Actual - ['0', '4', '4', '2', '5'] - Correct\n",
        "meter1172.jpg: Predicted - ['1', '0', '5', '4', '7'], Actual - ['1', '0', '5', '4', '7'] - Correct\n",
        "meter1173.jpg: Predicted - ['0', '7', '0', '5', '1'], Actual - ['0', '7', '0', '5', '1'] - Correct\n",
        "meter1174.jpg: Predicted - ['0', '6', '2', '1', '0'], Actual - ['0', '6', '2', '1', '0'] - Correct\n",
        "meter1175.jpg: Predicted - ['1', '1', '4', '9', '8'], Actual - ['1', '1', '4', '9', '6'] - Incorrect (digit 5)\n",
        "meter1176.jpg: Predicted - ['0', '8', '0', '2', '6'], Actual - ['0', '8', '0', '2', '6'] - Correct\n",
        "meter1177.jpg: Predicted - ['1', '0', '0', '9', '1'], Actual - ['1', '0', '0', '9', '3'] - Incorrect (digit 5)\n",
        "meter1178.jpg: Predicted - ['1', '4', '4', '2', '1'], Actual - ['1', '4', '4', '2', '5'] - Incorrect (digit 5)\n",
        "meter1179.jpg: Predicted - ['1', '2', '8', '3', '7'], Actual - ['1', '2', '8', '3', '7'] - Correct\n",
        "meter1180.jpg: Predicted - ['1', '0', '4', '1', '2'], Actual - ['1', '0', '4', '1', '2'] - Correct\n",
        "meter1181.jpg: Predicted - ['0', '8', '7', '6', '4'], Actual - ['0', '8', '7', '6', '4'] - Correct\n",
        "meter1182.jpg: Predicted - ['1', '3', '0', '0', '1'], Actual - ['1', '4', '0', '0', '1'] - Incorrect (digit 2)\n",
        "meter1183.jpg: Predicted - ['0', '8', '9', '8', '3'], Actual - ['0', '8', '9', '0', '3'] - Incorrect (digit 4)\n",
        "meter1184.jpg: Predicted - ['0', '4', '1', '7', '8'], Actual - ['0', '4', '1', '7', '8'] - Correct\n",
        "meter1185.jpg: Predicted - ['1', '4', '9', '7', '9'], Actual - ['1', '4', '9', '7', '9'] - Correct\n",
        "meter1186.jpg: Predicted - ['0', '0', '0', '0', '0'], Actual - ['0', '0', '0', '0', '0'] - Correct\n",
        "meter1187.jpg: Predicted - ['0', '9', '8', '3', '9'], Actual - ['0', '9', '8', '3', '9'] - Correct\n",
        "meter1188.jpg: Predicted - ['0', '2', '1', '9', '1'], Actual - ['0', '2', '7', '9', '1'] - Incorrect (digit 3)\n",
        "meter1189.jpg: Predicted - ['0', '3', '2', '2', '1'], Actual - ['0', '3', '2', '2', '3'] - Incorrect (digit 5)\n",
        "meter1190.jpg: Predicted - ['0', '8', '7', '8', '8'], Actual - ['0', '1', '7', '8', '8'] - Incorrect (digit 2)\n",
        "meter1191.jpg: Predicted - ['2', '0', '4', '0', '8'], Actual - ['2', '0', '4', '6', '9'] - Incorrect (digit 4, digit 5)\n",
        "meter1192.jpg: Predicted - ['1', '3', '0', '0', '4'], Actual - ['1', '3', '0', '0', '4'] - Correct\n",
        "meter1193.jpg: Predicted - ['0', '6', '0', '1', '8'], Actual - ['0', '6', '0', '1', '8'] - Correct\n",
        "meter1194.jpg: Predicted - ['1', '8', '3', '9', '5'], Actual - ['1', '8', '3', '9', '5'] - Correct\n",
        "meter1195.jpg: Predicted - ['0', '5', '5', '2', '5'], Actual - ['0', '5', '5', '2', '5'] - Correct\n",
        "meter1196.jpg: Predicted - ['3', '7', '1', '7', '1'], Actual - ['3', '7', '1', '7', '0'] - Incorrect (digit 5)\n",
        "meter1197.jpg: Predicted - ['0', '6', '6', '7', '3'], Actual - ['0', '6', '6', '7', '3'] - Correct\n",
        "meter1198.jpg: Predicted - ['0', '1', '5', '2', '1'], Actual - ['0', '1', '5', '2', '1'] - Correct\n",
        "meter1199.jpg: Predicted - ['2', '2', '2', '4', '7'], Actual - ['2', '2', '2', '4', '7'] - Correct\n",
        "meter1200.jpg: Predicted - ['0', '7', '3', '1', '7'], Actual - ['0', '7', '3', '1', '7'] - Correct\n",
        "\n",
        "Total correct predictions: 146\n",
        "Accuracy: 58.40%\n",
        "\n",
        "Total correct digits: 1089\n",
        "Overall digit accuracy: 87.12%\n",
        "Digit 1 accuracy: 87.20%\n",
        "Digit 2 accuracy: 86.80%\n",
        "Digit 3 accuracy: 90.40%\n",
        "Digit 4 accuracy: 91.20%\n",
        "Digit 5 accuracy: 80.00%\n",
        "Total digit accuracy across all readings: 87.12%\n"
      ],
      "metadata": {
        "id": "aiqNA2298jeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In den beiden Projektschritten  1. Counter Detection (YOLOv5) und 2. Digit Detection (CR-NET) wurde die Wirksamkeit eines zweistufigen Ansatzes für die automatische Zählerablesung (AMR) nachgewiesen.\n",
        "\n",
        "In dem kombinierten Prozess erreichte das CR-NET-Modell eine Gesamtablesegenauigkeit von 58,40 % bei 250 Testbildern, mit einer hohen Ziffernerkennungsgenauigkeit von 87,12 %. Das legt nahe, dass ein zweistufiger Ansatz, beginnend mit einer robusten Zählererkennung mit Modellen wie YOLOv5, gefolgt von einer präzisen Ziffernerkennung mit CR-NET, sehr effektiv ist. Um zuverlässigere Ergebnisse zu erzielen, sind jedoch größere Rechenressourcen und umfangreichere Trainingsdatensätze erforderlich (Epochen >50) Außerdem ist das derzeitige Modell, das auf fünfstelligen Zählern trainiert wurde, für reale Anwendungen, bei denen eine größere Vielfalt von Zählertypen und Ziffernzahlen üblich ist, möglicherweise noch nicht praktikabel. Eine weitere Verfeinerung und Skalierung des Modells ist notwendig, um seine praktische Anwendbarkeit zu verbessern."
      ],
      "metadata": {
        "id": "jrJPEQ0B8_Fx"
      }
    }
  ]
}